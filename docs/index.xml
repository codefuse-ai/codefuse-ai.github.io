<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CodeFuse-AI</title>
    <link>/</link>
    <description>Recent content on CodeFuse-AI</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>/docs/devops_eval/tool_learning_info/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/devops_eval/tool_learning_info/</guid>
      <description>æ•°æ®æ ·ä¾‹ åœ¨æ•°æ®ä¸Šæˆ‘ä»¬å®Œå…¨å…¼å®¹äº† OpenAI Function Callingï¼Œå…·ä½“æ ¼å¼å¦‚ä¸‹ï¼š&#xA;Function Callçš„æ•°æ®æ ¼å¼&#xA;Input Key Input Type Input Description functions List[Swagger] å·¥å…·é›†åˆ chatrounds List[chatround] å¤šè½®å¯¹è¯æ•°æ® chatroundsçš„æ•°æ®æ ¼å¼&#xA;Input Key Input Type Input Description role string è§’è‰²åç§°ï¼ŒåŒ…å«ä¸‰ç§ç±»åˆ«ï¼Œuserã€assistantã€function name string è‹¥roleä¸ºfunctionï¼Œåˆ™å­˜åœ¨nameå­—æ®µï¼Œä¸ºfunctionçš„åç§° content string roleçš„è¿”å›å†…å®¹ function_call dict å·¥å…·è°ƒç”¨ { &amp;#34;functions&amp;#34;: [ { &amp;#34;name&amp;#34;: &amp;#34;get_fudan_university_scoreline&amp;#34;, &amp;#34;description&amp;#34;: &amp;#34;æŸ¥è¯¢å¤æ—¦å¤§å­¦å¾€å¹´åˆ†æ•°çº¿ï¼Œä¾‹å¦‚ï¼šæŸ¥è¯¢2020å¹´å¤æ—¦å¤§å­¦çš„åˆ†æ•°çº¿&amp;#34;, &amp;#34;parameters&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;object&amp;#34;, &amp;#34;properties&amp;#34;: { &amp;#34;year&amp;#34;: { &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;, &amp;#34;description&amp;#34;: &amp;#34;å¹´ä»½ï¼Œä¾‹å¦‚ï¼š2020ï¼Œ2019ï¼Œ2018&amp;#34; } }, &amp;#34;required&amp;#34;: [ &amp;#34;year&amp;#34; ] } } ], &amp;#34;chatrounds&amp;#34;: [ { &amp;#34;role&amp;#34;: &amp;#34;system&amp;#34;, &amp;#34;content&amp;#34;: &amp;#34;CodeFuseæ˜¯ä¸€ä¸ªé¢å‘ç ”å‘é¢†åŸŸçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œæ—¨åœ¨ä¸­ç«‹çš„ã€æ— å®³çš„å¸®åŠ©ç”¨æˆ·è§£å†³å¼€å‘ç›¸å…³çš„é—®é¢˜ï¼Œæ‰€æœ‰çš„å›ç­”å‡ä½¿ç”¨Markdownæ ¼å¼è¿”å›ã€‚\nä½ èƒ½åˆ©ç”¨è®¸å¤šå·¥å…·å’ŒåŠŸèƒ½æ¥å®Œæˆç»™å®šçš„ä»»åŠ¡ï¼Œåœ¨æ¯ä¸€æ­¥ä¸­ï¼Œä½ éœ€è¦åˆ†æå½“å‰çŠ¶æ€ï¼Œå¹¶é€šè¿‡æ‰§è¡Œå‡½æ•°è°ƒç”¨æ¥ç¡®å®šä¸‹ä¸€æ­¥çš„è¡ŒåŠ¨æ–¹å‘ã€‚ä½ å¯ä»¥è¿›è¡Œå¤šæ¬¡å°è¯•ã€‚å¦‚æœä½ è®¡åˆ’è¿ç»­å°è¯•ä¸åŒçš„æ¡ä»¶ï¼Œè¯·æ¯æ¬¡å°è¯•ä¸€ç§æ¡ä»¶ã€‚è‹¥ç»™å®šäº†Finishå‡½æ•°,åˆ™ä»¥Finishè°ƒç”¨ç»“æŸï¼Œè‹¥æ²¡æä¾›Finishå‡½æ•°ï¼Œåˆ™ä»¥ä¸å¸¦function_callçš„å¯¹è¯ç»“æŸã€‚&amp;#34; }, { &amp;#34;role&amp;#34;: &amp;#34;user&amp;#34;, &amp;#34;content&amp;#34;: &amp;#34;æŸ¥è¯¢2020å¹´å¤æ—¦å¤§å­¦çš„åˆ†æ•°çº¿&amp;#34; }, { &amp;#34;role&amp;#34;: &amp;#34;assistant&amp;#34;, &amp;#34;content&amp;#34;: null, &amp;#34;function_call&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;get_fudan_university_scoreline&amp;#34;, &amp;#34;arguments&amp;#34;: &amp;#34;{\n \&amp;#34;year\&amp;#34;: \&amp;#34;2020\&amp;#34;\n}&amp;#34; } }, { &amp;#34;role&amp;#34;: &amp;#34;function&amp;#34;, &amp;#34;name&amp;#34;: &amp;#34;get_fudan_university_scoreline&amp;#34;, &amp;#34;content&amp;#34;: &amp;#34;{\n \&amp;#34;scoreline\&amp;#34;:{\n \&amp;#34;æ–‡ç§‘ä¸€æ‰¹\&amp;#34;: 630, \n \&amp;#34;æ–‡ç§‘äºŒæ‰¹\&amp;#34;: 610, \n \&amp;#34;ç†ç§‘ä¸€æ‰¹\&amp;#34;: 650, \n \&amp;#34;ç†ç§‘äºŒæ‰¹\&amp;#34;: 630 \n }\n}&amp;#34; }, { &amp;#34;role&amp;#34;: &amp;#34;assistant&amp;#34;, &amp;#34;content&amp;#34;: &amp;#34;2020å¹´å¤æ—¦å¤§å­¦çš„åˆ†æ•°çº¿å¦‚ä¸‹ï¼š\n\n- æ–‡ç§‘ä¸€æ‰¹ï¼š630åˆ†\n- æ–‡ç§‘äºŒæ‰¹ï¼š610åˆ†\n- ç†ç§‘ä¸€æ‰¹ï¼š650åˆ†\n- ç†ç§‘äºŒæ‰¹ï¼š630åˆ†&amp;#34; } ] } ä¸Šè¿°Function Callçš„æ•°æ®æ ·ä¾‹ä¸ºç»™å®šç‰¹å®šå·¥å…·é›†åï¼Œç”¨äºå›ç­”ç”¨æˆ·æŸ¥è¯¢æŸé«˜æ ¡å½•å–åˆ†æ•°çº¿çš„é—®é¢˜ã€‚</description>
    </item>
    <item>
      <title></title>
      <link>/docs/devops_eval/tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/devops_eval/tutorial/</guid>
      <description>Evaluate Tutorial ğŸš€ How to Evaluate If you need to test your own huggingface-formatted model, the overall steps are as follows:&#xA;Write the loader function for the model. Write the context_builder function for the model. Register the model in the configuration file. Run the testing script. If the model does not require any special processing after loading, and the input does not need to be converted to a specific format (e.</description>
    </item>
    <item>
      <title>Abstract</title>
      <link>/docs/abstract/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/abstract/</guid>
      <description>Abstract With the increasing popularity of large-scale software development, the demand for scalable and adaptable static code analysis techniques is growing. Traditional static analysis tools such as Clang Static Analyzer (CSA) or PMD have shown good results in checking programming rules or style issues. However, these tools are often designed for specific objectives and are unable to meet the diverse and changing needs of modern software development environments. These needs may relate to Quality of Service (QoS), various programming languages, different algorithmic requirements, and various performance needs.</description>
    </item>
    <item>
      <title>Acknowledgements</title>
      <link>/contribution/acknowledgements/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/contribution/acknowledgements/</guid>
      <description>The documentation homepage of CodeFuse-ai is built on docura&#xA;The ChatBot project is based on langchain-chatchat and codebox-api.&#xA;&amp;hellip;&amp;hellip;&#xA;Deep gratitude is extended for their open-source contributions!</description>
    </item>
    <item>
      <title>Agent Flow</title>
      <link>/coagent/agent-flow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/coagent/agent-flow/</guid>
      <description>Introduction to Core Connectors To facilitate everyone&amp;rsquo;s understanding of the entire CoAgent link, we use a Flow format to detail how to build through configuration settings.&#xA;Below, we will first introduce the related core components&#xA;Agent At the design level of the Agent, we provide four basic types of Agents, which allows for the basic role settings of these Agents to meet the interaction and usage of a variety of common scenarios.</description>
    </item>
    <item>
      <title>Agent Flow</title>
      <link>/muagent/agent-flow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/muagent/agent-flow/</guid>
      <description>Introduction to Core Connectors To facilitate everyone&amp;rsquo;s understanding of the entire muagent link, we adopt the Flow format to introduce in detail how to build through configuration&#xA;Below, we first introduce the related core components&#xA;Agent On the design level of the Agent, we provide four basic types of Agents, with Role settings for these Agents that can meet the interactions and uses of various common scenarios:&#xA;BaseAgent: Provides basic question answering, tool usage, and code execution functions, and realizes input =&amp;gt; output according to the Prompt format.</description>
    </item>
    <item>
      <title>ChatBot-RoadMap</title>
      <link>/docs/chatbot-roadmap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/chatbot-roadmap/</guid>
      <description>ä¸­æ–‡&amp;nbsp ï½œ &amp;nbspEnglish&amp;nbsp RoadMap Roadmap Overview&#xA;Sandbox Environment âœ… Isolated sandbox environment for code execution âœ… File upload and download âœ… Support for Java execution environment â¬œ Vector Database &amp;amp; Retrieval âœ… Task retrieval âœ… Tool retrieval âœ… Prompt Management âœ… Memory Management âœ… Multi Agent Framework âœ… PRD (Product Requirement Document), system analysis, interface design â¬œ Generate code based on requirement documents, system analysis, and interface design â¬œ Automated testing, automated debugger â¬œ Operations process integration (ToolLearning) â¬œ Fully automated end-to-end process â¬œ Integration with LLM based on fastchat âœ… Integration with Text Embedding based on sentencebert âœ… Improved vector loading speed âœ… Connector âœ… React Mode based on langchain âœ… Tool retrieval completed with langchain âœ… General Capability for Web Crawl â¬œ Technical documentation: Zhihu, CSDN, Alibaba Cloud Developer Forum, Tencent Cloud Developer Forum, etc.</description>
    </item>
    <item>
      <title>CoAgent</title>
      <link>/coagent/coagent/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/coagent/coagent/</guid>
      <description>ç®€ä»‹ To enhance the performance of large language models (LLMs) in terms of inference accuracy, the industry has seen various innovative approaches to utilizing LLMs. From the earliest Chain of Thought (CoT), Text of Thought (ToT), to Graph of Thought (GoT), these methods have continually expanded the capability boundaries of LLMs. In dealing with complex problems, we can use the ReAct process to select, invoke, and execute tool feedback, achieving multi-round tool usage and multi-step execution.</description>
    </item>
    <item>
      <title>Codefuse-ChatBot Development by Private Knowledge Augmentation</title>
      <link>/docs/codefuse-chatbot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-chatbot/</guid>
      <description>ä¸­æ–‡&amp;nbsp ï½œ &amp;nbspEnglish&amp;nbsp This project is an open-source AI intelligent assistant, specifically designed for the entire lifecycle of software development, covering design, coding, testing, deployment, and operations. Through knowledge retrieval, tool utilization, and sandbox execution, Codefuse-ChatBot can not only answer professional questions you encounter during the development process but also coordinate multiple independent, dispersed platforms through a conversational interface.&#xA;ğŸ“œ Contents ğŸ¤ Introduction ğŸ§­ Technical Route ğŸ¤ Introduction ğŸ’¡ The aim of this project is to construct an AI intelligent assistant for the entire lifecycle of software development, covering design, coding, testing, deployment, and operations, through Retrieval Augmented Generation (RAG), Tool Learning, and sandbox environments.</description>
    </item>
    <item>
      <title>Codefuse-ChatBot Development by Private Knowledge Augmentation</title>
      <link>/docs/overview/codefuse-chatbot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/overview/codefuse-chatbot/</guid>
      <description>ä¸­æ–‡&amp;nbsp ï½œ &amp;nbspEnglish&amp;nbsp This project is an open-source AI intelligent assistant, specifically designed for the entire lifecycle of software development, covering design, coding, testing, deployment, and operations. Through knowledge retrieval, tool utilization, and sandbox execution, Codefuse-ChatBot can not only answer professional questions you encounter during the development process but also coordinate multiple independent, dispersed platforms through a conversational interface.&#xA;ğŸ“œ Contents ğŸ¤ Introduction ğŸ§­ Technical Route ğŸ¤ Introduction ğŸ’¡ The aim of this project is to construct an AI intelligent assistant for the entire lifecycle of software development, covering design, coding, testing, deployment, and operations, through Retrieval Augmented Generation (RAG), Tool Learning, and sandbox environments.</description>
    </item>
    <item>
      <title>codefuse-devops-eval</title>
      <link>/docs/codefuse-devops-eval/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-devops-eval/</guid>
      <description>Comming soon</description>
    </item>
    <item>
      <title>codefuse-devops-eval</title>
      <link>/docs/overview/codefuse-devops-eval/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/overview/codefuse-devops-eval/</guid>
      <description>DevOps-Eval is a comprehensive evaluation suite specifically designed for foundation models in the DevOps field. We hope DevOps-Eval could help developers, especially in the DevOps field, track the progress and analyze the important strengths/shortcomings of their models.&#xA;ğŸ“š This repo contains questions and exercises related to DevOps, including the AIOps, ToolLearning;&#xA;ğŸ’¥ï¸ There are currently 7486 multiple-choice questions spanning 8 diverse general categories, as shown below.&#xA;ğŸ”¥ There are a total of 2840 samples in the AIOps subcategory, covering scenarios such as log parsing, time series anomaly detection, time series classification, time series forecasting, and root cause analysis.</description>
    </item>
    <item>
      <title>codefuse-devops-model</title>
      <link>/docs/codefuse-devops-model/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-devops-model/</guid>
      <description>Comming soon</description>
    </item>
    <item>
      <title>codefuse-devops-model</title>
      <link>/docs/overview/codefuse-devops-model/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/overview/codefuse-devops-model/</guid>
      <description>codeFuse-devops-model DevOps-Model is a large language model for the Chinese DevOps field jointly released by Ant Group and Peking University. By collecting professional data related to the DevOps domain and conducting additional training and alignment on the model, a large model has been produced to help engineers enhance efficiency throughout the entire development and operations lifecycle. This fills the current gap in large models within the DevOps domain, with the aim to provide solutions to any problems by asking DevOps-Model!</description>
    </item>
    <item>
      <title>CodeFuse-MFT-VLM</title>
      <link>/docs/overview/codefuse-mft-vlm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/overview/codefuse-mft-vlm/</guid>
      <description>CodeFuse-VLM CodeFuse-VLM is a Multimodal LLM(MLLM) framework that provides users with multiple vision encoders, multimodal alignment adapters, and LLMs. Through CodeFuse-VLM framework, users are able to customize their own MLLM model to adapt their own tasks. As more and more models are published on Huggingface community, there will be more open-source vision encoders and LLMs. Each of these models has their own specialties, e.g. Code-LLama is good at code-related tasks but has poor performance for Chinese tasks.</description>
    </item>
    <item>
      <title>CodeFuse-ModelCache</title>
      <link>/docs/codefuse-modelcache/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-modelcache/</guid>
      <description>CodeFuse-ModelCache CodeFuse-ModelCache</description>
    </item>
    <item>
      <title>CodeFuse-ModelCache</title>
      <link>/docs/overview/codefuse-modelcache/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/overview/codefuse-modelcache/</guid>
      <description>ä¸­æ–‡ | English Contents news Introduction Modules Acknowledgements Contributing news ğŸ”¥ğŸ”¥[2023.12.10] we integrate LLM embedding frameworks such as &amp;rsquo;llmEmb&amp;rsquo;, &amp;lsquo;ONNX&amp;rsquo;, &amp;lsquo;PaddleNLP&amp;rsquo;, &amp;lsquo;FastText&amp;rsquo;, alone with the image embedding framework &amp;rsquo;timm&amp;rsquo;, to bolster embedding functionality. ğŸ”¥ğŸ”¥[2023.11.20] codefuse-ModelCache has integrated local storage, such as sqlite and faiss, providing users with the convenience of quickly initiating tests. [2023.08.26] codefuse-ModelCache&amp;hellip; Introduction Codefuse-ModelCache is a semantic cache for large language models (LLMs). By caching pre-generated model results, it reduces response time for similar requests and improves user experience.</description>
    </item>
    <item>
      <title>CodeFuse-Query</title>
      <link>/docs/codefuse-query/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-query/</guid>
      <description>CodeFuse-Query CodeFuse-Query</description>
    </item>
    <item>
      <title>CodeFuse-Query</title>
      <link>/docs/overview/codefuse-query/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/overview/codefuse-query/</guid>
      <description>CodeFuse-Query With the increasing popularity of large-scale software development, the demand for scalable and adaptable static code analysis techniques is growing. Traditional static analysis tools such as Clang Static Analyzer (CSA) or PMD have shown good results in checking programming rules or style issues. However, these tools are often designed for specific objectives and are unable to meet the diverse and changing needs of modern software development environments. These needs may relate to Quality of Service (QoS), various programming languages, different algorithmic requirements, and various performance needs.</description>
    </item>
    <item>
      <title>CodeFuseEval: Multi-tasking Evaluation Benchmark for Code Large Language Model</title>
      <link>/docs/codefuse-evalution/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-evalution/</guid>
      <description>CodeFuseEval: Multi-tasking Evaluation Benchmark for Code Large Language Model ç®€ä½“ä¸­æ–‡ï½œ&#xD;CodeFuseEval on ModelScopeï½œ&#xD;CodeFuseEval on Hugging Face&#xD;CodeFuseEval is a Code Generation benchmark that combines the multi-tasking scenarios of CodeFuse Model with the benchmarks of HumanEval-x and MBPP. This benchmark is designed to evaluate the performance of models in various multi-tasking tasks, including code completion, code generation from natural language, test case generation, cross-language code translation, and code generation from Chinese commands, among others.</description>
    </item>
    <item>
      <title>Connector Agent</title>
      <link>/coagent/connector-agent/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/coagent/connector-agent/</guid>
      <description>å¿«é€Ÿæ„å»ºä¸€ä¸ªAgent é¦–å…ˆå¢åŠ openaié…ç½®ï¼Œä¹Ÿå¯ä»¥æ˜¯å…¶å®ƒç±»ä¼¼äºopenaiæ¥å£çš„æ¨¡å‹ï¼ˆé€šè¿‡fastchatå¯åŠ¨ï¼‰ from coagent.base_configs.env_config import JUPYTER_WORK_PATH, KB_ROOT_PATH&#xD;from coagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from coagent.connector.configs import AGETN_CONFIGS&#xD;from coagent.connector.agents import BaseAgent&#xD;from coagent.connector.schema import Message, load_role_configs&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxx&amp;#34;&#xD;# os.environ[&amp;#34;OPENAI_PROXY&amp;#34;] = &amp;#34;socks5h://127.0.0.1:13659&amp;#34;&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; é…ç½®ç›¸å…³ LLM å’Œ Embedding Model # LLM å’Œ Embedding Model é…ç½®&#xD;llm_config = LLMConfig(&#xD;model_name=&amp;#34;gpt-3.5-turbo&amp;#34;, model_device=&amp;#34;cpu&amp;#34;,api_key=os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;], api_base_url=os.environ[&amp;#34;API_BASE_URL&amp;#34;], temperature=0.3&#xD;)&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=&amp;#34;text2vec-base-chinese&amp;#34;, embed_model_path=&amp;#34;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/embedding_models/text2vec-base-chinese&amp;#34;&#xD;) è¿™é‡Œä»å·²æœ‰çš„agenté…ç½®é€‰ä¸€ä¸ªroleæ¥åšç¤ºä¾‹ # ä»å·²æœ‰çš„é…ç½®ä¸­é€‰æ‹©ä¸€ä¸ªconfigï¼Œå…·ä½“å‚æ•°ç»†èŠ‚è§ä¸‹é¢&#xD;role_configs = load_role_configs(AGETN_CONFIGS)&#xD;agent_config = role_configs[&amp;#34;general_planner&amp;#34;]&#xD;# ç”Ÿæˆagentå®ä¾‹&#xD;base_agent = BaseAgent(&#xD;role=agent_config.</description>
    </item>
    <item>
      <title>Connector Agent</title>
      <link>/muagent/connector-agent/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/muagent/connector-agent/</guid>
      <description>Quickly Build an Agent First, add an OpenAI configuration, or a model with a similar interface to OpenAI (launched through fastchat) import os, sys&#xD;api_key = &amp;#34;sk-xxx&amp;#34;&#xD;api_base_url= &amp;#34;https://api.openai.com/v1&amp;#34;&#xD;model_name = &amp;#34;gpt-3.5-turbo&amp;#34;&#xD;embed_model = &amp;#34;{{embed_model_name}}&amp;#34;&#xD;embed_model_path = &amp;#34;{{embed_model_path}}&amp;#34;&#xD;#&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; Then Set LLM Configuration and Vector Model Configuration Configure related LLM and Embedding Model from muagent.base_configs.env_config import JUPYTER_WORK_PATH&#xD;from muagent.connector.agents import BaseAgent, ReactAgent, ExecutorAgent, SelectorAgent&#xD;from muagent.</description>
    </item>
    <item>
      <title>Connector Chain</title>
      <link>/coagent/connector-chain/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/coagent/connector-chain/</guid>
      <description>å¿«é€Ÿæ„å»ºä¸€ä¸ª agent chain é¦–å…ˆå¢åŠ openaié…ç½®ï¼Œä¹Ÿå¯ä»¥æ˜¯å…¶å®ƒç±»ä¼¼äºopenaiæ¥å£çš„æ¨¡å‹ï¼ˆé€šè¿‡fastchatå¯åŠ¨ï¼‰ # è®¾ç½®openaiçš„api-key&#xD;import os, sys&#xD;import openai&#xD;import importlib&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xxxx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxxx&amp;#34;&#xD;# os.environ[&amp;#34;OPENAI_PROXY&amp;#34;] = &amp;#34;socks5h://127.0.0.1:13659&amp;#34;&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; é…ç½®ç›¸å…³ LLM å’Œ Embedding Model # LLM å’Œ Embedding Model é…ç½®&#xD;llm_config = LLMConfig(&#xD;model_name=&amp;#34;gpt-3.5-turbo&amp;#34;, model_device=&amp;#34;cpu&amp;#34;,api_key=os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;], api_base_url=os.environ[&amp;#34;API_BASE_URL&amp;#34;], temperature=0.3&#xD;)&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=&amp;#34;text2vec-base-chinese&amp;#34;, embed_model_path=&amp;#34;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/embedding_models/text2vec-base-chinese&amp;#34;&#xD;) è¿™é‡Œä»å·²æœ‰çš„agenté…ç½®é€‰å¤šä¸ªroleç»„åˆæˆ agent chain from coagent.base_configs.env_config import JUPYTER_WORK_PATH, KB_ROOT_PATH&#xD;from coagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from coagent.</description>
    </item>
    <item>
      <title>Connector Chain</title>
      <link>/muagent/connector-chain/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/muagent/connector-chain/</guid>
      <description>Quickly Build an Agent First, add an OpenAI configuration, or a model with a similar interface to OpenAI (launched through fastchat) import os, sys&#xD;api_key = &amp;#34;sk-xxx&amp;#34;&#xD;api_base_url= &amp;#34;https://api.openai.com/v1&amp;#34;&#xD;model_name = &amp;#34;gpt-3.5-turbo&amp;#34;&#xD;embed_model = &amp;#34;{{embed_model_name}}&amp;#34;&#xD;embed_model_path = &amp;#34;{{embed_model_path}}&amp;#34;&#xD;#&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; Then Set LLM Configuration and Vector Model Configuration Configure related LLM and Embedding Model&#xA;from muagent.base_configs.env_config import JUPYTER_WORK_PATH&#xD;from muagent.connector.agents import BaseAgent, ReactAgent, ExecutorAgent, SelectorAgent&#xD;from muagent.</description>
    </item>
    <item>
      <title>Connector Memory</title>
      <link>/coagent/connector-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/coagent/connector-memory/</guid>
      <description>Memory Manager ä¸»è¦ç”¨äº chat history çš„ç®¡ç†ï¼Œæš‚æœªå®Œæˆ&#xA;å°†chat historyåœ¨æ•°æ®åº“è¿›è¡Œè¯»å†™ç®¡ç†ï¼ŒåŒ…æ‹¬user inputã€ llm outputã€doc retrievalã€code retrievalã€search retrieval å¯¹ chat history è¿›è¡Œå…³é”®ä¿¡æ¯æ€»ç»“ summary contextï¼Œä½œä¸º prompt context æä¾›æ£€ç´¢åŠŸèƒ½ï¼Œæ£€ç´¢ chat history æˆ–è€… summary context ä¸­ä¸é—®é¢˜ç›¸å…³ä¿¡æ¯ï¼Œè¾…åŠ©é—®ç­” ä½¿ç”¨ç¤ºä¾‹ åˆ›å»º memory manager å®ä¾‹ import os&#xD;import openai&#xD;from coagent.base_configs.env_config import KB_ROOT_PATH&#xD;from coagent.connector.memory_manager import BaseMemoryManager, LocalMemoryManager&#xD;from coagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from coagent.connector.schema import Message&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxx&amp;#34;&#xD;# os.environ[&amp;#34;OPENAI_PROXY&amp;#34;] = &amp;#34;socks5h://127.0.0.1:13659&amp;#34;&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.</description>
    </item>
    <item>
      <title>Connector Memory</title>
      <link>/muagent/connector-memory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/muagent/connector-memory/</guid>
      <description>Memory Manager Primarily used for managing chat history, not yet completed&#xA;Read and write chat history in the database, including user input, llm output, doc retrieval, code retrieval, search retrieval. Summarize key information from the chat history into a summary context, serving as a prompt context. Provide a search function to retrieve information related to the question from chat history or summary context, aiding in Q&amp;amp;A. Usage Example Create memory manager instance import os&#xD;import openai&#xD;from coagent.</description>
    </item>
    <item>
      <title>Connector Phase</title>
      <link>/coagent/connector-phase/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/coagent/connector-phase/</guid>
      <description>å¿«é€Ÿæ„å»ºä¸€ä¸ª agent phase é¦–å…ˆå¢åŠ openaié…ç½®ï¼Œä¹Ÿå¯ä»¥æ˜¯å…¶å®ƒç±»ä¼¼äºopenaiæ¥å£çš„æ¨¡å‹ï¼ˆé€šè¿‡fastchatå¯åŠ¨ï¼‰ from coagent.base_configs.env_config import JUPYTER_WORK_PATH, KB_ROOT_PATH&#xD;from coagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from coagent.connector.configs import AGETN_CONFIGS&#xD;from coagent.connector.phase import BasePhase&#xD;from coagent.connector.schema import Message, load_role_configs&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxx&amp;#34;&#xD;# os.environ[&amp;#34;OPENAI_PROXY&amp;#34;] = &amp;#34;socks5h://127.0.0.1:13659&amp;#34;&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; é…ç½®ç›¸å…³ LLM å’Œ Embedding Model # LLM å’Œ Embedding Model é…ç½®&#xD;llm_config = LLMConfig(&#xD;model_name=&amp;#34;gpt-3.5-turbo&amp;#34;, model_device=&amp;#34;cpu&amp;#34;,api_key=os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;], api_base_url=os.environ[&amp;#34;API_BASE_URL&amp;#34;], temperature=0.3&#xD;)&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=&amp;#34;text2vec-base-chinese&amp;#34;, embed_model_path=&amp;#34;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/embedding_models/text2vec-base-chinese&amp;#34;&#xD;) è¿™é‡Œä»å·²æœ‰çš„ phase é…ç½®ä¸­é€‰ä¸€ä¸ª phase æ¥åšç¤ºä¾‹ # log-levelï¼Œprint promptå’Œllm predict&#xD;os.</description>
    </item>
    <item>
      <title>Connector Phase</title>
      <link>/muagent/connector-phase/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/muagent/connector-phase/</guid>
      <description>Quickly Build an Agent Phase First, add OpenAI configuration, which can be models with similar interfaces to OpenAI (triggered via fastchat). import os, sys&#xD;api_key = &amp;#34;sk-xxx&amp;#34;&#xD;api_base_url= &amp;#34;https://api.openai.com/v1&amp;#34;&#xD;model_name = &amp;#34;gpt-3.5-turbo&amp;#34;&#xD;embed_model = &amp;#34;{{embed_model_name}}&amp;#34;&#xD;embed_model_path = &amp;#34;{{embed_model_path}}&amp;#34;&#xD;#&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; Then Set LLM Configuration and Vector Model Configuration Configure related LLM and Embedding Model. from muagent.base_configs.env_config import JUPYTER_WORK_PATH&#xD;from muagent.connector.agents import BaseAgent, ReactAgent, ExecutorAgent, SelectorAgent&#xD;from muagent.</description>
    </item>
    <item>
      <title>Connector Prompt</title>
      <link>/coagent/connector-prompt/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/coagent/connector-prompt/</guid>
      <description>Prompt çš„æ ‡å‡†ç»“æ„ åœ¨æ•´ä¸ªPromptçš„æ•´ä¸ªç»“æ„ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å»å®šä¹‰ä¸‰ä¸ªéƒ¨åˆ†&#xA;Agent Profil Input Format Response Output Format #### Agent Profile&#xD;Agent Description ...&#xD;#### Input Format&#xD;**Origin Query:** the initial question or objective that the user wanted to achieve&#xD;**Context:** the current status and history of the tasks to determine if Origin Query has been achieved.&#xD;#### Response Output Format&#xD;**Action Status:** finished or continued&#xD;If it&amp;#39;s &amp;#39;finished&amp;#39;, the context can answer the origin query.&#xD;If it&amp;#39;s &amp;#39;continued&amp;#39;, the context cant answer the origin query.</description>
    </item>
    <item>
      <title>Connector Prompt</title>
      <link>/muagent/connector-prompt/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/muagent/connector-prompt/</guid>
      <description>Prompt Manager Managing prompt creation in multi-agent linkages&#xA;Quick Configuration: Utilizing preset processing functions, users can easily configure by simply defining the inputs and outputs of the agents, enabling fast assembly and configuration of multi-agent prompts. Customization Support: Allows users to customize the internal processing logic of each module within the prompt to achieve personalized implementation of the agent prompt. Preset Template Structure for Prompts Agent Profile: This section involves the basic description of the agent, including but not limited to the type of agent, its functions, and command set.</description>
    </item>
    <item>
      <title>Contribution Guide</title>
      <link>/contribution/contribution-guide/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/contribution/contribution-guide/</guid>
      <description>ä¸­æ–‡&amp;nbsp ï½œ &amp;nbspEnglish&amp;nbsp Thank you for your interest in the Codefuse project. We warmly welcome any suggestions, opinions (including criticisms), comments, and contributions to the Codefuse project.&#xA;Your suggestions, opinions, and comments on Codefuse can be directly submitted through GitHub Issues.&#xA;There are many ways to participate in the Codefuse project and contribute to it: code implementation, test writing, process tool improvement, documentation enhancement, and more. We welcome any contributions and will add you to our list of contributors.</description>
    </item>
    <item>
      <title>Custom Retrieval</title>
      <link>/muagent/custom-retrieval/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/muagent/custom-retrieval/</guid>
      <description>Basic Introduction Doc Retrieval is the document vector database, which is the most mainstream method for knowledge base construction nowadays. It uses Text Embedding models to vectorize documents and stores them in a vector database. In the future, we will also support querying based on knowledge graph and automatically extracting entities and relationships through large models to explore various complex relationships in data.&#xA;Code Retrieval LLM faces challenges in tasks such as code generation, repair, and component understanding, including lagging code training data and the inability to perceive the dependency structure of code context.</description>
    </item>
    <item>
      <title>Custom Tool</title>
      <link>/muagent/custom-tool/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/muagent/custom-tool/</guid>
      <description>Introduction In MuAgent, it also supports the registration of Tools by Agents. By registering the BaseToolModel class with Python and writing&#xA;Tool_name Tool_description ToolInputArgs ToolOutputArgs run and other relevant properties and methods, the quick integration of tools can be achieved. It also supports the direct use of the langchain Tool interface. For example, functions like the aforementioned XXRetrieval can also be registered as a Tool, to be ultimately called by an LLM.</description>
    </item>
    <item>
      <title>Customed Examples</title>
      <link>/coagent/customed-examples/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/coagent/customed-examples/</guid>
      <description>å¦‚ä½•åˆ›å»ºä½ ä¸ªæ€§åŒ–çš„ agent phase åœºæ™¯ ä¸‹é¢é€šè¿‡ autogen çš„ auto_feedback_from_code_execution æ„å»ºè¿‡æ¥ï¼Œæ¥è¯¦ç»†æ¼”ç¤ºå¦‚ä½•è‡ªå®šä¹‰ä¸€ä¸ª agent phase çš„æ„å»º&#xA;è®¾è®¡ä½ çš„promptç»“æ„ import os, sys, requests&#xD;# from configs.model_config import *&#xD;from coagent.connector.phase import BasePhase&#xD;from coagent.connector.chains import BaseChain&#xD;from coagent.connector.schema import Message&#xD;from coagent.connector.configs import AGETN_CONFIGS, CHAIN_CONFIGS, PHASE_CONFIGS&#xD;import importlib&#xD;# update new agent configs&#xD;auto_feedback_from_code_execution_PROMPT = &amp;#34;&amp;#34;&amp;#34;#### Agent Profile&#xD;You are a helpful AI assistant. Solve tasks using your coding and language skills.&#xD;In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.</description>
    </item>
    <item>
      <title>Customed Examples</title>
      <link>/muagent/custom-examples/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/muagent/custom-examples/</guid>
      <description>How to Create Your Personalized Agent Phase Scenario Below we will use a code repository to demonstrate the automatic generation of API documentation from code, detailing how to customize the construction of an agent phase.&#xA;Design Your Prompt Structure codeGenDocGroup_PROMPT, create group Agent Prompt # update new agent configs&#xD;codeGenDocGroup_PROMPT = &amp;#34;&amp;#34;&amp;#34;#### Agent Profile&#xD;Your goal is to response according the Context Data&amp;#39;s information with the role that will best facilitate a solution, taking into account all relevant context (Context) provided.</description>
    </item>
    <item>
      <title>Data</title>
      <link>/docs/data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/data/</guid>
      <description>â¬ Data Download Method 1: Download the zip file (you can also simply open the following link with the browser): wget https://huggingface.co/datasets/codefuse-admin/devopseval-exam/resolve/main/devopseval-exam.zip then unzip it and you may load the data with pandas: import os import pandas as pd File_Dir=&amp;#34;devopseval-exam&amp;#34; test_df=pd.read_csv(os.path.join(File_Dir,&amp;#34;test&amp;#34;,&amp;#34;UnitTesting.csv&amp;#34;)) Method 2: Directly load the dataset using Hugging Face datasets: from datasets import load_dataset dataset=load_dataset(r&amp;#34;DevOps-Eval/devopseval-exam&amp;#34;,name=&amp;#34;UnitTesting&amp;#34;) print(dataset[&amp;#39;val&amp;#39;][0]) # {&amp;#34;id&amp;#34;: 1, &amp;#34;question&amp;#34;: &amp;#34;å•å…ƒæµ‹è¯•åº”è¯¥è¦†ç›–ä»¥ä¸‹å“ªäº›æ–¹é¢ï¼Ÿ&amp;#34;, &amp;#34;A&amp;#34;: &amp;#34;æ­£å¸¸è·¯å¾„&amp;#34;, &amp;#34;B&amp;#34;: &amp;#34;å¼‚å¸¸è·¯å¾„&amp;#34;, &amp;#34;C&amp;#34;: &amp;#34;è¾¹ç•Œå€¼æ¡ä»¶&amp;#34;ï¼Œ&amp;#34;D&amp;#34;: æ‰€æœ‰ä»¥ä¸Šï¼Œ&amp;#34;answer&amp;#34;: &amp;#34;D&amp;#34;, &amp;#34;explanation&amp;#34;: &amp;#34;&amp;#34;} ``` ğŸ‘€ Notes To facilitate usage, we have organized the category name handlers and English/Chinese names corresponding to 55 subcategories.</description>
    </item>
    <item>
      <title>Embedding Config</title>
      <link>/muagent/embedding-model-config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/muagent/embedding-model-config/</guid>
      <description>Prepare Relevant Parameters First, add the OpenAI configuration; this could also be a model similar to the OpenAI interface (launched via fastchat).&#xA;import os, sys&#xD;api_key = &amp;#34;sk-xxx&amp;#34;&#xD;api_base_url= &amp;#34;https://api.openai.com/v1&amp;#34;&#xD;embed_model = &amp;#34;{{embed_model_name}}&amp;#34;&#xD;embed_model_path = &amp;#34;{{embed_model_path}}&amp;#34; Build LLM Config Constructing with a local model file from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=embed_model, embed_model_path=embed_model_path&#xD;) Constructing via OpenAI from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;openai&amp;#34;, api_key=api_key, api_base_url=api_base_url,&#xD;) Customizing and inputting langchain embeddings from muagent.</description>
    </item>
    <item>
      <title>Evaluate</title>
      <link>/docs/codefuse-devops-eval-quickstart/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-devops-eval-quickstart/</guid>
      <description>ğŸš€ How to Evaluate If you need to test your own huggingface-formatted model, the overall steps are as follows:&#xA;Write the loader function for the model. Write the context_builder function for the model. Register the model in the configuration file. Run the testing script. If the model does not require any special processing after loading, and the input does not need to be converted to a specific format (e.g. chatml format or other human-bot formats), you can directly proceed to step 4 to initiate the testing.</description>
    </item>
    <item>
      <title>FasterTransformer4CodeFuse</title>
      <link>/docs/fastertransformer4codefuse/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/fastertransformer4codefuse/</guid>
      <description>FasterTransformer4CodeFuse FasterTransformer4CodeFuse</description>
    </item>
    <item>
      <title>FasterTransformer4CodeFuse</title>
      <link>/docs/overview/fastertransformer4codefuse/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/overview/fastertransformer4codefuse/</guid>
      <description>FasterTransformer4CodeFuse FasterTransformer4CodeFuse</description>
    </item>
    <item>
      <title>Feature</title>
      <link>/docs/codefuse-modelcache-feature/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-modelcache-feature/</guid>
      <description>From a functional standpoint, to address Huggingface network issues and improve inference speed, local inference capabilities for embeddings have been added. Given some limitations in the SQLAlchemy framework, we have rewritten the relational database interaction module for more flexible database operations. In practice, large model products need to interface with multiple users and models; thus, support for multi-tenancy has been added to ModelCache, as well as preliminary compatibility with system commands and multi-turn conversations.</description>
    </item>
    <item>
      <title>GodelLanguage</title>
      <link>/docs/codefuse-query-godellanguage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-query-godellanguage/</guid>
      <description>GÃ¶delScript Query Language Index GÃ¶delScript Basic Concepts and Syntax Introduction Basic Program Structure Fundamental Types and Compiler Built-in Functions Functions Statements Schema Database Trait Import Query Ungrounded Error: Unassigned/Unbound Error Query Examples Java Python JavaScript XML Go Query Debugging and Optimization Tips Schema Arguments Causing Excessively Large Cartesian Products Multiple Layers of for Causing Excessively Large Cartesian Products Avoid Misusing @inline and Strategies for Necessary Inline Optimization Using Query Scripts on a Local Machine Basic Concepts and Syntax of GÃ¶delScript Introduction // script fn hello(greeting: string) -&amp;gt; bool { return greeting = &amp;#34;hello world!</description>
    </item>
    <item>
      <title>How to better configure your cache</title>
      <link>/docs/codefuse-modelcache-config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-modelcache-config/</guid>
      <description>Environment Dependencies Python version: 3.8 or higher To install dependencies: pip install requirements.txt Service Startup Before starting the service, the following environment configurations should be performed: Install relational database MySQL, import SQL to create tables, SQL file: reference_doc/create_table.sql Install vector database Milvus Add database access information to the configuration files, which are: modelcache/config/milvus_config.ini modelcache/config/mysql_config.ini Download offline model bin files, refer to: https://huggingface.co/shibing624/text2vec-base-chinese/tree/main, and place the downloaded bin files into the model/text2vec-base-chinese folder Start the backend service using the flask4modelcache.</description>
    </item>
    <item>
      <title>Introduction</title>
      <link>/docs/codefuse-query-introduction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-query-introduction/</guid>
      <description>Introduction CodeFuse-Query is a code data platform that supports structured analysis of various programming languages. The core idea is to transform all code into data using various language parsers and to store this data in a structured format within a code database. Data analysis is then performed according to business needs using a custom query language, as shown in the diagram below: 2.1 Architecture of CodeFuse-Query Overall, the CodeFuse-Query code data platform is divided into three main parts: the code data model, the code query DSL (Domain-Specific Language), and platform productization services.</description>
    </item>
    <item>
      <title>Introduction</title>
      <link>/docs/mftcoder-introduction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/mftcoder-introduction/</guid>
      <description>Introduction High Accuracy and efficiency Multi-task Fine-tuning framework for Code LLMs.&#xA;MFTCoder is an open-source project of CodeFuse for accurate and efficient Multi-task Fine-tuning(MFT) on Large Language Models(LLMs), especially on Code-LLMs(large language model for code tasks). Moreover, we open source Code LLM models and code-related datasets along with the MFTCoder framework.&#xA;In MFTCoder, we released two codebases for finetuning Large Language Models:&#xA;MFTCoder-accelerate is a framework with accelerate and DeepSpeed/FSDP. All tech-stacks are open-source and vibrant.</description>
    </item>
    <item>
      <title>Issue Report</title>
      <link>/contribution/issue-report/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/contribution/issue-report/</guid>
      <description>ä¸­æ–‡&amp;nbsp ï½œ &amp;nbspEnglish&amp;nbsp Issue Type Issues can be categorized into three types:&#xA;Bug: Issues where code or execution examples contain bugs or lack dependencies, resulting in incorrect execution. Documentation: Discrepancies in documentation, inconsistencies between documentation content and code, etc. Feature: New functionalities that evolve from the current codebase. Issue Template Issue: Bug Template Checklist before submitting an issue Please confirm that you have checked the document, issues, discussions (GitHub feature), and other publicly available documentation.</description>
    </item>
    <item>
      <title>LLM Config</title>
      <link>/muagent/llm-model-config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/muagent/llm-model-config/</guid>
      <description>Prepare Relevant Parameters First, add the OpenAI configuration, or you can use another model similar to the OpenAI interface (launched through fastchat).&#xA;import os, sys&#xD;api_key = &amp;#34;sk-xxx&amp;#34;&#xD;api_base_url= &amp;#34;https://api.openai.com/v1&amp;#34;&#xD;model_name = &amp;#34;gpt-3.5-turbo&amp;#34; Build LLM Config By passing the class openai from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;llm_config = LLMConfig(&#xD;model_name=model_name, api_key=api_key, api_base_url=api_base_url, temperature=0.3,&#xD;stop=&amp;#34;**Observation:**&amp;#34;&#xD;) Customizing and inputting langchain LLM from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from langchain.llms.base import BaseLLM, LLM&#xD;class CustomizedModel(LLM):&#xD;repetition_penalty = 1.</description>
    </item>
    <item>
      <title>LLM-Configuration</title>
      <link>/docs/LLM-Configuration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/LLM-Configuration/</guid>
      <description>ä¸­æ–‡&amp;nbsp ï½œ &amp;nbspEnglish&amp;nbsp Local Privatization/Large Model Interface Access Leveraging open-source LLMs (Large Language Models) and Embedding models, this project enables offline private deployment based on open-source models.&#xA;In addition, the project supports the invocation of OpenAI API.&#xA;Local Privatization Model Access Example of model address configuration, modification of the model_config.py configuration:&#xA;# Recommendation: Use Hugging Face models, preferably the chat models, and avoid using base models, which may not produce correct outputs.</description>
    </item>
    <item>
      <title>MFTCoder</title>
      <link>/docs/mftcoder/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/mftcoder/</guid>
      <description>MFTCoder MFTCoder</description>
    </item>
    <item>
      <title>MFTCoder Training: Atorch Framework</title>
      <link>/docs/mftcoder-atorch/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/mftcoder-atorch/</guid>
      <description>[ä¸­æ–‡] [English]&#xA;1. Updates ğŸ”¥ MFTCoder supports fine-tuning of the GPTNeoX model under the Atorch framework.&#xA;ğŸ”¥ MFTCoder supports both fully supervised fine-tuning.&#xA;ğŸ”¥ MFTCoder supports LoRA using the Atorch Framework.&#xA;2. Data Format 2.1 Training Data Format The training data is in a uniformed JSONL format, in which each line of data has the following JSON format. The &amp;ldquo;chat_rounds&amp;rdquo; field is required, and other fields can be added or removed based on the specific need.</description>
    </item>
    <item>
      <title>MFTCoder-accelerate: Training Framework with Accelerate and DeepSpeed/FSDP</title>
      <link>/docs/mftcoder-accelerate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/mftcoder-accelerate/</guid>
      <description>[ä¸­æ–‡] [English]&#xA;1. Updates ğŸ”¥ MFTCoder-accelerate supports Full-parameters/LoRA using accelerate + FSDP Framework;&#xA;ğŸ”¥ MFTCoder-accelerate supports MFT/SFT on more new mainstream open-source base models: mistral, mixtral-8x7b(Mixture of Experts), deepseek, chatglm3;&#xA;ğŸ”¥ MFTCoder-accelerate supports Self-Paced Loss for Convergence Balance;&#xA;ğŸ”¥ MFTCoder-accelerate supports Full-parameters/QLoRA/LoRA using accelerate + DeepSpeed Framework;&#xA;ğŸ”¥ MFTCoder-accelerate supports Multitask Fine-Tuning(MFT), which is able to balance diffenrent tasks in data level.&#xA;ğŸ”¥ MFTCoder-accelerate supports finetuning most of mainstream open-source base models: codellama, llama2, llama, starcoder, codegeex2, chatglm2, qwen.</description>
    </item>
    <item>
      <title>MFTCoder: High Accuracy and Efficiency Multi-task Fine-Tuning Framework</title>
      <link>/docs/overview/mftcoder/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/overview/mftcoder/</guid>
      <description>ğŸ¤— HuggingFace â€¢ ğŸ¤– ModelScope [ä¸­æ–‡] [English]&#xA;Contents News Articles Introduction Requirements Training Models Datasets Star History News ğŸ”¥ğŸ”¥ğŸ”¥ [2024/01/17] We released MFTCoder v0.3.0, mainly for MFTCoder-accelerate. It now supports new models like Mixtral(MoE), DeepSeek-coder, chatglm3. It supports FSDP as an option. It also supports Self-paced Loss as a solution for convergence balance in Multitask Fine-tuning.&#xA;ğŸ”¥ğŸ”¥ğŸ”¥ [2024/01/17] CodeFuse-DeepSeek-33B has been released, achieving a pass@1 (greedy decoding) score of 78.</description>
    </item>
    <item>
      <title>MuAgent</title>
      <link>/muagent/muagent/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/muagent/muagent/</guid>
      <description>Introduction To enhance the performance of large models in terms of inference accuracy, various innovative Large Language Model (LLM) playbooks have emerged in the industry. From the earliest Chain of Thought (CoT) and Thread of Thought (ToT) to Games on Tracks (GoT), these methods have continually expanded the capability boundaries of LLMs. When handling complex problems, we can select, invoke and execute tool feedback through the ReAct process, while realizing multi-round tool use and multi-step execution.</description>
    </item>
    <item>
      <title>overview</title>
      <link>/docs/en_overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/en_overview/</guid>
      <description>HuggingFace | ModelScope Hello World! This is CodeFuse! CodeFuse aims to develop Code Large Language Models (Code LLMs) to support and enhance full-lifecycle AI native sotware developing, covering crucial stages such as design requirements, coding, testing, building, deployment, operations, and insight analysis.&#xA;We are passionating about creating innovative open-source solutions that empower developers throughout the software development process as shown above. We also encourage engineers and researchers within this community to join us in co-constructing/improving CodeFuse.</description>
    </item>
    <item>
      <title>Prompt Manager</title>
      <link>/coagent/prompt-manager/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/coagent/prompt-manager/</guid>
      <description>æç¤ºç®¡ç†å™¨ï¼ˆPrompt Managerï¼‰ ç®¡ç†å¤šæ™ºèƒ½ä½“é“¾è·¯ä¸­çš„promptåˆ›å»º&#xA;å¿«é€Ÿé…ç½®ï¼šé‡‡ç”¨é¢„è®¾çš„å¤„ç†å‡½æ•°ï¼Œç”¨æˆ·ä»…éœ€é€šè¿‡å®šä¹‰æ™ºèƒ½ä½“çš„è¾“å…¥è¾“å‡ºå³å¯è½»æ¾é…ç½®ï¼Œå®ç°å¤šæ™ºèƒ½ä½“çš„promptå¿«é€Ÿç»„è£…å’Œé…ç½®ã€‚ è‡ªå®šä¹‰æ”¯æŒï¼šå…è®¸ç”¨æˆ·è‡ªå®šä¹‰promptå†…éƒ¨å„æ¨¡å—çš„å¤„ç†é€»è¾‘ï¼Œä»¥è¾¾åˆ°ä¸ªæ€§åŒ–çš„æ™ºèƒ½ä½“promptå®ç°ã€‚ Prompté¢„è®¾æ¨¡æ¿ç»“æ„ Agent Profileï¼šæ­¤éƒ¨åˆ†æ¶‰åŠåˆ°æ™ºèƒ½ä½“çš„åŸºç¡€æè¿°ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºä»£ç†çš„ç±»å‹ã€åŠŸèƒ½å’ŒæŒ‡ä»¤é›†ã€‚ç”¨æˆ·å¯ä»¥åœ¨è¿™é‡Œè®¾ç½®æ™ºèƒ½ä½“çš„åŸºæœ¬å±æ€§ï¼Œç¡®ä¿å…¶è¡Œä¸ºä¸é¢„æœŸç›¸ç¬¦ã€‚ Contextï¼šä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç»™æ™ºèƒ½ä½“åšå‚è€ƒï¼Œå¸®åŠ©æ™ºèƒ½ä½“æ›´å¥½çš„è¿›è¡Œå†³ç­–ã€‚ Tool Informationï¼šæ­¤éƒ¨åˆ†ä¸ºæ™ºèƒ½ä½“æä¾›äº†ä¸€å¥—å¯ç”¨å·¥å…·çš„æ¸…å•ï¼Œæ™ºèƒ½ä½“å¯ä»¥æ ¹æ®å½“å‰çš„åœºæ™¯éœ€æ±‚ä»ä¸­æŒ‘é€‰åˆé€‚çš„å·¥å…·ä»¥è¾…åŠ©å…¶æ‰§è¡Œä»»åŠ¡ã€‚ Reference Documentsï¼šè¿™é‡Œå¯ä»¥åŒ…å«ä»£ç†å‚è€ƒä½¿ç”¨çš„æ–‡æ¡£æˆ–ä»£ç ç‰‡æ®µï¼Œä»¥ä¾¿äºå®ƒåœ¨å¤„ç†è¯·æ±‚æ—¶èƒ½å¤Ÿå‚ç…§ç›¸å…³èµ„æ–™ã€‚ Session Recordsï¼šåœ¨è¿›è¡Œå¤šè½®å¯¹è¯æ—¶ï¼Œæ­¤éƒ¨åˆ†ä¼šè®°å½•ä¹‹å‰çš„äº¤è°ˆå†…å®¹ï¼Œç¡®ä¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨ä¸Šä¸‹æ–‡ä¸­ä¿æŒè¿è´¯æ€§ã€‚ Response Output Formatï¼šç”¨æˆ·å¯ä»¥åœ¨æ­¤è®¾ç½®æ™ºèƒ½ä½“çš„è¾“å‡ºæ ¼å¼ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„å“åº”æ»¡è¶³ç‰¹å®šçš„æ ¼å¼è¦æ±‚ï¼ŒåŒ…æ‹¬ç»“æ„ã€è¯­æ³•ç­‰ã€‚ Responseï¼šåœ¨ä¸æ™ºèƒ½ä½“çš„å¯¹è¯ä¸­ï¼Œå¦‚æœç”¨æˆ·å¸Œæœ›æ™ºèƒ½ä½“ç»§ç»­æŸä¸ªè¯é¢˜æˆ–å†…å®¹ï¼Œå¯ä»¥åœ¨æ­¤æ¨¡å—ä¸­è¾“å…¥ç»­å†™çš„ä¸Šæ–‡ã€‚ä¾‹å¦‚ï¼Œåœ¨è¿ç”¨REACTæ¨¡å¼æ—¶ï¼Œå¯ä»¥åœ¨æ­¤åŒºåŸŸå†…è¯¦ç»†é˜è¿°æ™ºèƒ½ä½“å…ˆå‰çš„è¡Œä¸ºå’Œè§‚å¯Ÿç»“æœï¼Œä»¥ä¾¿äºæ™ºèƒ½ä½“æ„å»ºè¿è´¯çš„åç»­å“åº”ã€‚ Promptè‡ªå®šä¹‰é…ç½® Promptæ¨¡å—å‚æ•° field_nameï¼šå”¯ä¸€çš„å­—æ®µåç§°æ ‡è¯†ï¼Œå¿…é¡»æä¾›ã€‚ functionï¼šæŒ‡å®šå¦‚ä½•å¤„ç†è¾“å…¥æ•°æ®çš„å‡½æ•°ï¼Œå¿…é¡»æä¾›ã€‚ titleï¼šå®šä¹‰æ¨¡å—çš„æ ‡é¢˜ã€‚è‹¥æœªæä¾›ï¼Œå°†è‡ªåŠ¨ç”Ÿæˆä¸€ä¸ªæ ‡é¢˜ï¼Œè¯¥æ ‡é¢˜é€šè¿‡æŠŠå­—æ®µåç§°ä¸­çš„ä¸‹åˆ’çº¿æ›¿æ¢ä¸ºç©ºæ ¼å¹¶å°†æ¯ä¸ªå•è¯çš„é¦–å­—æ¯å¤§å†™æ¥æ„å»ºã€‚ descriptionï¼šæä¾›æ¨¡å—çš„ç®€è¦æè¿°ï¼Œä½äºæ¨¡å—æœ€ä¸Šæ–¹ï¼ˆæ ‡é¢˜ä¸‹æ–¹ï¼‰ã€‚é»˜è®¤ä¸ºç©ºï¼Œå¯é€‰å¡«ã€‚ is_contextï¼šæ ‡è¯†è¯¥å­—æ®µæ˜¯å¦å±äºä¸Šä¸‹æ–‡æ¨¡å—çš„ä¸€éƒ¨åˆ†ã€‚é»˜è®¤ä¸ºTrueï¼Œæ„å‘³ç€é™¤éæ˜¾å¼æŒ‡å®šä¸ºFalseï¼Œå¦åˆ™éƒ½è¢«è§†ä¸ºä¸Šä¸‹æ–‡çš„ä¸€éƒ¨åˆ†ã€‚ omit_if_emptyï¼šè®¾å®šå½“æ¨¡å—å†…å®¹ä¸ºç©ºæ—¶ï¼Œæ˜¯å¦åœ¨promptä¸­çœç•¥è¯¥æ¨¡å—ï¼Œå³ä¸æ˜¾ç¤ºç›¸åº”çš„æ¨¡æ¿æ ‡é¢˜å’Œå†…å®¹ã€‚é»˜è®¤ä¸ºFalseï¼Œæ„å‘³ç€å³ä½¿å†…å®¹ä¸ºç©ºä¹Ÿä¼šæ˜¾ç¤ºæ ‡é¢˜ã€‚å¦‚æœå¸Œæœ›å†…å®¹ä¸ºç©ºæ—¶çœç•¥æ¨¡å—ï¼Œéœ€æ˜¾å¼è®¾ç½®ä¸ºTrueã€‚ Prompté…ç½®ç¤ºä¾‹ Prompté…ç½®ç”±ä¸€ç³»åˆ—å®šä¹‰promptæ¨¡å—çš„å­—å…¸ç»„æˆï¼Œè¿™äº›æ¨¡å—å°†æ ¹æ®æŒ‡å®šçš„å‚æ•°å’ŒåŠŸèƒ½æ¥å¤„ç†è¾“å…¥æ•°æ®å¹¶ç»„ç»‡æˆä¸€ä¸ªå®Œæ•´çš„promptã€‚&#xA;åœ¨é…ç½®ä¸­ï¼Œæ¯ä¸ªå­—å…¸ä»£è¡¨ä¸€ä¸ªæ¨¡å—ï¼Œå…¶ä¸­åŒ…å«ç›¸å…³çš„å‚æ•°å¦‚ field_name, function_name, is_context, title, description, å’Œ omit_if_emptyï¼Œç”¨ä»¥æ§åˆ¶æ¨¡å—çš„è¡Œä¸ºå’Œå‘ˆç°æ–¹å¼ã€‚&#xA;context_placeholder å­—æ®µç”¨äºæ ‡è¯†ä¸Šä¸‹æ–‡æ¨¡æ¿çš„ä½ç½®ï¼Œå…è®¸åœ¨promptä¸­æ’å…¥åŠ¨æ€å†…å®¹ã€‚&#xA;[ {&amp;#34;field_name&amp;#34;: &amp;#39;agent_profile&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_agent_profile&amp;#39;, &amp;#34;is_context&amp;#34;: False}, {&amp;#34;field_name&amp;#34;: &amp;#39;context_placeholder&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;&amp;#39;, &amp;#34;is_context&amp;#34;: True}, {&amp;#34;field_name&amp;#34;: &amp;#39;tool_information&amp;#39;,&amp;#34;function_name&amp;#34;: &amp;#39;handle_tool_data&amp;#39;, &amp;#34;is_context&amp;#34;: True}, {&amp;#34;field_name&amp;#34;: &amp;#39;reference_documents&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_doc_info&amp;#39;}, {&amp;#34;field_name&amp;#34;: &amp;#39;session_records&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_session_records&amp;#39;}, {&amp;#34;field_name&amp;#34;: &amp;#39;task_records&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_task_records&amp;#39;}, {&amp;#34;field_name&amp;#34;: &amp;#39;output_format&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_output_format&amp;#39;, &amp;#39;title&amp;#39;: &amp;#39;Response Output Format&amp;#39;, &amp;#34;is_context&amp;#34;: False}, {&amp;#34;field_name&amp;#34;: &amp;#39;response&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_response&amp;#39;, &amp;#34;title&amp;#34;=&amp;#34;begin!</description>
    </item>
    <item>
      <title>Prompt Manager</title>
      <link>/coagent/prompt-manager/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/coagent/prompt-manager/</guid>
      <description>æç¤ºç®¡ç†å™¨ï¼ˆPrompt Managerï¼‰ ç®¡ç†å¤šæ™ºèƒ½ä½“é“¾è·¯ä¸­çš„promptåˆ›å»º&#xA;å¿«é€Ÿé…ç½®ï¼šé‡‡ç”¨é¢„è®¾çš„å¤„ç†å‡½æ•°ï¼Œç”¨æˆ·ä»…éœ€é€šè¿‡å®šä¹‰æ™ºèƒ½ä½“çš„è¾“å…¥è¾“å‡ºå³å¯è½»æ¾é…ç½®ï¼Œå®ç°å¤šæ™ºèƒ½ä½“çš„promptå¿«é€Ÿç»„è£…å’Œé…ç½®ã€‚ è‡ªå®šä¹‰æ”¯æŒï¼šå…è®¸ç”¨æˆ·è‡ªå®šä¹‰promptå†…éƒ¨å„æ¨¡å—çš„å¤„ç†é€»è¾‘ï¼Œä»¥è¾¾åˆ°ä¸ªæ€§åŒ–çš„æ™ºèƒ½ä½“promptå®ç°ã€‚ Prompté¢„è®¾æ¨¡æ¿ç»“æ„ Agent Profileï¼šæ­¤éƒ¨åˆ†æ¶‰åŠåˆ°æ™ºèƒ½ä½“çš„åŸºç¡€æè¿°ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºä»£ç†çš„ç±»å‹ã€åŠŸèƒ½å’ŒæŒ‡ä»¤é›†ã€‚ç”¨æˆ·å¯ä»¥åœ¨è¿™é‡Œè®¾ç½®æ™ºèƒ½ä½“çš„åŸºæœ¬å±æ€§ï¼Œç¡®ä¿å…¶è¡Œä¸ºä¸é¢„æœŸç›¸ç¬¦ã€‚ Contextï¼šä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç»™æ™ºèƒ½ä½“åšå‚è€ƒï¼Œå¸®åŠ©æ™ºèƒ½ä½“æ›´å¥½çš„è¿›è¡Œå†³ç­–ã€‚ Tool Informationï¼šæ­¤éƒ¨åˆ†ä¸ºæ™ºèƒ½ä½“æä¾›äº†ä¸€å¥—å¯ç”¨å·¥å…·çš„æ¸…å•ï¼Œæ™ºèƒ½ä½“å¯ä»¥æ ¹æ®å½“å‰çš„åœºæ™¯éœ€æ±‚ä»ä¸­æŒ‘é€‰åˆé€‚çš„å·¥å…·ä»¥è¾…åŠ©å…¶æ‰§è¡Œä»»åŠ¡ã€‚ Reference Documentsï¼šè¿™é‡Œå¯ä»¥åŒ…å«ä»£ç†å‚è€ƒä½¿ç”¨çš„æ–‡æ¡£æˆ–ä»£ç ç‰‡æ®µï¼Œä»¥ä¾¿äºå®ƒåœ¨å¤„ç†è¯·æ±‚æ—¶èƒ½å¤Ÿå‚ç…§ç›¸å…³èµ„æ–™ã€‚ Session Recordsï¼šåœ¨è¿›è¡Œå¤šè½®å¯¹è¯æ—¶ï¼Œæ­¤éƒ¨åˆ†ä¼šè®°å½•ä¹‹å‰çš„äº¤è°ˆå†…å®¹ï¼Œç¡®ä¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨ä¸Šä¸‹æ–‡ä¸­ä¿æŒè¿è´¯æ€§ã€‚ Response Output Formatï¼šç”¨æˆ·å¯ä»¥åœ¨æ­¤è®¾ç½®æ™ºèƒ½ä½“çš„è¾“å‡ºæ ¼å¼ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„å“åº”æ»¡è¶³ç‰¹å®šçš„æ ¼å¼è¦æ±‚ï¼ŒåŒ…æ‹¬ç»“æ„ã€è¯­æ³•ç­‰ã€‚ Responseï¼šåœ¨ä¸æ™ºèƒ½ä½“çš„å¯¹è¯ä¸­ï¼Œå¦‚æœç”¨æˆ·å¸Œæœ›æ™ºèƒ½ä½“ç»§ç»­æŸä¸ªè¯é¢˜æˆ–å†…å®¹ï¼Œå¯ä»¥åœ¨æ­¤æ¨¡å—ä¸­è¾“å…¥ç»­å†™çš„ä¸Šæ–‡ã€‚ä¾‹å¦‚ï¼Œåœ¨è¿ç”¨REACTæ¨¡å¼æ—¶ï¼Œå¯ä»¥åœ¨æ­¤åŒºåŸŸå†…è¯¦ç»†é˜è¿°æ™ºèƒ½ä½“å…ˆå‰çš„è¡Œä¸ºå’Œè§‚å¯Ÿç»“æœï¼Œä»¥ä¾¿äºæ™ºèƒ½ä½“æ„å»ºè¿è´¯çš„åç»­å“åº”ã€‚ Promptè‡ªå®šä¹‰é…ç½® Promptæ¨¡å—å‚æ•° field_nameï¼šå”¯ä¸€çš„å­—æ®µåç§°æ ‡è¯†ï¼Œå¿…é¡»æä¾›ã€‚ functionï¼šæŒ‡å®šå¦‚ä½•å¤„ç†è¾“å…¥æ•°æ®çš„å‡½æ•°ï¼Œå¿…é¡»æä¾›ã€‚ titleï¼šå®šä¹‰æ¨¡å—çš„æ ‡é¢˜ã€‚è‹¥æœªæä¾›ï¼Œå°†è‡ªåŠ¨ç”Ÿæˆä¸€ä¸ªæ ‡é¢˜ï¼Œè¯¥æ ‡é¢˜é€šè¿‡æŠŠå­—æ®µåç§°ä¸­çš„ä¸‹åˆ’çº¿æ›¿æ¢ä¸ºç©ºæ ¼å¹¶å°†æ¯ä¸ªå•è¯çš„é¦–å­—æ¯å¤§å†™æ¥æ„å»ºã€‚ descriptionï¼šæä¾›æ¨¡å—çš„ç®€è¦æè¿°ï¼Œä½äºæ¨¡å—æœ€ä¸Šæ–¹ï¼ˆæ ‡é¢˜ä¸‹æ–¹ï¼‰ã€‚é»˜è®¤ä¸ºç©ºï¼Œå¯é€‰å¡«ã€‚ is_contextï¼šæ ‡è¯†è¯¥å­—æ®µæ˜¯å¦å±äºä¸Šä¸‹æ–‡æ¨¡å—çš„ä¸€éƒ¨åˆ†ã€‚é»˜è®¤ä¸ºTrueï¼Œæ„å‘³ç€é™¤éæ˜¾å¼æŒ‡å®šä¸ºFalseï¼Œå¦åˆ™éƒ½è¢«è§†ä¸ºä¸Šä¸‹æ–‡çš„ä¸€éƒ¨åˆ†ã€‚ omit_if_emptyï¼šè®¾å®šå½“æ¨¡å—å†…å®¹ä¸ºç©ºæ—¶ï¼Œæ˜¯å¦åœ¨promptä¸­çœç•¥è¯¥æ¨¡å—ï¼Œå³ä¸æ˜¾ç¤ºç›¸åº”çš„æ¨¡æ¿æ ‡é¢˜å’Œå†…å®¹ã€‚é»˜è®¤ä¸ºFalseï¼Œæ„å‘³ç€å³ä½¿å†…å®¹ä¸ºç©ºä¹Ÿä¼šæ˜¾ç¤ºæ ‡é¢˜ã€‚å¦‚æœå¸Œæœ›å†…å®¹ä¸ºç©ºæ—¶çœç•¥æ¨¡å—ï¼Œéœ€æ˜¾å¼è®¾ç½®ä¸ºTrueã€‚ Prompté…ç½®ç¤ºä¾‹ Prompté…ç½®ç”±ä¸€ç³»åˆ—å®šä¹‰promptæ¨¡å—çš„å­—å…¸ç»„æˆï¼Œè¿™äº›æ¨¡å—å°†æ ¹æ®æŒ‡å®šçš„å‚æ•°å’ŒåŠŸèƒ½æ¥å¤„ç†è¾“å…¥æ•°æ®å¹¶ç»„ç»‡æˆä¸€ä¸ªå®Œæ•´çš„promptã€‚&#xA;åœ¨é…ç½®ä¸­ï¼Œæ¯ä¸ªå­—å…¸ä»£è¡¨ä¸€ä¸ªæ¨¡å—ï¼Œå…¶ä¸­åŒ…å«ç›¸å…³çš„å‚æ•°å¦‚ field_name, function_name, is_context, title, description, å’Œ omit_if_emptyï¼Œç”¨ä»¥æ§åˆ¶æ¨¡å—çš„è¡Œä¸ºå’Œå‘ˆç°æ–¹å¼ã€‚&#xA;context_placeholder å­—æ®µç”¨äºæ ‡è¯†ä¸Šä¸‹æ–‡æ¨¡æ¿çš„ä½ç½®ï¼Œå…è®¸åœ¨promptä¸­æ’å…¥åŠ¨æ€å†…å®¹ã€‚&#xA;[ {&amp;#34;field_name&amp;#34;: &amp;#39;agent_profile&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_agent_profile&amp;#39;, &amp;#34;is_context&amp;#34;: False}, {&amp;#34;field_name&amp;#34;: &amp;#39;context_placeholder&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;&amp;#39;, &amp;#34;is_context&amp;#34;: True}, {&amp;#34;field_name&amp;#34;: &amp;#39;tool_information&amp;#39;,&amp;#34;function_name&amp;#34;: &amp;#39;handle_tool_data&amp;#39;, &amp;#34;is_context&amp;#34;: True}, {&amp;#34;field_name&amp;#34;: &amp;#39;reference_documents&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_doc_info&amp;#39;}, {&amp;#34;field_name&amp;#34;: &amp;#39;session_records&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_session_records&amp;#39;}, {&amp;#34;field_name&amp;#34;: &amp;#39;task_records&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_task_records&amp;#39;}, {&amp;#34;field_name&amp;#34;: &amp;#39;output_format&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_output_format&amp;#39;, &amp;#39;title&amp;#39;: &amp;#39;Response Output Format&amp;#39;, &amp;#34;is_context&amp;#34;: False}, {&amp;#34;field_name&amp;#34;: &amp;#39;response&amp;#39;, &amp;#34;function_name&amp;#34;: &amp;#39;handle_response&amp;#39;, &amp;#34;title&amp;#34;=&amp;#34;begin!</description>
    </item>
    <item>
      <title>Pull Request</title>
      <link>/contribution/pull-request/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/contribution/pull-request/</guid>
      <description>ä¸­æ–‡&amp;nbsp ï½œ &amp;nbspEnglish&amp;nbsp Contribution Pre-Checklist First, confirm whether you have checked the document, issue, discussion (GitHub features), or other publicly available documentation. Find the GitHub issue you want to address. If none exists, create an issue or draft PR and ask a Maintainer for a check Check for related, similar, or duplicate pull requests Create a draft pull request Complete the PR template for the description Link any GitHub issue(s) that are resolved by your PR Description A description of the PR should be articulated in concise language, highlighting the work completed by the PR.</description>
    </item>
    <item>
      <title>Quick Start</title>
      <link>/coagent/quick-start/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/coagent/quick-start/</guid>
      <description>Quick Start First, set up the LLM configuration import os, sys&#xD;import openai&#xD;# llm config&#xD;os.environ[&amp;#34;API_BASE_URL&amp;#34;] = OPENAI_API_BASE&#xD;os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#34;sk-xxx&amp;#34;&#xD;openai.api_key = &amp;#34;sk-xxx&amp;#34;&#xD;# os.environ[&amp;#34;OPENAI_PROXY&amp;#34;] = &amp;#34;socks5h://127.0.0.1:13659&amp;#34; Next, configure the LLM settings and vector model from coagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;llm_config = LLMConfig(&#xD;model_name=&amp;#34;gpt-3.5-turbo&amp;#34;, model_device=&amp;#34;cpu&amp;#34;,api_key=os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;], api_base_url=os.environ[&amp;#34;API_BASE_URL&amp;#34;], temperature=0.3&#xD;)&#xD;embed_config = EmbedConfig(&#xD;embed_engine=&amp;#34;model&amp;#34;, embed_model=&amp;#34;text2vec-base-chinese&amp;#34;, embed_model_path=&amp;#34;D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/embedding_models/text2vec-base-chinese&amp;#34;&#xD;) Finally, choose a pre-existing scenario to execute from coagent.tools import toLangchainTools, TOOL_DICT, TOOL_SETS&#xD;from coagent.</description>
    </item>
    <item>
      <title>Quick Start</title>
      <link>/muagent/quick-start/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/muagent/quick-start/</guid>
      <description>Quick Start For a complete example, see examples/muagent_examples&#xA;First, prepare the relevant configuration information import os, sys&#xD;api_key = &amp;#34;sk-xxx&amp;#34;&#xD;api_base_url= &amp;#34;https://api.openai.com/v1&amp;#34;&#xD;model_name = &amp;#34;gpt-3.5-turbo&amp;#34;&#xD;embed_model = &amp;#34;{{embed_model_name}}&amp;#34;&#xD;embed_model_path = &amp;#34;{{embed_model_path}}&amp;#34;&#xD;#&#xD;os.environ[&amp;#34;DUCKDUCKGO_PROXY&amp;#34;] = os.environ.get(&amp;#34;DUCKDUCKGO_PROXY&amp;#34;) or &amp;#34;socks5://127.0.0.1:13659&amp;#34; Then, set up LLM configuration and Embedding model configuration from muagent.base_configs.env_config import JUPYTER_WORK_PATH&#xD;from muagent.tools import toLangchainTools, TOOL_DICT, TOOL_SETS&#xD;from muagent.llm_models.llm_config import EmbedConfig, LLMConfig&#xD;from muagent.connector.phase import BasePhase&#xD;from muagent.connector.schema import Message&#xD;llm_config = LLMConfig(&#xD;model_name=model_name, api_key=api_key, api_base_url=api_base_url, temperature=0.</description>
    </item>
    <item>
      <title>QuickStart</title>
      <link>/docs/codefuse-chatbot-quickstart/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-chatbot-quickstart/</guid>
      <description>ä¸­æ–‡&amp;nbsp ï½œ &amp;nbspEnglish&amp;nbsp ğŸš€ Quick Start To deploy private models, please install the NVIDIA driver by yourself. This project has been tested on Python 3.9.18 and CUDA 11.7 environments, as well as on Windows and macOS systems with x86 architecture. For Docker installation, private LLM access, and related startup issues, see: Start-detail&amp;hellip;&#xA;Preparation of Python environment It is recommended to use conda to manage the python environment (optional) # Prepare conda environment conda create --name Codefusegpt python=3.</description>
    </item>
    <item>
      <title>QuickStart</title>
      <link>/docs/codefuse-evalution-quickstart/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-evalution-quickstart/</guid>
      <description>Generation environmentï¼š CodeFuse-13B: Python 3.8 or above,PyTorch 1.12 or above, with a recommendation for 2.0 or above, Transformers 4.24.0 or above ,CUDA 11.4 or above (for GPU users and flash-attention users, this option should be considered).&#xA;CodeFuse-CodeLlama-34B:python&amp;gt;=3.8,pytorch&amp;gt;=2.0.0,transformers==4.32.0,Sentencepiece,CUDA 11.&#xA;Evaluation Environment The evaluation of the generated codes involves compiling and running in multiple programming languages. The versions of the programming language environments and packages we use are as follows:&#xA;Dependency Version Python 3.</description>
    </item>
    <item>
      <title>QuickStart</title>
      <link>/docs/codefuse-mft-vlm/quickstart/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-mft-vlm/quickstart/</guid>
      <description>Contents Install Datasets Multimodal Alignment Visual Instruction Tuning Evaluation Install Please run sh init_env.sh&#xA;Datasets Here&amp;rsquo;s the table of datasets we used to train CodeFuse-VLM-14B:&#xA;Dataset Task Type Number of Samples synthdog-en OCR 800,000 synthdog-zh OCR 800,000 cc3m(downsampled) Image Caption 600,000 cc3m(downsampled) Image Caption 600,000 SBU Image Caption 850,000 Visual Genome VQA (Downsampled) Visual Question Answer(VQA) 500,000 Visual Genome Region descriptions (Downsampled) Reference Grouding 500,000 Visual Genome objects (Downsampled) Grounded Caption 500,000 OCR VQA (Downsampled) OCR and VQA 500,000 Please download these datasets on their own official websites.</description>
    </item>
    <item>
      <title>QuickStart</title>
      <link>/docs/codefuse-modelcache-quickstart/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-modelcache-quickstart/</guid>
      <description>ModelCache is easy to use, and you can build a cache testing demo in just one step.&#xA;Quick Start Building a Cache The default interface for Cache is shown below:&#xA;class Cache:&#xD;# it should be called when start the cache system&#xD;def __init__(self):&#xD;self.has_init = False&#xD;self.cache_enable_func = None&#xD;self.embedding_func = None&#xD;self.post_process_messages_func = None&#xD;self.config = Config() Before creating a ModelCache, consider the following questions:&#xA;How will you generate embedding vectors for queries?</description>
    </item>
    <item>
      <title>QuickStart</title>
      <link>/docs/codefuse-query-quickstart/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-query-quickstart/</guid>
      <description>Installation, Configuration, and Running Hardware and Software Requirements Hardware: 4C8G&#xA;Environment Requirements: Java 1.8 and Python 3.8 or above runtime environments. Please ensure Java and Python executables are available.&#xA;Sparrow Installation Steps and Guidance The CodeFuse-Query download package is a zip archive that contains tools, scripts, and various files specific to CodeFuse-Query. If you do not have a CodeFuse-Query license, downloading this archive indicates your agreement with the CodeFuse-Query Terms and Conditions.</description>
    </item>
    <item>
      <title>QuickStart</title>
      <link>/docs/codefuse-devops-model-quickstart/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-devops-model-quickstart/</guid>
      <description>Dependency Installation Please install the packages listed in the requirements.txt file from the GitHub address first. You can refer to the following code:&#xA;pip install -r requirements.txt Model Download Model download information is as follows:&#xA;ğŸ¤— Huggingface Address&#xA;- Base Model Aligned Model 7B DevOps-Model-7B-Base DevOps-Model-7B-Chat 14B DevOps-Model-14B-Base DevOps-Model-14B-Chat ğŸ¤– ModelScope Address&#xA;- Base Model Aligned Model 7B DevOps-Model-7B-Base DevOps-Model-7B-Chat 14B DevOps-Model-14B-Base DevOps-Model-14B-Chat Find the version of the Chat model you want to download; currently, 7B and 14B models are provided.</description>
    </item>
    <item>
      <title>QuickStart</title>
      <link>/docs/mftcoder-quickstart/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/mftcoder-quickstart/</guid>
      <description>Requirements To begin, ensure that you have successfully installed CUDA (version &amp;gt;= 11.4, preferably 11.7) along with the necessary drivers. Additionally, make sure you have installed torch (version 2.0.1).&#xA;Next, we have provided an init_env.sh script to simplify the installation of required packages. Execute the following command to run the script:&#xA;sh init_env.sh We highly recommend training with flash attention(version &amp;gt;= 2.1.0, preferably 2.3.6), please refer to the following link for installation instructions: https://github.</description>
    </item>
    <item>
      <title>QuickStart</title>
      <link>/docs/test-agent-quickstart/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/test-agent-quickstart/</guid>
      <description>QuickStart Prerequisites Model Download You can get detailed information about the model and download the model files from modelscope or huggingface. Please note: éœ€è¦æ³¨æ„çš„æ˜¯ï¼š If you download the model through modelscope, refer to the download instructions: Download Instructions; If you download the model through huggingface, please make sure you have proper access to huggingface.&#xA;Environment Installation python&amp;gt;=3.8 transformers==4.33.2 git clone https://github.com/codefuse-ai/Test-Agent cd Test-Agent pip install -r requirements.txt Before starting to run the TestGPT-7B model, please ensure that your execution environment has about 14GB of VRAM.</description>
    </item>
    <item>
      <title>Release Note</title>
      <link>/docs/codefuse-modelcache-release/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-modelcache-release/</guid>
      <description>æ—¶é—´ åŠŸèƒ½ ç‰ˆæœ¬å· 20230430 Completed GPTCache research, open-source process running through OpenAI interface, single-node form æ—  20230509 1. Completed technology selection and upstream/downstream interaction scheme&#xA;2. Redeveloped database module, replaced SQLAlchemy framework&#xA;3. Refactored llm_handler module, compatible with codegpt, adapted codegpt model parameters&#x9;æ•° V0.1.0 20230519 1. Dynamically selected codegpt service mode based on environment&#xA;2. Capability for local model loading and pre-loading&#xA;3. Added dynamic loading capability for local paths based on environment V0.</description>
    </item>
    <item>
      <title>Start-Detail</title>
      <link>/docs/chatbot/start-detail/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/chatbot/start-detail/</guid>
      <description>ä¸­æ–‡&amp;nbsp ï½œ &amp;nbspEnglish&amp;nbsp If you need to deploy a privatized model, please install the NVIDIA driver yourself.&#xA;Preparation of Python environment It is recommended to use conda to manage the python environment (optional) # Prepare conda environment conda create --name Codefusegpt python=3.9 conda activate Codefusegpt Install related dependencies cd Codefuse-ChatBot pip install -r requirements.txt Sandbox Environment Preparation Windows Docker installation: Docker Desktop for Windows supports 64-bit versions of Windows 10 Pro with Hyper-V enabled (Hyper-V is not required for versions v1903 and above), or 64-bit versions of Windows 10 Home v1903 and above.</description>
    </item>
    <item>
      <title>Test-Agent</title>
      <link>/docs/test-agent/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/test-agent/</guid>
      <description>Test-Agent Test-Agent</description>
    </item>
    <item>
      <title>Test-Agent: Your AI Test Assistant</title>
      <link>/docs/overview/test-agent-your-ai-test-assistant/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/overview/test-agent-your-ai-test-assistant/</guid>
      <description>Local Mac M1 Experience Moda Experience Moda Model Access Linkï¼šModelScope TestGPT-7B What is Test Agent? (Introduction) Test Agent aims to build an &amp;ldquo;intelligent agent&amp;rdquo; in the testing domain, integrating large models with engineering technologies in the quality domain to promote the generational upgrade of quality technology. We look forward to collaborating with community members to create innovative solutions in the testing domain, establish a 24-hour online testing assistant service, and make testing as smooth as silk.</description>
    </item>
    <item>
      <title>Toolchain</title>
      <link>/docs/codefuse-query-toolchain/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-query-toolchain/</guid>
      <description>Developing Plugins (VSCode) Installation Install from VSCode marketplace (Recommand) VSCode Extension&#xA;Install from local via VSIX pack Download the plugin. Manually install from vsix: Or use the command directly from the terminal to install: code --install-extension [extension vsix file path] Environment Preparation Sparrow CLI, refer to Section 3 Installation, Configuration, and Running. Extension Features This extension provides the following feature modules:&#xA;COREF AST Viewer GÃ¶del Language Server GÃ¶del Language Runner COREF AST Viewer The following features need to be enabled in the extension settings.</description>
    </item>
    <item>
      <title>Train Detail</title>
      <link>/docs/codefuse-devops-model-train/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-devops-model-train/</guid>
      <description>Training Process According to the literature review, it is known that most domain models are based on conversational models and undergo knowledge infusion through Supervised Fine-Tuning (SFT). However, the QA corpus required for SFT fine-tuning largely comes from ChatGPT generation, which may not fully cover domain knowledge.&#xA;Therefore, the DevOps-Model adopts a pre-training plus training followed by SFT fine-tuning approach, as illustrated in Figure 2.1. We believe that for large domain models, additional pre-training is necessary.</description>
    </item>
    <item>
      <title>User Case</title>
      <link>/docs/codefuse-query-usercase/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/docs/codefuse-query-usercase/</guid>
      <description>Use Cases Querying Code Features A developer wants to know which String type variables are used in Repo A, so he writes a GÃ¶del script as follows and submits it to the CodeFuse-Query system for results.&#xA;// script use coref::java::* fn out(var: string) -&amp;gt; bool { for(v in Variable(JavaDB::load(&amp;#34;coref_java_src.db&amp;#34;))) { if (v.getType().getName() = &amp;#34;String&amp;#34; &amp;amp;&amp;amp; var = v.getName()) { return true } } } fn main() { output(out()) } Similar needs: querying for classes, functions, variables, return values, call graphs, class inheritance, etc.</description>
    </item>
  </channel>
</rss>
