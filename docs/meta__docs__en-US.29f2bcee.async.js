"use strict";(self.webpackChunkCodeFuse_Docs=self.webpackChunkCodeFuse_Docs||[]).push([[7430],{35255:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"The CodeFuse Team is a group of enthusiastic people that aims to build Code Large Language Models (Code LLMs) to support and enhance AI-native software development throughout the full life cycle, covering crucial stages such as design requirements, coding, testing, building, deployment, operations, and insight analysis.We embrace open source and have released 15 code-related models, along with a series of open-sourced techniques, such as MFTCoder, CodeFuse-VLM, CodeFuse-DevOps, CodeFuse-Query, TestGPU, CodeFuse-muAgent.We also built CodeFuse-IDE plugins for users to benefit from the assistance of CodeFuse in their daily coding work. Feel free to chat with us, and you are welcome to join us for this amazing work!",paraId:0,tocIndex:0}]},61800:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"The ChatBot project is based on ",paraId:0},{value:"langchain-chatchat",paraId:0},{value:" and ",paraId:0},{value:"codebox-api",paraId:0},{value:".",paraId:0},{value:"......",paraId:1},{value:"Deep gratitude is extended for their open-source contributions!",paraId:2}]},23553:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"Thank you for your interest in the Codefuse project. We warmly welcome any suggestions, opinions (including criticisms), comments, and contributions to the Codefuse project.",paraId:0},{value:"Your suggestions, opinions, and comments on Codefuse can be directly submitted through GitHub Issues.",paraId:1},{value:"There are many ways to participate in the Codefuse project and contribute to it: code implementation, test writing, process tool improvement, documentation enhancement, and more. We welcome any contributions and will add you to our list of contributors.",paraId:2},{value:"Furthermore, with enough contributions, you may have the opportunity to become a Committer for Codefuse.",paraId:3},{value:"For any questions, you can contact us for timely answers through various means including WeChat, Gitter (an instant messaging tool provided by GitHub), email, and more.",paraId:4},{value:"If you are new to the Codefuse community, you can:",paraId:5,tocIndex:0},{value:"Follow the Codefuse GitHub repository.",paraId:6,tocIndex:0},{value:"Join related WeChat groups for Codefuse to ask questions at any time;",paraId:6,tocIndex:0},{value:"Through the above methods, you can stay up-to-date with the development dynamics of the Codefuse project and express your opinions on topics of interest.",paraId:7,tocIndex:0},{value:"This contribution guide is not just about writing code. We value and appreciate help in all areas. Here are some ways you can contribute:",paraId:8,tocIndex:1},{value:"Documentation",paraId:9,tocIndex:1},{value:"Issues",paraId:9,tocIndex:1},{value:"Pull Requests (PR)",paraId:9,tocIndex:1},{value:"Documentation is the main way for you to understand Codefuse and is also where we need the most help!",paraId:10,tocIndex:2},{value:"By browsing the documentation, you can deepen your understanding of Codefuse and also help you grasp the features and technical details of Codefuse. If you find any issues with the documentation, please contact us in time;",paraId:11,tocIndex:2},{value:"If you are interested in improving the quality of the documentation, whether it is revising an address of a page, correcting a link, or writing a better introductory document, we are very welcoming!",paraId:12,tocIndex:2},{value:"Most of our documentation is written in markdown format. You can directly modify and submit documentation changes in the docs/ directory on GitHub. For submitting code changes, please refer to Pull Requests.",paraId:13,tocIndex:2},{value:"If you discover a bug or issue, you can directly submit a new Issue through GitHub Issues, and someone will handle it regularly. For more details, see Issue Template.",paraId:14,tocIndex:3},{value:"Issue Template",paraId:15,tocIndex:3},{value:"You can also choose to read and analyze the code to fix it yourself (it is best to communicate with us before doing so, as someone might already be working on the same issue), and then submit a Pull Request.",paraId:16,tocIndex:3},{value:"You can download the code, compile, install, and deploy to try it out (you can refer to the compilation documentation to see if it works as you expected). If there are any issues, you can directly contact us, submit an Issue, or fix it yourself by reading and analyzing the source code. For more details, see",paraId:17,tocIndex:4},{value:"How to Submit a PR.",paraId:18,tocIndex:4},{value:"Whether it's fixing a bug or adding a feature, we warmly welcome it. If you wish to submit code to Doris, you need to fork the code repository to your project space on GitHub, create a new branch for your submitted code, add the original project as an upstream, and submit a PR. The method for submitting a PR can be referenced in the Pull Request documentation.",paraId:19,tocIndex:4}]},41549:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"Issues can be categorized into three types:",paraId:0,tocIndex:0},{value:"Bug: Issues where code or execution examples contain bugs or lack dependencies, resulting in incorrect execution.",paraId:1,tocIndex:0},{value:"Documentation: Discrepancies in documentation, inconsistencies between documentation content and code, etc.",paraId:1,tocIndex:0},{value:"Feature: New functionalities that evolve from the current codebase.",paraId:1,tocIndex:0},{value:"Checklist before submitting an issue",paraId:2,tocIndex:2},{value:"Please confirm that you have checked the document, issues, discussions (GitHub feature), and other publicly available documentation.",paraId:2,tocIndex:2},{value:"I have searched through all documentation related to Codefuse.",paraId:3,tocIndex:2},{value:"I used GitHub search to find a similar issue, but did not find one.",paraId:3,tocIndex:2},{value:"I have added a very descriptive title for this issue.",paraId:3,tocIndex:2},{value:"System Information",paraId:4,tocIndex:2},{value:"Please confirm your operating system, such as mac-xx, windows-xx, linux-xx.",paraId:4,tocIndex:2},{value:"Code Version",paraId:5,tocIndex:2},{value:"Please confirm the code version or branch, such as master, release, etc.",paraId:5,tocIndex:2},{value:"Problem Description",paraId:6,tocIndex:2},{value:"Describe the problem you encountered, what you want to achieve, or the bug encountered during code execution.",paraId:6,tocIndex:2},{value:"Code Example",paraId:7,tocIndex:2},{value:"Attach your execution code and relevant configuration to facilitate rapid intervention and reproduction.",paraId:7,tocIndex:2},{value:"Error Information, Logs",paraId:8,tocIndex:2},{value:"The error logs and related information after executing the above code example.",paraId:8,tocIndex:2},{value:"Related Dependencies",paraId:9,tocIndex:2},{value:"Taking the chatbot project as an example:",paraId:9,tocIndex:2},{value:"connector",paraId:10,tocIndex:2},{value:"codechat",paraId:10,tocIndex:2},{value:"sandbox",paraId:10,tocIndex:2},{value:"...",paraId:10,tocIndex:2},{value:"Issue with current documentation:",paraId:11,tocIndex:3},{value:"Please point out any problems, typos, or confusing points in the current documentation.",paraId:11,tocIndex:3},{value:"Idea or request for content",paraId:12,tocIndex:3},{value:"What do you think would be a reasonable way to express the documentation?",paraId:12,tocIndex:3},{value:"Checklist before submitting an issue",paraId:13,tocIndex:4},{value:"Please confirm that you have checked the document, issues, discussions (GitHub feature), and other publicly available documentation.",paraId:13,tocIndex:4},{value:"I have searched through all documentation related to Codefuse.",paraId:14,tocIndex:4},{value:"I used GitHub Issue search to find a similar issue, but did not find one.",paraId:14,tocIndex:4},{value:"I have added a very descriptive title for this issue.",paraId:14,tocIndex:4},{value:"Feature Description",paraId:15,tocIndex:4},{value:"Describe the purpose of this feature.",paraId:15,tocIndex:4},{value:"Related Examples",paraId:16,tocIndex:4},{value:"Provide references to documents, repositories, etc., Please provide links to any relevant GitHub repos, papers, or other resources if relevant.",paraId:16,tocIndex:4},{value:"Motivation",paraId:17,tocIndex:4},{value:"Describe the motivation for this feature. Why is it needed? Provide enough context information to help understand the demand for this feature.",paraId:17,tocIndex:4},{value:"Contribution",paraId:18,tocIndex:4},{value:"How you can contribute to the building of this feature (if you are participating).",paraId:18,tocIndex:4}]},26364:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"Confirm if you have checked the publicly available document information, such as documents, issues, and discussions (GitHub features).",paraId:0,tocIndex:1},{value:"Find the GitHub issue you wish to address. If it does not exist, create an issue or a draft PR and request a review from the maintainers.",paraId:0,tocIndex:1},{value:"Check for related, similar, or duplicate pull requests.",paraId:0,tocIndex:1},{value:"Create a draft pull request.",paraId:0,tocIndex:1},{value:"Complete the description in the PR template.",paraId:0,tocIndex:1},{value:"Link any GitHub issues your PR resolves.",paraId:0,tocIndex:1},{value:"A concise language should be used to describe what the PR achieves, with specific standards available in the ",paraId:1,tocIndex:2},{value:"Commit Format Standards",paraId:2,tocIndex:2},{value:".",paraId:1,tocIndex:2},{value:"#xx",paraId:3,tocIndex:3},{value:" if applicable",paraId:3,tocIndex:3},{value:"Please provide relevant test code if necessary.",paraId:4,tocIndex:4},{value:'Commit is divided into a "title" and a "body." In principle, the title is all in lowercase. The first letter of the body should be capitalized.',paraId:5,tocIndex:5},{value:"The title of the commit message: ",paraId:6,tocIndex:6},{value:"[<type>](<scope>) <subject> (#pr)",paraId:6,tocIndex:6},{value:"Type of this commit, constrained to the following:",paraId:7,tocIndex:7},{value:"fix: bug fixes",paraId:8,tocIndex:7},{value:"feature: new features",paraId:8,tocIndex:7},{value:"feature-wip: Features in development, such as parts of a function.",paraId:8,tocIndex:7},{value:"improvement: Optimization and enhancement of existing features",paraId:8,tocIndex:7},{value:"style: Code style adjustments",paraId:8,tocIndex:7},{value:"typo: Typos in code or documentation",paraId:8,tocIndex:7},{value:"refactor: Code refactoring (not involving feature changes)",paraId:8,tocIndex:7},{value:"performance/optize: Performance optimization",paraId:8,tocIndex:7},{value:"test: Adding or repairing unit tests",paraId:8,tocIndex:7},{value:"deps: Modifications to third-party dependencies",paraId:8,tocIndex:7},{value:`community: Modifications related to the community, such as editing GitHub Issue templates, etc.
Notes:
If multiple types appear in one commit, add multiple types.
If code refactoring leads to performance improvements, you can add both [refactor][optimize]
It is not allowed to have types other than those listed above. If necessary, new types should be added to this document.`,paraId:8,tocIndex:7},{value:`The module scope involved in this submission. As there are many functional modules, only a part is listed here, which will be continuously improved according to needs.
`,paraId:9,tocIndex:8},{value:"For example, the framework of a chatbot:",paraId:9,tocIndex:8},{value:"connector",paraId:10,tocIndex:8},{value:"codechat",paraId:10,tocIndex:8},{value:"sandbox",paraId:10,tocIndex:8},{value:`...
Notes:
Try to use options already listed in this document. If you need to add a new option, please update this document promptly.`,paraId:10,tocIndex:8},{value:"The title should clearly explain the main content of the submission.",paraId:11,tocIndex:9},{value:"Coming soon",paraId:12,tocIndex:10},{value:"doris-commit-format",paraId:13,tocIndex:11}]},95455:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:`
  `,paraId:0},{value:`
  `,paraId:1},{value:"HuggingFace",paraId:1},{value:` |
  `,paraId:1},{value:"ModelScope",paraId:1},{value:`Hello World! This is CodeFuse!
`,paraId:2},{value:"CodeFuse aims to develop Code Large Language Models (Code LLMs) to support and enhance full-lifecycle AI native sotware developing, covering crucial stages such as design requirements, coding, testing, building, deployment, operations, and insight analysis.",paraId:3},{value:`
  `,paraId:4},{value:`
We are passionating about creating innovative open-source solutions that empower developers throughout the software development process as shown above. We also encourage engineers and researchers within this community to join us in co-constructing/improving CodeFuse.`,paraId:1}]},89185:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"First, add an OpenAI configuration, or a model with a similar interface to OpenAI (launched through fastchat)",paraId:0,tocIndex:0},{value:`import os, sys

api_key = "sk-xxx"
api_base_url= "https://api.openai.com/v1"
model_name = "gpt-3.5-turbo"
embed_model = "{{embed_model_name}}"
embed_model_path = "{{embed_model_path}}"

#
os.environ["DUCKDUCKGO_PROXY"] = os.environ.get("DUCKDUCKGO_PROXY") or "socks5://127.0.0.1:13659"
`,paraId:1,tocIndex:0},{value:`Then Set LLM Configuration and Vector Model Configuration
Configure related LLM and Embedding Model`,paraId:2,tocIndex:0},{value:`from muagent.base_configs.env_config import JUPYTER_WORK_PATH
from muagent.connector.agents import BaseAgent, ReactAgent, ExecutorAgent, SelectorAgent
from muagent.connector.chains import BaseChain
from muagent.connector.schema import Role, Message, ChainConfig
from muagent.llm_models.llm_config import EmbedConfig, LLMConfig
from muagent.tools import toLangchainTools, TOOL_DICT, TOOL_SETS

llm_config = LLMConfig(
    model_name=model_name, api_key=api_key,  api_base_url=api_base_url, temperature=0.3,
    stop="**Observation:**"
)

embed_config = EmbedConfig(
    embed_engine="model", embed_model=embed_model, embed_model_path=embed_model_path
)
`,paraId:3,tocIndex:0},{value:"Define two react agents for actual task execution",paraId:4,tocIndex:1},{value:`# Here, predefined prompts are used, but you can also refer to the above prompts to complete the writing
from muagent.connector.configs.prompts import REACT_CODE_PROMPT, REACT_TOOL_PROMPT

# A tool agent based on react is defined
tool_role = Role(role_type="assistant", role_name="tool_reacter", prompt=REACT_TOOL_PROMPT)
tool_react_agent = ReactAgent(
    role=tool_role,
    task="",
    chat_turn=3,
    focus_agents=[],
    focus_message_keys=[],
    llm_config=llm_config, embed_config=embed_config,
)


# A code agent based on react is defined
code_role = Role(role_type="assistant", role_name="code_reacter", prompt=REACT_CODE_PROMPT)
code_react_agent = ReactAgent(
    role=code_role,
    task="",
    chat_turn=3,
    focus_agents=[],
    focus_message_keys=[],
    llm_config=llm_config, embed_config=embed_config,
)
`,paraId:5,tocIndex:1},{value:"Define a groupAgent for agent selection",paraId:6,tocIndex:1},{value:`prompt = """#### Agent Profile
Your goal is to respond according to the information in the Context Data with the role that will best facilitate a solution, taking into account all relevant context (Context) provided.
When you need to select the appropriate role for handling a user's query, carefully read the provided role names, role descriptions, and tool list.
ATTENTION: respond carefully following the "Response Output Format".
#### Response Output Format
**Thoughts:** think step by step about why you selected one role
**Role:** Select the role from the agent names.
"""

# A groupAgent is defined
role = Role(role_type="assistant", role_name="qaer", prompt=prompt)
base_agent = SelectorAgent(
    role=role,
    task="",
    chat_turn=3,
    focus_agents=[],
    focus_message_keys=[],
    llm_config=llm_config, embed_config=embed_config,
    group_agents=[tool_react_agent, code_react_agent]
)
`,paraId:7,tocIndex:1},{value:`# prepare your tools
tools = toLangchainTools([TOOL_DICT[i] for i in TOOL_SETS if i in TOOL_DICT])

# if you want to analyze a data.csv, please put the csv file into a jupyter_work_path (or your defined path)
import shutil
source_file = 'D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/jupyter_work/employee_data.csv'
shutil.copy(source_file, JUPYTER_WORK_PATH)
question = "Confirm if employee_data.csv exists locally, and check its columns and data types; then draw a bar chart"

query = Message(
    user_name="test", role_type="user", role_name="user", input_query=question,
    tools=tools,
)
# base_agent.pre_print(query)
output_message = base_agent.step(query)
print(output_message.input_query)
print(output_message.role_content)
`,paraId:8,tocIndex:2},{value:`# Configuration structure is in this directory
from muagent.connector.schema import Role
`,paraId:9,tocIndex:3},{value:"Config Key Name",paraId:10,tocIndex:4},{value:"Type",paraId:10,tocIndex:4},{value:"Description",paraId:10,tocIndex:4},{value:"role",paraId:10,tocIndex:4},{value:"Role",paraId:10,tocIndex:4},{value:"Role description",paraId:10,tocIndex:4},{value:"focus_agents",paraId:10,tocIndex:4},{value:"List[String]",paraId:10,tocIndex:4},{value:"Logic of MetaGPT, focusing on the messages generated by which agents, optional values are: role_name",paraId:10,tocIndex:4},{value:"focus_message_keys",paraId:10,tocIndex:4},{value:"List[String]",paraId:10,tocIndex:4},{value:"Additional logic, focusing on specific key information in the message, optional values are: agent's output_keys",paraId:10,tocIndex:4},{value:"chat_turn",paraId:10,tocIndex:4},{value:"int",paraId:10,tocIndex:4},{value:"Valid only for ReactAgent",paraId:10,tocIndex:4},{value:"llm_config",paraId:10,tocIndex:4},{value:"LLMConfig",paraId:10,tocIndex:4},{value:"Large language model configuration",paraId:10,tocIndex:4},{value:"embed_config",paraId:10,tocIndex:4},{value:"EmbedConfig",paraId:10,tocIndex:4},{value:"Vector model configuration",paraId:10,tocIndex:4},{value:"sandbox_server",paraId:10,tocIndex:4},{value:"Dict",paraId:10,tocIndex:4},{value:"Sandbox environment, i.e., notebook startup configuration",paraId:10,tocIndex:4},{value:"jupyter_work_path",paraId:10,tocIndex:4},{value:"str",paraId:10,tocIndex:4},{value:"Working directory of the sandbox environment",paraId:10,tocIndex:4},{value:"kb_root_path",paraId:10,tocIndex:4},{value:"str",paraId:10,tocIndex:4},{value:"Storage path for memory",paraId:10,tocIndex:4},{value:"log_verbose",paraId:10,tocIndex:4},{value:"str",paraId:10,tocIndex:4},{value:"Log printing level of agent prompt & predict",paraId:10,tocIndex:4},{value:"Config Key Name",paraId:11,tocIndex:5},{value:"Type",paraId:11,tocIndex:5},{value:"Description",paraId:11,tocIndex:5},{value:"role_type",paraId:11,tocIndex:5},{value:"str",paraId:11,tocIndex:5},{value:"Role type, Enum: system, user, assistant, function, observation, summary",paraId:11,tocIndex:5},{value:"role_name",paraId:11,tocIndex:5},{value:"str",paraId:11,tocIndex:5},{value:"Role name",paraId:11,tocIndex:5},{value:"role_desc",paraId:11,tocIndex:5},{value:"str",paraId:11,tocIndex:5},{value:"Role description",paraId:11,tocIndex:5},{value:"agent_type",paraId:11,tocIndex:5},{value:"str",paraId:11,tocIndex:5},{value:"Agent type",paraId:11,tocIndex:5},{value:"role_prompt",paraId:11,tocIndex:5},{value:"str",paraId:11,tocIndex:5},{value:"Role instruction",paraId:11,tocIndex:5},{value:"prompt",paraId:11,tocIndex:5},{value:"str",paraId:11,tocIndex:5},{value:"Complete prompt structure",paraId:11,tocIndex:5}]},37901:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:`import os, sys

api_key = "sk-xxx"
api_base_url= "https://api.openai.com/v1"
model_name = "gpt-3.5-turbo"
embed_model = "{{embed_model_name}}"
embed_model_path = "{{embed_model_path}}"
#
os.environ["DUCKDUCKGO_PROXY"] = os.environ.get("DUCKDUCKGO_PROXY") or "socks5://127.0.0.1:13659"
`,paraId:0,tocIndex:1},{value:"Configure related LLM and Embedding Model",paraId:1,tocIndex:2},{value:`from muagent.base_configs.env_config import JUPYTER_WORK_PATH
from muagent.connector.agents import BaseAgent, ReactAgent, ExecutorAgent, SelectorAgent
from muagent.connector.chains import BaseChain
from muagent.connector.schema import Role, Message, ChainConfig
from muagent.llm_models.llm_config import EmbedConfig, LLMConfig
from muagent.tools import toLangchainTools, TOOL_DICT, TOOL_SETS

llm_config = LLMConfig(
    model_name=model_name, api_key=api_key,  api_base_url=api_base_url, temperature=0.3,
    stop="**Observation:**"
)

embed_config = EmbedConfig(
    embed_engine="model", embed_model=embed_model, embed_model_path=embed_model_path
)
`,paraId:2,tocIndex:2},{value:"Define two react agents for actual task execution",paraId:3,tocIndex:3},{value:`# Here, predefined prompts are used, but you can also refer to the above prompts to complete the writing
from muagent.connector.configs.prompts import REACT_CODE_PROMPT, REACT_TOOL_PROMPT

# A tool agent based on react is defined
tool_role = Role(role_type="assistant", role_name="tool_reacter", prompt=REACT_TOOL_PROMPT)
tool_react_agent = ReactAgent(
    role=tool_role,
    task="",
    chat_turn=3,
    focus_agents=[],
    focus_message_keys=[],
    llm_config=llm_config, embed_config=embed_config,
)

# A code agent based on react is defined
code_role = Role(role_type="assistant", role_name="code_reacter", prompt=REACT_CODE_PROMPT)
code_react_agent = ReactAgent(
    role=code_role,
    task="",
    chat_turn=3,
    focus_agents=[],
    focus_message_keys=[],
    llm_config=llm_config, embed_config=embed_config,
)
`,paraId:4,tocIndex:3},{value:"Define a groupAgent for agent selection",paraId:5,tocIndex:3},{value:`prompt = """#### Agent Profile
Your goal is to respond according to the information in the Context Data with the role that will best facilitate a solution, taking into account all relevant context (Context) provided.
When you need to select the appropriate role for handling a user's query, carefully read the provided role names, role descriptions, and tool list.
ATTENTION: respond carefully following the "Response Output Format".
#### Response Output Format
**Thoughts:** think step by step about why you selected one role
**Role:** Select the role from the agent names.
"""
# A groupAgent is defined
role = Role(role_type="assistant", role_name="qaer", prompt=prompt)
base_agent = SelectorAgent(
    role=role,
    task="",
    chat_turn=3,
    focus_agents=[],
    focus_message_keys=[],
    llm_config=llm_config, embed_config=embed_config,
    group_agents=[tool_react_agent, code_react_agent]
)
`,paraId:6,tocIndex:3},{value:`chain_config = ChainConfig(chain_name="group_chain", agents=[base_agent.role.role_name], chat_turn=1)
base_chain = BaseChain(
    chainConfig=chain_config, agents=[base_agent],
    llm_config=llm_config, embed_config=embed_config,
)
`,paraId:7,tocIndex:4},{value:`# prepare your tools
tools = toLangchainTools([TOOL_DICT[i] for i in TOOL_SETS if i in TOOL_DICT])

# if you want to analyze a data.csv, please put the csv file into a jupyter_work_path (or your defined path)
import shutil
source_file = 'D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/jupyter_work/employee_data.csv'
shutil.copy(source_file, JUPYTER_WORK_PATH)
question = "Confirm if employee_data.csv exists locally, and check its columns and data types; then draw a bar chart"
query = Message(
    user_name="test", role_type="user", role_name="user", input_query=question,
    tools=tools,
)

# base_chain.pre_print(query)
output_message, output_memory = base_chain.step(query)
print(output_message.input_query)
print(output_message.role_content)
print(output_memory.to_str_messages(return_all=True, content_key="parsed_output_list"))
`,paraId:8,tocIndex:5},{value:"Config Key Name",paraId:9,tocIndex:6},{value:"Type",paraId:9,tocIndex:6},{value:"Description",paraId:9,tocIndex:6},{value:"agents",paraId:9,tocIndex:6},{value:"List[BaseAgent]",paraId:9,tocIndex:6},{value:"llm_config",paraId:9,tocIndex:6},{value:"LLMConfig",paraId:9,tocIndex:6},{value:"Large Language Model Configuration",paraId:9,tocIndex:6},{value:"embed_config",paraId:9,tocIndex:6},{value:"EmbedConfig",paraId:9,tocIndex:6},{value:"Vector Model Configuration",paraId:9,tocIndex:6},{value:"sandbox_server",paraId:9,tocIndex:6},{value:"Dict",paraId:9,tocIndex:6},{value:"Sandbox environment or notebook startup configuration",paraId:9,tocIndex:6},{value:"jupyter_work_path",paraId:9,tocIndex:6},{value:"str",paraId:9,tocIndex:6},{value:"Working directory for the sandbox environment",paraId:9,tocIndex:6},{value:"kb_root_path",paraId:9,tocIndex:6},{value:"str",paraId:9,tocIndex:6},{value:"Storage path for memory",paraId:9,tocIndex:6},{value:"log_verbose",paraId:9,tocIndex:6},{value:"str",paraId:9,tocIndex:6},{value:"Log printing level for agent prompts & predictions",paraId:9,tocIndex:6}]},78608:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:`import os
import openai
from coagent.base_configs.env_config import KB_ROOT_PATH
from coagent.connector.memory_manager import BaseMemoryManager, LocalMemoryManager
from coagent.llm_models.llm_config import EmbedConfig, LLMConfig
from coagent.connector.schema import Message


os.environ["API_BASE_URL"] = OPENAI_API_BASE
os.environ["OPENAI_API_KEY"] = "sk-xx"
openai.api_key = "sk-xxx"
# os.environ["OPENAI_PROXY"] = "socks5h://127.0.0.1:13659"
os.environ["DUCKDUCKGO_PROXY"] = os.environ.get("DUCKDUCKGO_PROXY") or "socks5://127.0.0.1:13659"


# LLM and Embedding Model configurations
llm_config = LLMConfig(
    model_name=os.environ["model_name"], api_key=os.environ["OPENAI_API_KEY"],
    api_base_url=os.environ["API_BASE_URL"], temperature=0.3
)

embed_config = EmbedConfig(
    embed_engine="model", embed_model=os.environ["embed_model"],
    embed_model_path=os.environ["embed_model_path"]
)
`,paraId:0,tocIndex:1},{value:`# prepare your message
message1 = Message(
    chat_index="default", role_name="test1", role_type="user", role_content="hello",
    parsed_output_list=[{"input": "hello"}], user_name="default"
)

text = "hi! how can I help you?"
message2 = Message(
    chat_index="shuimo", role_name="test2", role_type="assistant", role_content=text, parsed_output_list=[{"answer": text}],
    user_name="shuimo"
)

text = "they say hello and hi to each other"
message3 = Message(
    chat_index="shanshi", role_name="test3", role_type="summary", role_content=text,
    parsed_output_list=[{"summary": text}],
    user_name="shanshi"
    )

# append or extend test
local_memory_manager = LocalMemoryManager(embed_config=embed_config, llm_config=llm_config, do_init=True)
# append can ignore user_name
local_memory_manager.append(message=message1)
local_memory_manager.append(message=message2)
local_memory_manager.append(message=message3)
`,paraId:1,tocIndex:2},{value:`local_memory_manager = LocalMemoryManager(embed_config=embed_config, llm_config=llm_config, do_init=False)
local_memory_manager.load()
print(local_memory_manager.get_memory_pool("default").messages)
print(local_memory_manager.get_memory_pool("shuimo").messages)
print(local_memory_manager.get_memory_pool("shanshi").messages)
`,paraId:2,tocIndex:3},{value:`# embedding retrieval test
text = "say hi to each other, i want some help"
# retrieval_type=datetime => retrieval from datetime and jieba
print(local_memory_manager.router_retrieval(chat_index="shanshi", text=text, datetime="2024-03-12 17:48:00", n=4, top_k=5, retrieval_type= "datetime"))
# retrieval_type=eembedding => retrieval from embedding
print(local_memory_manager.router_retrieval(chat_index="shanshi", text=text, top_k=5, retrieval_type= "embedding"))
# retrieval_type=text => retrieval from jieba
print(local_memory_manager.router_retrieval(chat_index="shanshi", text=text, top_k=5, retrieval_type= "text"))
`,paraId:3,tocIndex:4},{value:`# recursive_summary test
print(local_memory_manager.recursive_summary(local_memory_manager.get_memory_pool("shanshi").messages, split_n=1, chat_index="shanshi"))
`,paraId:4,tocIndex:5}]},19498:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"Primarily used for managing chat history, not yet completed",paraId:0,tocIndex:0},{value:"Read and write chat history in the database, including user input, llm output, doc retrieval, code retrieval, search retrieval.",paraId:1,tocIndex:0},{value:"Summarize key information from the chat history into a summary context, serving as a prompt context.",paraId:1,tocIndex:0},{value:"Provide a search function to retrieve information related to the question from chat history or summary context, aiding in Q&A.",paraId:1,tocIndex:0},{value:"Examples see ~/tests/connector/memory_manager_test.py",paraId:2,tocIndex:1}]},2964:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"First, add OpenAI configuration, which can be models with similar interfaces to OpenAI (triggered via fastchat).",paraId:0,tocIndex:0},{value:`import os, sys
api_key = "sk-xxx"
api_base_url= "https://api.openai.com/v1"
model_name = "gpt-3.5-turbo"
embed_model = "{{embed_model_name}}"
embed_model_path = "{{embed_model_path}}"
#
os.environ["DUCKDUCKGO_PROXY"] = os.environ.get("DUCKDUCKGO_PROXY") or "socks5://127.0.0.1:13659"
`,paraId:1,tocIndex:0},{value:"Configure related LLM and Embedding Model.",paraId:2,tocIndex:1},{value:`from muagent.base_configs.env_config import JUPYTER_WORK_PATH
from muagent.connector.agents import BaseAgent, ReactAgent, ExecutorAgent, SelectorAgent
from muagent.connector.chains import BaseChain
from muagent.connector.phase import BasePhase
from muagent.connector.schema import Role, Message, ChainConfig
from muagent.llm_models.llm_config import EmbedConfig, LLMConfig
from muagent.tools import toLangchainTools, TOOL_DICT, TOOL_SETS
llm_config = LLMConfig(
    model_name=model_name, api_key=api_key,  api_base_url=api_base_url, temperature=0.3,
    stop="**Observation:**"
)
embed_config = EmbedConfig(
    embed_engine="model", embed_model=embed_model, embed_model_path=embed_model_path
)
`,paraId:3,tocIndex:1},{value:"Define two react agents for actual task execution.",paraId:4,tocIndex:2},{value:`# Predefined prompts are used here; you can also refer to the above-mentioned prompts to write your own.
from muagent.connector.configs.prompts import REACT_CODE_PROMPT, REACT_TOOL_PROMPT
# Defined a tool agent based on react
tool_role = Role(role_type="assistant", role_name="tool_reacter", prompt=REACT_TOOL_PROMPT)
tool_react_agent = ReactAgent(
    role=tool_role,
    task="",
    chat_turn=3,
    focus_agents=[],
    focus_message_keys=[],
    llm_config=llm_config, embed_config=embed_config,
)
# Defined a code agent based on react
code_role = Role(role_type="assistant", role_name="code_reacter", prompt=REACT_CODE_PROMPT)
code_react_agent = ReactAgent(
    role=code_role,
    task="",
    chat_turn=3,
    focus_agents=[],
    focus_message_keys=[],
    llm_config=llm_config, embed_config=embed_config,
)
`,paraId:5,tocIndex:2},{value:"Define a GroupAgent for agent selection.",paraId:6,tocIndex:2},{value:`prompt = """#### Agent Profile
Your goal is to respond according to the information provided by the Context Data's with the role that will best facilitate a solution, taking into account all relevant context data (Context).
When you need to select the appropriate role for handling a user's query, carefully read the provided role names, role descriptions, and tool list.
ATTENTION: respond carefully, referenced to the "Response Output Format" standard.
#### Response Output Format
**Thoughts:** think the reason step by step about why you select one role
**Role:** Select the role from the agent names.
"""
# Defined a GroupAgent
role = Role(role_type="assistant", role_name="qaer", prompt=prompt)
base_agent = SelectorAgent(
    role=role,
    task="",
    chat_turn=3,
    focus_agents=[],
    focus_message_keys=[],
    llm_config=llm_config, embed_config=embed_config,
    group_agents=[tool_react_agent, code_react_agent]
)
`,paraId:7,tocIndex:2},{value:`chain_config = ChainConfig(chain_name="group_chain", agents=[base_agent.role.role_name], chat_turn=1)
base_chain = BaseChain(
    chainConfig=chain_config, agents=[base_agent],
    llm_config=llm_config, embed_config=embed_config,
)
`,paraId:8,tocIndex:3},{value:`base_phase = BasePhase(
    phase_name="group_phase", chains=[base_chain],
    embed_config=embed_config, llm_config=llm_config
)
`,paraId:9,tocIndex:4},{value:"Start execution.",paraId:10,tocIndex:5},{value:`# prepare your tools
tools = toLangchainTools([TOOL_DICT[i] for i in TOOL_SETS if i in TOOL_DICT])

# if you want to analyze a data.csv, please put the csv file into a jupyter_work_path (or your defined path)
import shutil
source_file = 'D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/jupyter_work/employee_data.csv'
shutil.copy(source_file, JUPYTER_WORK_PATH)
question = "Confirm whether employee_data.csv exists locally, and review its columns and data types; then plot a bar chart."
query = Message(
    user_name="test", role_type="user", role_name="user", input_query=question,
    tools=tools,
)


# base_phase.pre_print(query)
output_message, output_memory = base_phase.step(query)
print(output_message.input_query)
print(output_message.role_content)
print(output_memory.to_str_messages(return_all=True, content_key="parsed_output_list"))
`,paraId:11,tocIndex:5},{value:"Config Key Name",paraId:12,tocIndex:6},{value:"Type",paraId:12,tocIndex:6},{value:"Description",paraId:12,tocIndex:6},{value:"phase_name",paraId:12,tocIndex:6},{value:"String",paraId:12,tocIndex:6},{value:"Scenario name",paraId:12,tocIndex:6},{value:"chains",paraId:12,tocIndex:6},{value:"List[Chain]",paraId:12,tocIndex:6},{value:"List of chains to be executed in order",paraId:12,tocIndex:6},{value:"llm_config",paraId:12,tocIndex:6},{value:"LLMConfig",paraId:12,tocIndex:6},{value:"Large Language Model configuration",paraId:12,tocIndex:6},{value:"embed_config",paraId:12,tocIndex:6},{value:"EmbedConfig",paraId:12,tocIndex:6},{value:"Vector model configuration",paraId:12,tocIndex:6},{value:"sandbox_server",paraId:12,tocIndex:6},{value:"Dict",paraId:12,tocIndex:6},{value:"Sandbox environment, i.e., notebook startup configuration",paraId:12,tocIndex:6},{value:"jupyter_work_path",paraId:12,tocIndex:6},{value:"str",paraId:12,tocIndex:6},{value:"Working directory in the sandbox environment",paraId:12,tocIndex:6},{value:"kb_root_path",paraId:12,tocIndex:6},{value:"str",paraId:12,tocIndex:6},{value:"Storage path for memory",paraId:12,tocIndex:6},{value:"log_verbose",paraId:12,tocIndex:6},{value:"str",paraId:12,tocIndex:6},{value:"Log print level for agent prompts & predictions",paraId:12,tocIndex:6}]},74265:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"Managing prompt creation in multi-agent linkages",paraId:0,tocIndex:0},{value:"Quick Configuration: Utilizing preset processing functions, users can easily configure by simply defining the inputs and outputs of the agents, enabling fast assembly and configuration of multi-agent prompts.",paraId:1,tocIndex:0},{value:"Customization Support: Allows users to customize the internal processing logic of each module within the prompt to achieve personalized implementation of the agent prompt.",paraId:1,tocIndex:0},{value:"Agent Profile: This section involves the basic description of the agent, including but not limited to the type of agent, its functions, and command set. Users can set the basic attributes of the agent here to ensure its behavior aligns with expectations.",paraId:2,tocIndex:1},{value:`Context: Contextual Information, provided as a reference for the agent, aiding in better decision-making.
`,paraId:2,tocIndex:1},{value:"Tool Information: This part provides the agent with a list of available tools, from which the agent can choose appropriate ones to assist in task execution based on current scenario requirements.",paraId:3,tocIndex:1},{value:"Reference Documents: This may include documents or code snippets for the agent to refer to when handling requests, to facilitate the use of relevant information.",paraId:3,tocIndex:1},{value:"Session Records: In multi-round conversations, this section records previous dialogue content to ensure continuity within the context.",paraId:3,tocIndex:1},{value:"Response Output Format: Here the user can set the output format of the agent to ensure that the generated responses meet specific formatting requirements, including structure, grammar, etc.",paraId:2,tocIndex:1},{value:"In the entire structure of a Prompt, we need to define three parts:",paraId:4,tocIndex:2},{value:"Agent Profile",paraId:5,tocIndex:2},{value:"Input Format: such as ",paraId:5,tocIndex:2},{value:"**key_name:** key_description",paraId:5,tocIndex:2},{value:"Response Output Format: such as ",paraId:5,tocIndex:2},{value:"**key_name:** key_description",paraId:5,tocIndex:2},{value:`#### Agent Profile
Agent Description ...

#### Input Format
**Origin Query:** the initial question or objective that the user wanted to achieve
**Context:** the current status and history of the tasks to determine if Origin Query has been achieved.

#### Response Output Format
**Action Status:** finished or continued
If it's 'finished', the context can answer the origin query.
If it's 'continued', the context can't answer the origin query.
**REASON:** Justify the decision of choosing 'finished' or 'continued' by evaluating the progress step by step.
Consider all relevant information. If the tasks were aimed at an ongoing process, assess whether it has reached a satisfactory conclusion.
`,paraId:6,tocIndex:2},{value:"Here, we have integrated some of the common operations of the ",paraId:7,tocIndex:2},{value:"Input Format",paraId:7,tocIndex:2},{value:`, with certain fields and operational procedures built in to form a standardized configurable operation.
In the future, we will also make parts of the Agent Profile and Response Output Format configurable to reduce the difficulty of writing Prompts.`,paraId:7,tocIndex:2},{value:"Implement construction with custom fields according to actual needs",paraId:8,tocIndex:3},{value:`class CodeGenDocer(BaseAgent):
    def start_action_step(self, message: Message) -> Message:
        '''do action before agent predict '''
        # Get code snippets and node information based on the question
        action_json = CodeRetrievalSingle.run(message.code_engine_name, message.input_query, llm_config=self.llm_config,
                                              embed_config=self.embed_config, local_graph_path=message.local_graph_path, use_nh=message.use_nh,search_type="tag")
        current_vertex = action_json['vertex']
        message.customed_kargs["Code Snippet"] = action_json["code"]
        message.customed_kargs['Current_Vertex'] = current_vertex
        return message

`,paraId:9,tocIndex:3},{value:"After building phases, chains, or agents, we can confirm agent linkages using the pre-print function of methods, allowing for debugging in advance to avoid discovering issues only after execution.",paraId:10,tocIndex:4},{value:`from muagent.base_configs.env_config import JUPYTER_WORK_PATH
from muagent.connector.agents import BaseAgent, ReactAgent, ExecutorAgent, SelectorAgent
from muagent.connector.chains import BaseChain
from muagent.connector.schema import Role, Message, ChainConfig
from muagent.llm_models.llm_config import EmbedConfig, LLMConfig
from muagent.tools import toLangchainTools, TOOL_DICT, TOOL_SETS


import os, sys
api_key = "sk-xxx"
api_base_url= "https://api.openai.com/v1"
model_name = "gpt-3.5-turbo"
embed_model = "{{embed_model_name}}"
embed_model_path = "{{embed_model_path}}"
#
os.environ["DUCKDUCKGO_PROXY"] = os.environ.get("DUCKDUCKGO_PROXY") or "socks5://127.0.0.1:13659"

llm_config = LLMConfig(
    model_name="gpt-4", api_key=api_key,  api_base_url=api_base_url, temperature=0.3
)
embed_config = EmbedConfig(
    embed_engine="model", embed_model=embed_model, embed_model_path=embed_model_path
)

phase_name = "baseGroupPhase"
phase = BasePhase(
    phase_name, embed_config=embed_config, llm_config=llm_config,
)

question = "Confirm if employee_data.csv exists locally, and check its columns and data types; then draw a bar chart"
query = Message(
    user_name="test", role_type="user", role_name="user", input_query=question,
)
phase.pre_print(query)
`,paraId:11,tocIndex:4},{value:"Here, pre-defined agents are used,\uFF0Ccustom case can be seen ",paraId:12,tocIndex:4},{value:"customed_example",paraId:13,tocIndex:4},{value:`##########################
<<<<baseGroup's prompt>>>>
##########################

### Agent Profile
Your goal is to response according the Context Data's information with the role that will best facilitate a solution, taking into account all relevant context (Context) provided.
When you need to select the appropriate role for handling a user's query, carefully read the provided role names, role descriptions and tool list.
ATTENTION: response carefully referenced "Response Output Format" in format.

### Tool Information

### Agent Infomation
        Please ensure your selection is one of the listed roles. Available roles for selection:
        "role name: tool_react
role description:  Agent Profile,When interacting with users, your role is to respond in a helpful and accurate manner using the tools available. Follow the steps below to ensure efficient and effective use of the tools.,Please note that all the tools you can use are listed below. You can only choose from these tools for use. ,If there are no suitable tools, please do not invent any tools. Just let the user know that you do not have suitable tools to use.,ATTENTION: The Action Status field ensures that the tools or code mentioned in the Action can be parsed smoothly. Please make sure not to omit the Action Status field when replying.,"
"role name: code_react
role description:  Agent Profile,When users need help with coding, your role is to provide precise and effective guidance.,Write the code step by step, showing only the part necessary to solve the current problem. Each reply should contain only the code required for the current step.,"
        Please ensure select the Role from agent names, such as tool_react, code_react

### Context Data

#### Reference Documents

#### Session Records

#### Current Plan

### Response Output Format
**Thoughts:** think the reason step by step about why you selecte one role
**Role:** Select the role from agent names.

### Begin!!!

###################
<<<<LLM PREDICT>>>>
###################

**Thoughts:**
**Role:**


###########################
<<<<tool_react's prompt>>>>
###########################
### Agent Profile
When interacting with users, your role is to respond in a helpful and accurate manner using the tools available. Follow the steps below to ensure efficient and effective use of the tools.
Please note that all the tools you can use are listed below. You can only choose from these tools for use.
If there are no suitable tools, please do not invent any tools. Just let the user know that you do not have suitable tools to use.
ATTENTION: The Action Status field ensures that the tools or code mentioned in the Action can be parsed smoothly. Please make sure not to omit the Action Status field when replying.

### Tool Information

### Context Data

#### Reference Documents

#### Session Records

#### Task Records

### Response Output Format
**Thoughts:** According the previous observations, plan the approach for using the tool effectively.
...

### Begin!!!

###################
<<<<LLM PREDICT>>>>
###################
**Thoughts:**
**Action Status:**
**Action:**
**Observation:**
**Thoughts:**
**Action Status:**
**Action:**

###########################
<<<<code_react's prompt>>>>
###########################
### Agent Profile
When users need help with coding, your role is to provide precise and effective guidance.
Write the code step by step, showing only the part necessary to solve the current problem. Each reply should contain only the code required for the current step.

### Context Data

#### Reference Documents

#### Session Records

### Response Output Format

**Thoughts:** According the previous context, solve the problem step by step, only displaying the thought process necessary for the current step of solving the problem,
outline the plan for executing this step.

**Action Status:** Set to 'stopped' or 'code_executing'.
If it's 'stopped', the action is to provide the final answer to the session records and executed steps.
If it's 'code_executing', the action is to write the code.
...

### Begin!!!

###################
<<<<LLM PREDICT>>>>
###################

**Thoughts:**
**Action Status:**
**Action:**
**Observation:**
**Thoughts:**
**Action Status:**
**Action:**

`,paraId:14,tocIndex:5}]},90207:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:`import os
import openai
from coagent.base_configs.env_config import KB_ROOT_PATH
from coagent.connector.memory_manager import BaseMemoryManager, LocalMemoryManager
from coagent.llm_models.llm_config import EmbedConfig, LLMConfig
from coagent.connector.schema import Message


os.environ["API_BASE_URL"] = OPENAI_API_BASE
os.environ["OPENAI_API_KEY"] = "sk-xx"
openai.api_key = "sk-xxx"
# os.environ["OPENAI_PROXY"] = "socks5h://127.0.0.1:13659"
os.environ["DUCKDUCKGO_PROXY"] = os.environ.get("DUCKDUCKGO_PROXY") or "socks5://127.0.0.1:13659"

# TBASE CONFIG
TBASE_ARGS = {
        'host': '{url}',
        'port': 6379,
        'username': '',
        'password': ''
    }


# LLM and Embedding Model configurations
llm_config = LLMConfig(
    model_name=os.environ["model_name"], api_key=os.environ["OPENAI_API_KEY"],
    api_base_url=os.environ["API_BASE_URL"], temperature=0.3
)

embed_config = EmbedConfig(
    embed_engine="model", embed_model=os.environ["embed_model"],
    embed_model_path=os.environ["embed_model_path"]
)


#specify index_name
index_name = 'your_index_name'
th = TbaseHandler(TBASE_ARGS, index_name, definition_value="message")


# # drop index
# th.drop_index(index_name)

# create tbase memory manager
memory_manager = TbaseMemoryManager(
            unique_name="EKG",
            embed_config=embed_config,
            llm_config=llm_config,
            tbase_handler=th,
            use_vector=False
        )
`,paraId:0,tocIndex:1},{value:`import uuid

# example1
message = Message(
    chat_index="wyp311395_test_chatindex_0",
    message_index= f"nodeid0-{uuid.uuid4()}",
    user_name="311395",
    role_name = "311395", # agent \u540D\u5B57\uFF0C
    role_type = "user", # agent \u7C7B\u578B\uFF0C\u9ED8\u8BA4assistant\uFF0C\u53EF\u9009observation
    ## llm output
    role_content = "\u4ECA\u5929\u5929\u6C14\u5982\u4F55\uFF1F", # \u8F93\u5165
)

memory_manager.append(message)

# example2
message = Message(
    chat_index="wyp311395_test_chatindex_0",
    message_index= f"nodeid1-{uuid.uuid4()}",
    user_name="311395",
    role_name = "tester_0", # agent \u540D\u5B57\uFF0C
    role_type = "assistant", # agent \u7C7B\u578B\uFF0C\u9ED8\u8BA4assistant\uFF0C\u53EF\u9009observation
    ## llm output
    role_content = "<functioncall> {'date': '2024-04-17'}", # \u8F93\u5165
)

memory_manager.append(message)
`,paraId:1,tocIndex:2},{value:`
logger.debug(f'\u6309user_name\u68C0\u7D22\uFF1A{memory_manager.get_memory_pool("311395")}')

logger.debug(f'\u5168\u5C40\u68C0\u7D22\uFF1A{memory_manager.get_memory_pool_by_content("\u4ECA\u5929\u5929\u6C14\u5982\u4F55\uFF1F")}')

logger.debug(f'\u5168\u5C40\u68C0\u7D22\uFF1A{memory_manager.get_memory_pool_by_content("functioncall")}')

logger.debug(f'\u6309kev-value\u68C0\u7D22\uFF1A{memory_manager.get_memory_pool_by_key_content("role_content", "functioncall")}')

logger.debug(f'\u6309key-value\u68C0\u7D22\uFF1A{memory_manager.get_memory_pool_by_all({"chat_index": "wyp311395_test_chatindex_0", "role_content": "functioncall"})}')

logger.debug(f'\u6309key-value\u68C0\u7D22\uFF1A{memory_manager.get_memory_pool_by_all({"keyword": "nodeid3"})}')

#
logger.debug(f'\u6309datetime\u68C0\u7D22\uFF1A{memory_manager.router_retrieval(chat_index="wyp311395_test_chatindex_0", datetime="2024-03-12 17:48:00", n=4, top_k=5, retrieval_type= "datetime")}')

#
logger.debug(f'\u6309datetime\u68C0\u7D22\uFF1A{memory_manager.router_retrieval(chat_index="wyp311395_test_chatindex_0", datetime="2024-04-18 11:30:00", n=4, top_k=5, retrieval_type= "datetime")}')

#
logger.debug(f'\u6309text\u68C0\u7D22\uFF1A{memory_manager.router_retrieval(chat_index="wyp311395_test_chatindex_0", text="\u4ECA\u5929\u5929\u6C14", top_k=5, retrieval_type= "text")}')

#
logger.debug(f'\u6309embedding\u68C0\u7D22\uFF1A{memory_manager.router_retrieval(chat_index="wyp311395_test_chatindex_0", text="\u4ECA\u5929\u5929\u6C14", top_k=5, retrieval_type= "embedding")}')

`,paraId:2,tocIndex:3},{value:`# recursive_summary test
messages = memory_manager.router_retrieval(chat_index="wyp311395_test_chatindex_0", text="\u4ECA\u5929\u5929\u6C14", top_k=5, retrieval_type= "embedding")
print(memory_manager.recursive_summary(messages, chat_index="wyp311395_test_chatindex_0", nodeid="nodeid3", user_name="311395", split_n=1))
`,paraId:3,tocIndex:4}]},38099:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"Below we will use a code repository to demonstrate the automatic generation of API documentation from code, detailing how to customize the construction of an agent phase.",paraId:0,tocIndex:0},{value:"codeGenDocGroup_PROMPT, create group Agent Prompt",paraId:1,tocIndex:1},{value:`# update new agent configs
codeGenDocGroup_PROMPT = """#### Agent Profile

Your goal is to response according the Context Data's information with the role that will best facilitate a solution, taking into account all relevant context (Context) provided.

When you need to select the appropriate role for handling a user's query, carefully read the provided role names, role descriptions and tool list.

#### Input Format

#### Response Output Format

**Code Path:** Extract the paths for the class/method/function that need to be addressed from the context

**Role:** Select the role from agent names
"""
`,paraId:2,tocIndex:1},{value:"classGenDoc_PROMPT, create class code to api doc Prompt",paraId:3,tocIndex:1},{value:`classGenDoc_PROMPT = """#### Agent Profile
As an advanced code documentation generator, you are proficient in translating class definitions into comprehensive documentation with a focus on instantiation parameters.
Your specific task is to parse the given code snippet of a class, extract information regarding its instantiation parameters.

#### Input Format

**Current_Vertex:** Provide the code vertex of the function or method.

**Code Snippet:** Provide the full class definition, including the constructor and any parameters it may require for instantiation.

#### Response Output Format
**Class Base:** Specify the base class or interface from which the current class extends, if any.

**Class Description:** Offer a brief description of the class's purpose and functionality.

**Init Parameters:** List each parameter from construct. For each parameter, provide:
    - \`param\`: The parameter name
    - \`param_description\`: A concise explanation of the parameter's purpose.
    - \`param_type\`: The data type of the parameter, if explicitly defined.

    \`\`\`json
    [
        {
            "param": "parameter_name",
            "param_description": "A brief description of what this parameter is used for.",
            "param_type": "The data type of the parameter"
        },
        ...
    ]
    \`\`\`


    If no parameter for construct, return
    \`\`\`json
    []
    \`\`\`
"""
`,paraId:4,tocIndex:1},{value:"funcGenDoc_PROMPT\uFF0Ccreate function code to api doc Prompt",paraId:5,tocIndex:1},{value:`funcGenDoc_PROMPT = """#### Agent Profile
You are a high-level code documentation assistant, skilled at extracting information from function/method code into detailed and well-structured documentation.


#### Input Format
**Code Path:** Provide the code path of the function or method you wish to document.
This name will be used to identify and extract the relevant details from the code snippet provided.

**Current_Vertex:** Provide the code vertex of the function or method.

**Code Snippet:** A segment of code that contains the function or method to be documented.

#### Response Output Format

**Class Description:** Offer a brief description of the method(function)'s purpose and functionality.

**Parameters:** Extract parameter for the specific function/method Code from Code Snippet. For parameter, provide:
    - \`param\`: The parameter name
    - \`param_description\`: A concise explanation of the parameter's purpose.
    - \`param_type\`: The data type of the parameter, if explicitly defined.
    \`\`\`json
    [
        {
            "param": "parameter_name",
            "param_description": "A brief description of what this parameter is used for.",
            "param_type": "The data type of the parameter"
        },
        ...
    ]
    \`\`\`

    If no parameter for function/method, return
    \`\`\`json
    []
    \`\`\`

**Return Value Description:** Describe what the function/method returns upon completion.

**Return Type:** Indicate the type of data the function/method returns (e.g., string, integer, object, void).
"""
`,paraId:6,tocIndex:1},{value:"First, add openai configuration or similar interfaces to models such as openai (launched via fastchat)",paraId:7,tocIndex:2},{value:`import os, sys, json
from muagent.base_configs.env_config import CB_ROOT_PATH
from muagent.llm_models.llm_config import EmbedConfig, LLMConfig
from muagent.connector.phase import BasePhase
from muagent.connector.agents import BaseAgent, SelectorAgent
from muagent.connector.chains import BaseChain
from muagent.connector.schema import Message, Role, ChainConfig
from muagent.codechat.codebase_handler.codebase_handler import CodeBaseHandler
from loguru import logger
from muagent.tools import CodeRetrievalSingle


api_key = "sk-xxx"
api_base_url= "https://api.openai.com/v1"
model_name = "gpt-3.5-turbo"
embed_model = "{{embed_model_name}}"
embed_model_path = "{{embed_model_path}}"
#
os.environ["DUCKDUCKGO_PROXY"] = os.environ.get("DUCKDUCKGO_PROXY") or "socks5://127.0.0.1:13659"
`,paraId:8,tocIndex:2},{value:"For custom key-value information",paraId:9,tocIndex:3},{value:`class CodeGenDocer(BaseAgent):
    def start_action_step(self, message: Message) -> Message:
        '''do action before agent predict '''
        # Retrieve code snippets and node information based on the question
        action_json = CodeRetrievalSingle.run(message.code_engine_name, message.input_query, llm_config=self.llm_config,
                                              embed_config=self.embed_config, local_graph_path=message.local_graph_path, use_nh=message.use_nh,search_type="tag")
        current_vertex = action_json['vertex']
        message.customed_kargs["Code Snippet"] = action_json["code"]
        message.customed_kargs['Current_Vertex'] = current_vertex
        return message

`,paraId:10,tocIndex:3},{value:`llm_config = LLMConfig(
    model_name="gpt-4", api_key=api_key,  api_base_url=api_base_url, temperature=0.3
)
embed_config = EmbedConfig(
    embed_engine="model", embed_model=embed_model, embed_model_path=embed_model_path
)
`,paraId:11,tocIndex:4},{value:`# initialize codebase
# delete codebase
codebase_name = 'client_nebula'
code_path = "D://chromeDownloads/devopschat-bot/client_v2/client"
use_nh = True
do_interpret = False
cbh = CodeBaseHandler(codebase_name, code_path, crawl_type='dir', use_nh=use_nh, local_graph_path=CB_ROOT_PATH,
                      llm_config=llm_config, embed_config=embed_config)
cbh.delete_codebase(codebase_name=codebase_name)
# load codebase
cbh = CodeBaseHandler(codebase_name, code_path, crawl_type='dir', use_nh=use_nh, local_graph_path=CB_ROOT_PATH,
                      llm_config=llm_config, embed_config=embed_config)
cbh.import_code(do_interpret=do_interpret)
`,paraId:12,tocIndex:5},{value:`# log-level, print prompt, and llm predict
os.environ["log_verbose"] = "1"

funcGenDoc_role = Role(role_type="assistant", role_name="funcGenDoc_role", prompt=funcGenDoc_PROMPT)
funcGenDoc = CodeGenDocer(
    role=funcGenDoc_role,
    chat_turn=1,
    llm_config=llm_config, embed_config=embed_config,
)

classGenDoc_role = Role(role_type="assistant", role_name="classGenDoc_role", prompt=classGenDoc_PROMPT)
classGenDoc = CodeGenDocer(
    role=classGenDoc_role,
    chat_turn=1,
    llm_config=llm_config, embed_config=embed_config,
)

codeGenDocGroup_role = Role(role_type="assistant", role_name="codeGenDocGroup_role", prompt=codeGenDocGroup_PROMPT)
codeGenDocGroup = SelectorAgent(
    role=codeGenDocGroup_role,
    chat_turn=1,
    llm_config=llm_config, embed_config=embed_config,
    group_agents=[funcGenDoc, classGenDoc]
)

chain_config = ChainConfig(
    chain_name="codeGenDocGroup_chain", agents=[codeGenDocGroup.role.role_name,],
    chat_turn=1)
chain = BaseChain(
    chainConfig=chain_config, agents=[codeGenDocGroup],
    llm_config=llm_config, embed_config=embed_config,
)

phase = BasePhase(
    phase_name="codeGenDocGroup_phase", chains=[chain],
    embed_config=embed_config, llm_config=llm_config
)
`,paraId:13,tocIndex:6},{value:`# Initialize based on the previous loading process
cbh = CodeBaseHandler(codebase_name, code_path, crawl_type='dir', use_nh=use_nh, local_graph_path=CB_ROOT_PATH,
                      llm_config=llm_config, embed_config=embed_config)
cbh.search_vertices(vertex_type="method")
# Begin transforming code into API documentation structure
for vertex_type in ["class", "method"]:
    vertices = cbh.search_vertices(vertex_type=vertex_type)
    logger.info(f"vertices={vertices}")
    # round-1
    docs = []
    for vertex in vertices:
        vertex = vertex.split("-")[0] # '-' is the delimiter for method parameters
        query_content = f"Generate documentation for {vertex_type} node {vertex}"
        query = Message(
            role_name="human", role_type="user", input_query=query_content,
            code_engine_name=codebase_name, score_threshold=1.0, top_k=3, cb_search_type="tag", use_nh=use_nh,
            local_graph_path=CB_ROOT_PATH,
            )
        output_message, output_memory = phase.step(query, reinit_memory=True)
        # print(output_memory.to_str_messages(return_all=True, content_key="parsed_output_list"))
        docs.append(output_memory.get_spec_parserd_output())
        os.makedirs(f"{CB_ROOT_PATH}/docs", exist_ok=True)
        with open(f"{CB_ROOT_PATH}/docs/raw_{vertex_type}.json", "w") as f:
            json.dump(docs, f)


# Convert the generated document information into markdown text
from muagent.utils.code2doc_util import *
import json
with open(f"/home/user/code_base/docs/raw_method.json", "r") as f:
    method_raw_data = json.load(f)


with open(f"/home/user/code_base/docs/raw_class.json", "r") as f:
    class_raw_data = json.load(f)

method_data = method_info_decode(method_raw_data)
class_data = class_info_decode(class_raw_data)
method_mds = encode2md(method_data, method_text_md)
class_mds = encode2md(class_data, class_text_md)

docs_dict = {}
for k,v in class_mds.items():
    method_textmds = method_mds.get(k, [])
    for vv in v:
        # Theoretically, there should only be one
        text_md = vv
    for method_textmd in method_textmds:
        text_md += "\\n<br>" + method_textmd
    docs_dict.setdefault(k, []).append(text_md)

    with open(f"/home/user/code_base/docs/{k}.md", "w") as f:
        f.write(text_md)
`,paraId:14,tocIndex:7}]},12810:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"First, add the OpenAI configuration; this could also be a model similar to the OpenAI interface (launched via fastchat).",paraId:0,tocIndex:0},{value:`import os, sys

api_key = "sk-xxx"
api_base_url= "https://api.openai.com/v1"
embed_model = "{{embed_model_name}}"
embed_model_path = "{{embed_model_path}}"
`,paraId:1,tocIndex:0},{value:"Constructing with a local model file",paraId:2,tocIndex:1},{value:`from muagent.llm_models.llm_config import EmbedConfig, LLMConfig

embed_config = EmbedConfig(
    embed_engine="model", embed_model=embed_model, embed_model_path=embed_model_path
)
`,paraId:3,tocIndex:1},{value:"Constructing via OpenAI",paraId:4,tocIndex:1},{value:`from muagent.llm_models.llm_config import EmbedConfig, LLMConfig

embed_config = EmbedConfig(
    embed_engine="openai", api_key=api_key, api_base_url=api_base_url,
)
`,paraId:5,tocIndex:1},{value:"Customizing and inputting langchain embeddings",paraId:6,tocIndex:1},{value:`from muagent.llm_models.llm_config import EmbedConfig, LLMConfig

class CustomizedEmbeddings(Embeddings):
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        embeddings = []
        # add your embedding code
        return embeddings
    def embed_query(self, text: str) -> List[float]:
        """Compute query embeddings using a HuggingFace transformer model.
        Args:
            text: The text to embed.
        Returns:
            Embeddings for the text.
        """
        # add your embedding code
        return embedding


embeddings = CustomizedEmbeddings()
embed_config = EmbedConfig(
    embed_model="default",
    langchain_embeddings=embeddings
)
`,paraId:7,tocIndex:1}]},41290:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"First, add the OpenAI configuration, or you can use another model similar to the OpenAI interface (launched through fastchat).",paraId:0,tocIndex:0},{value:`import os, sys

api_key = "sk-xxx"
api_base_url= "https://api.openai.com/v1"
model_name = "gpt-3.5-turbo"
`,paraId:1,tocIndex:0},{value:"By passing the class ",paraId:2,tocIndex:1},{value:"openai",paraId:2,tocIndex:1},{value:`from muagent.llm_models.llm_config import EmbedConfig, LLMConfig

llm_config = LLMConfig(
    model_name=model_name, api_key=api_key, api_base_url=api_base_url, temperature=0.3,
    stop="**Observation:**"
)
`,paraId:3,tocIndex:1},{value:"Customizing and inputting langchain LLM",paraId:4,tocIndex:1},{value:`from muagent.llm_models.llm_config import EmbedConfig, LLMConfig
from langchain.llms.base import BaseLLM, LLM


class CustomizedModel(LLM):
        repetition_penalty = 1.1
        temperature = 0.2
        top_k = 40
        top_p = 0.9

        def predict(self, prompt: str, stop: Optional[List[str]] = None) -> str:
            return self._call(prompt, stop)

        def _call(self, prompt: str,
                  stop: Optional[List[str]] = None) -> str:
            """_call"""
            return ""


llm = CustomizedModel()
llm_config = LLMConfig(
    llm=llm
)
`,paraId:5,tocIndex:1}]},64015:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"To facilitate everyone's understanding of the entire CoAgent link, we use a Flow format to detail how to build through configuration settings.",paraId:0,tocIndex:0},{value:`
  `,paraId:1},{value:"Below, we first introduce the related core components",paraId:2,tocIndex:0},{value:"On the design level of the Agent, we provide four basic types of Agents, with Role settings for these Agents that can meet the interactions and uses of various common scenarios:",paraId:3,tocIndex:1},{value:"BaseAgent: Provides basic question answering, tool usage, and code execution functions, and realizes input => output according to the Prompt format.",paraId:4,tocIndex:1},{value:"ReactAgent: Provides standard React functionality, accomplishing current tasks based on questions.",paraId:4,tocIndex:1},{value:"ExecutorAgent: Performs sequential execution of task lists, completing related tasks according to plans arranged by the User or the previous Agent.",paraId:4,tocIndex:1},{value:"SelectorAgent: Provides the function of selecting an Agent, choosing the appropriate Agent to respond according to the question from the User or the previous Agent. After output, the message is pushed into the memory pool, which will later be managed by the Memory Manager.",paraId:4,tocIndex:1},{value:"It selects the appropriate Agent to respond based on the question from the User or the previous Agent. After output, the message is pushed into the memory pool, which is subsequently managed by the Memory Manager.",paraId:5,tocIndex:1},{value:"Basic Chain: BaseChain, connects the interactions of agents, manages the related messages and memory.",paraId:6,tocIndex:2},{value:"Basic Phase: BasePhase, connects the interactions of chains, and manages the related messages and memory.",paraId:7,tocIndex:3},{value:"The prompt creation for each agent in the Mutli-Agent link:",paraId:8,tocIndex:4},{value:"By setting simple prompt_input_keys and prompt_output_keys, the preset Prompt Context creation logic can be followed to quickly configure the agent prompt.",paraId:9,tocIndex:4},{value:"It is also possible to design a new key-context in the prompt manager module, achieving personalized Agent Prompt.",paraId:9,tocIndex:4},{value:"Mainly used for the management of chat history:",paraId:10,tocIndex:5},{value:"Manages the reading and writing of chat history in a database, including user input, llm output, doc retrieval, code retrieval, search retrieval.",paraId:11,tocIndex:5},{value:"Summarizes the key information in chat history to create a summary context, which serves as a prompt context.",paraId:11,tocIndex:5},{value:"Provides a retrieval function to search for information related to the question in the chat history or summary context, assisting with question answering.",paraId:11,tocIndex:5}]},42677:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"To enhance the performance of large models in terms of inference accuracy, various innovative Large Language Model (LLM) playbooks have emerged in the industry. From the earliest Chain of Thought (CoT) and Thread of Thought (ToT) to Games on Tracks (GoT), these methods have continually expanded the capability boundaries of LLMs. When handling complex problems, we can select, invoke and execute tool feedback through the ReAct process, while realizing multi-round tool use and multi-step execution.",paraId:0,tocIndex:0},{value:"However, for more complex scenarios, such as the development of complex code, a single-function LLM Agent is clearly not up to the task. Therefore, the community has begun to develop combinations of multiple Agents, such as projects focused on the development field like metaGPT, GPT-Engineer, and chatDev, as well as the AutoGen project that focuses on automating the construction of Agents and Agent dialogue.",paraId:1,tocIndex:0},{value:"After an in-depth analysis of these frameworks, it has been found that most Agent frameworks are highly coupled, with poor usability and extensibility. They implement specific scenarios in preset settings, but expanding to new scenarios can be very challenging.",paraId:2,tocIndex:0},{value:"Therefore, we hope to build an extensible, easy-to-use Multi-Agent framework to support ChatBots in retrieving knowledge base information while assisting with various general tasks such as daily office work, data analysis, development, and operations.",paraId:3,tocIndex:0},{value:"This project's Multi-Agent framework incorporates excellent designs from multiple frameworks, such as the message pool from metaGPT and the agent selector from autogen.",paraId:4,tocIndex:0},{value:`
  `,paraId:5},{value:"In MuAgent, in addition to defining the Agent interaction link and AgentBase basic execution flow, we have also designed two basic components: Prompt Manager and Memory Manager, which are used for automated construction of Prompts and chat history management, respectively. We have built an extensible, easy-to-use Multi-Agent framework, including the following content:",paraId:6,tocIndex:1},{value:"Agent Base:",paraId:7,tocIndex:1},{value:" Established four basic types of Agents \u2013 BaseAgent, ReactAgent, ExecutorAgent, SelectorAgent \u2013 to support basic activities in various scenarios.",paraId:7,tocIndex:1},{value:"Communication:",paraId:7,tocIndex:1},{value:" Completes the transfer of information between Agents through Message and Parse Message entities, and interacts with Memory Manager to manage memory in the Memory Pool.",paraId:7,tocIndex:1},{value:"Prompt Manager:",paraId:7,tocIndex:1},{value:" Automates the assembly of Customized Agent Prompts through Role Handler, Doc/Tool Handler, Session Handler, Customized Handler.",paraId:7,tocIndex:1},{value:"Memory Manager:",paraId:7,tocIndex:1},{value:" Supports storage management of chat history, information compression, memory retrieval, and finally storage in databases, local or vector databases through the Memory Pool.",paraId:7,tocIndex:1},{value:"Component:",paraId:7,tocIndex:1},{value:" Auxiliary ecosystem components for building Agents, including Retrieval, Tool, Action, Sandbox, etc.",paraId:7,tocIndex:1},{value:"Customized Model:",paraId:7,tocIndex:1},{value:" Supports the integration of private LLM and Embedding.",paraId:7,tocIndex:1},{value:"At the Agent level, we provide four basic types of Agents, with Role settings for these Agents that can meet the interactions and uses of various common scenarios. All Actions are executed by Agents.",paraId:8,tocIndex:2},{value:"BaseAgent: Provides basic question answering, tool usage, and code execution functions, and realizes input => output according to the Prompt format.",paraId:9,tocIndex:2},{value:`
  `,paraId:5},{value:"ReactAgent: Provides standard React functionality, according to questions to execute current tasks.",paraId:10,tocIndex:2},{value:`
  `,paraId:5},{value:"ExecutorAgent: Sequentially executes a list of tasks, completing related tasks according to plans arranged by the User or the previous Agent. The Agent receives a task list ([List[task]) and loops through the tasks (Feedback Agents can also be added in the middle for task re-optimization), until the task is complete.",paraId:11,tocIndex:2},{value:`
  `,paraId:5},{value:"SelectorAgent: Provides the function of selecting an Agent, choosing the appropriate Agent to respond based on the question from the User or the previous Agent.",paraId:12,tocIndex:2},{value:`
  `,paraId:5},{value:"To enable better interaction between Agents, as well as to provide each Agent with enough information to complete their specific tasks, we have divided the Message information body into several parts, such as System Content, Info Content, LLM Content, and LLM Parsed Content, etc.",paraId:13,tocIndex:3},{value:`System Content: Used to store and manage the timing of the current LLM output, Role information, etc.
Info Content: LLM auxiliary information, such as knowledge base query information, code library retrieval information, tool information, Agent information, etc.`,paraId:14,tocIndex:3},{value:`LLM Content: Directly stores and conveys information generated by the LLM.
LLM Parsed Content: Parses the LLM's output into a more manageable key-value data structure, making it easier to filter through LLM content.
Customized Content: Manages key-value data content generated by custom actions, used for subsequent assembly and construction of custom Prompt templates.
By defining the above message formats, we can accomplish the transfer and management of general messages. Specific assembly methods can be seen in the Prompt Manager module.`,paraId:15,tocIndex:3},{value:"Mainly used for the management of chat history:",paraId:16,tocIndex:5},{value:"Storage Management: Implements the save and load management of chat history in the database or locally, including user input, LLM output, observation output.",paraId:17,tocIndex:5},{value:"Information Compression: Summarizes key information from the chat history into a summary context, such as single text summaries, summaries from different angles, key information extraction, multi-text summaries, and serves as Prompt context.",paraId:17,tocIndex:5},{value:"Memory Retrieval: Provides basic retrieval functions, retrieving information related to questions from chat history or Summary Context to assist in Q&A.",paraId:17,tocIndex:5},{value:"LLM Automatic Trigger: Future definitions of policies or the use of LLM to trigger the compression summary and retrieval functions.",paraId:17,tocIndex:5},{value:"Asking LLMs has become common practice, but how to coordinate the planning and usage of tools, code writing abilities among multiple large models to guide their expected outputs has become a key issue. Essentially, this involves abstracting business problems into executable Prompts, so we're not just designing Agents but rather engaging in framework design after a deep understanding of the current demands.",paraId:18,tocIndex:6},{value:"In actual business scenarios where LLMs are involved (excluding the SFT process), we can designate LLM to complete specific tasks and obtain expected outputs through the design of Agent Prompt content. In the process of MuAgent, the Prompt is divided into three parts: System Prompt, Context Prompt, Customized Prompt.",paraId:19,tocIndex:6},{value:"System Prompt includes Role Name, Role Description, Task, etc.",paraId:20,tocIndex:6},{value:"Context Prompt includes Doc Context, Code Context, Tool Context, Agent Context, Session Context, etc.",paraId:20,tocIndex:6},{value:`Customized Prompt involves custom inputs and outputs, such as...
We can also ask the model to output structured texts, such as the JSON string of a tool, code\\ncode_content, etc., to complete particular workflows.`,paraId:20,tocIndex:6},{value:"Automatic Prompt Assemble",paraId:21,tocIndex:6},{value:"After defining the structure as above, we can complete the automation assembly of Prompts in the following ways, without having to make extensive adjustments to the prompt each time:",paraId:22,tocIndex:6},{value:"Upon defining an Agent, configure Role Name, Role Description, Task, etc., to determine what the Agent needs to do.",paraId:23,tocIndex:6},{value:"Pre-package some reusable Context Prompt general strategies, such as selectable Role's SessionContext, configurable Tool, Code Retrieval, Doc Retrieval, Search Retrieval, Agent to complete corresponding assemblies.",paraId:23,tocIndex:6},{value:"As the Agent's Prompt requires relatively personalized operations, it also supports the addition of new key-context designs within the Prompt Manager module to achieve personalized Agent Prompts.",paraId:23,tocIndex:6},{value:"Automatic Prompt Design",paraId:24,tocIndex:6},{value:`
Able to automatically design the best prompt based on role description, task, query, etc.; to be defined...`,paraId:24,tocIndex:6},{value:"Multi Prompt Design",paraId:25,tocIndex:6},{value:`
Based on the previous definition of Prompt, we know that a Prompt consists of three parts: System Prompt, Context Prompt, Customized Prompt. Any changes in the three parts may cause changes in the final output of the LLM.`,paraId:25,tocIndex:6},{value:"For the same type of task, their System Prompt is the same. So, without considering the variations of Customiezd Prompt, it is possible to achieve the assembly differences of different contexts. For example, Prompt A obtains 10 rounds of chat history, while Prompt B uses 5 rounds of chat history, or alternatively, filters and compresses information in chat history.",paraId:26,tocIndex:6},{value:"To be implemented...",paraId:27,tocIndex:6},{value:"In all Prompts' Contexts, aside from Chat History session information, information based on external document libraries, code repositories, internet search results is also relied upon. This knowledge system beyond the model parameters can significantly enhance the Agent's ability to complete complex tasks.",paraId:28,tocIndex:8},{value:"Thus, in MuAgent, we integrated three ways to retrieve information: Doc, Internet Search, Code Retrieval, and defined an abstract class IMRetrieval, supporting developers to customize their knowledge bases to complete the Agent's knowledge base registration.",paraId:29,tocIndex:8},{value:"Doc Retrieval",paraId:30,tocIndex:8},{value:"Document vector databases are currently the mainstream method for building knowledge bases, using Text Embedding models to vectorize documents and store them in vector databases. In the future, we will also support queries based on knowledge graphs and automatically extract entities and relations through large models to explore the complex relationships in data.",paraId:31,tocIndex:8},{value:"Code Retrieval",paraId:32,tocIndex:8},{value:"LLMs face the challenge of lagging training data for code generation, repair, and component understanding tasks, as well as not being able to perceive the context-dependent structure of code. During development, understanding, retrieving and querying metadata from the existing codebase and dependencies can take a considerable amount of time. Hence, we hope to provide an external knowledge system",paraId:33,tocIndex:8},{value:"Search Retrieval",paraId:34,tocIndex:8},{value:`
In addition to the readily available document and code knowledge bases, in daily practice, browsing a large amount of web content to acquire more knowledge helps us understand emerging scenarios, businesses, technologies, and more. Hence, we've integrated duckduckgosearch, an open-source search tool, to provide LLMs with content beyond their knowledge reserves.`,paraId:34,tocIndex:8},{value:`With OpenAI launching the Function Call feature, which generates parameters for specified tools through LLM and executes the call, machines can better understand and respond to human needs, thus solving practical problems and repetitive work. Nowadays, the ability to learn tools is increasingly becoming a standard feature of open-source models. Therefore, in MuAgent, it also supports agents to complete Tool registration. By using the Python registration template BaseToolModel class and writing related properties and methods such as Tool_name, Tool_description, ToolInputArgs, ToolOutputArgs, and run, tools can be quickly integrated. It also supports the direct use of langchain Tool interfaces.
For example, functions like the above XXRetrieval can also be registered as a Tool, ultimately called by LLM.`,paraId:35,tocIndex:9},{value:`In the definition of MuAgent, Action is viewed as a specific action or action flow that LLM needs to execute, including LLM information processing, knowledge retrieval, tool invocation, and code execution, etc., constituting a comprehensive and complex dynamic process. For instance, in the React process, we obtained a Tool parameter through LLM, and then "putting the tool parameter into the Tool and executing the call" is an Action, which practically invokes the Tool. Or, we defined an Agent, who orchestrates a fixed agent's Action steps, with the input parameters of this Agent specially designated by the Action. That is to say, whether the parameters are generated by LLM or set by engineering, as long as it involves a specific execution process, it is an Action.`,paraId:36,tocIndex:10},{value:"connector",paraId:37,tocIndex:11},{value:"document_loaders",paraId:38,tocIndex:11},{value:"embeddings",paraId:38,tocIndex:11},{value:"llm_models",paraId:38,tocIndex:11},{value:"orm",paraId:38,tocIndex:11},{value:"sandbox",paraId:38,tocIndex:11},{value:"service",paraId:38,tocIndex:11},{value:"text_splitter",paraId:38,tocIndex:11},{value:"tools",paraId:38,tocIndex:11},{value:"utils",paraId:38,tocIndex:11}]},24193:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"For a complete example, see ",paraId:0,tocIndex:0},{value:"examples/muagent_examples",paraId:0,tocIndex:0},{value:`import os, sys

api_key = "sk-xxx"
api_base_url= "https://api.openai.com/v1"
model_name = "gpt-3.5-turbo"
embed_model = "{{embed_model_name}}"
embed_model_path = "{{embed_model_path}}"
#
os.environ["DUCKDUCKGO_PROXY"] = os.environ.get("DUCKDUCKGO_PROXY") or "socks5://127.0.0.1:13659"
`,paraId:1,tocIndex:1},{value:`from muagent.base_configs.env_config import JUPYTER_WORK_PATH
from muagent.tools import toLangchainTools, TOOL_DICT, TOOL_SETS
from muagent.llm_models.llm_config import EmbedConfig, LLMConfig
from muagent.connector.phase import BasePhase
from muagent.connector.schema import Message


llm_config = LLMConfig(
    model_name=model_name, api_key=api_key,  api_base_url=api_base_url, temperature=0.3,
    stop="**Observation:**"
)
embed_config = EmbedConfig(
    embed_engine="model", embed_model=embed_model, embed_model_path=embed_model_path
)
`,paraId:2,tocIndex:2},{value:`# if you want to analyze a data.csv, please put the csv file into a jupyter_work_path (or your defined path)
import shutil
source_file = 'D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/jupyter_work/employee_data.csv'
shutil.copy(source_file, JUPYTER_WORK_PATH)

# Choose a scenario
phase_name = "baseGroupPhase"
phase = BasePhase(
    phase_name, embed_config=embed_config, llm_config=llm_config
)

# round-1 needs to be completed by code interpreter
query_content = "Confirm whether employee_data.csv exists locally and view its columns and data types; then draw a bar chart"
query = Message(
    role_name="human", role_type="user", tools=[], input_query=query_content,
)
# phase.pre_print(query)  # This function is used to pre-print the Prompt of the Agents' execution chain
output_message, output_memory = phase.step(query)
print(output_memory.to_str_messages(return_all=True, content_key="parsed_output_list"))


# round-2 requires the execution of a tool
tools = toLangchainTools([TOOL_DICT[i] for i in TOOL_SETS if i in TOOL_DICT])
query_content = "Please help me check if the server at 127.0.0.1 had any issues at 10 o'clock, help me to determine"
query = Message(
    role_name="human", role_type="user", tools=tools, input_query=query_content,
)
# phase.pre_print(query)  # This function is used to pre-print the Prompt of the Agents' execution chain
output_message, output_memory = phase.step(query)
print(output_memory.to_str_messages(return_all=True, content_key="parsed_output_list"))
`,paraId:3,tocIndex:3},{value:"Refer to ",paraId:4,tocIndex:4},{value:"How to Customize Phase",paraId:5,tocIndex:4},{value:`Below are some specific scene introductions and usages.
We also welcome everyone to brainstorm and construct some interesting cases.`,paraId:6,tocIndex:5},{value:"Scenarios involving task segmentation and multi-step execution of xAgents",paraId:7,tocIndex:6},{value:`# if you want to analyze a data.csv, please put the csv file into a jupyter_work_path (or your defined path)
import shutil
source_file = 'D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/jupyter_work/employee_data.csv'
shutil.copy(source_file, JUPYTER_WORK_PATH)

# log-level\uFF0Cprint prompt\u548Cllm predict
os.environ["log_verbose"] = "2"

phase_name = "baseTaskPhase"
phase = BasePhase(
phase_name, embed_config=embed_config, llm_config=llm_config,
)


# round-1
query_content = "Check if employee_data.csv exists locally and see what columns and data types it has; then draw a bar chart"
query = Message(
    role_name="human", role_type="user", input_query=query_content,
    )
output_message, output_memory = phase.step(query)
print(output_memory.to_str_messages(return_all=True, content_key="parsed_output_list"))
`,paraId:8,tocIndex:6},{value:"The code interpreter scenario based on React",paraId:9,tocIndex:7},{value:`# if you want to analyze a data.csv, please put the csv file into a jupyter_work_path (or your defined path)
import shutil
source_file = 'D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/jupyter_work/book_data.csv'
shutil.copy(source_file, JUPYTER_WORK_PATH)

# then, create a data analyze phase
phase_name = "codeReactPhase"
phase = BasePhase(
    phase_name, embed_config=embed_config, llm_config=llm_config,
    jupyter_work_path=JUPYTER_WORK_PATH,
)

# round-1
query_content = "Check if 'employee_data.csv' exists locally, view its columns and data types; then draw a bar chart"
query = Message(
    role_name="human", role_type="user",
    role_content=query_content, input_query=query_content, origin_query=query_content,
    )

output_message, output_memory = phase.step(query)
print(output_memory.to_str_messages(return_all=True, content_key="parsed_output_list"))
`,paraId:10,tocIndex:7},{value:"The tool invocation and code interpreter scenario based on the React template",paraId:11,tocIndex:8},{value:`TOOL_SETS = [
     "StockName", "StockInfo",
    ]
tools = toLangchainTools([TOOL_DICT[i] for i in TOOL_SETS if i in TOOL_DICT])

# log-level\uFF0Cprint prompt\u548Cllm predict
os.environ["log_verbose"] = "2"

phase_name = "codeToolReactPhase"

phase = BasePhase(
    phase_name, embed_config=embed_config, llm_config=llm_config,
)

query_content =  "Query the stock code of Kweichow Moutai and acquire the time series data of the last 10 days up to the current date (December 24th, 2023); then use code to draw a line chart and analyze it"

query = Message(role_name="human", role_type="user", input_query=query_content, tools=tools)
output_message, output_memory = phase.step(query)
print(output_memory.to_str_messages(return_all=True, content_key="parsed_output_list"))
`,paraId:12,tocIndex:8},{value:"Knowledge Base Retrieval and Question-Answering Pipeline",paraId:13,tocIndex:9},{value:"example 1",paraId:14,tocIndex:9},{value:`# create your knowledge base
from muagent.service.kb_api import create_kb, upload_files2kb
from muagent.utils.server_utils import run_async
from muagent.orm import create_tables


# use to test, don't create some directory
create_tables()

# create a knowledge base
kb_name = "example_test"
run_async(create_kb(knowledge_base_name=kb_name, vector_store_type="faiss", embed_config=embed_config, kb_root_path=KB_ROOT_PATH))

# add doc to knowledge base
file = os.path.join("D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/sources/docs/langchain_text_10.jsonl")
files = [file]
upload_files2kb(files, kb_name, embed_config, kb_root_path=KB_ROOT_PATH)


## start to chat with knowledge base
# log-level, print prompt, and llm predict
os.environ["log_verbose"] = "0"

## example 1
# set chat phase
phase_name = "docChatPhase"
phase = BasePhase(
    phase_name, embed_config=embed_config, llm_config=llm_config, kb_root_path=KB_ROOT_PATH,
)

# round-1
query_content = "What modules does langchain have?"
query = Message(
    role_name="human", role_type="user", input_query=query_content,
    doc_engine_name=kb_name, score_threshold=1.0, top_k=3
    )

output_message, output_memory = phase.step(query)
print(output_memory.to_str_messages(return_all=True, content_key="parsed_output_list"))

# round-2
query_content = "What is the use of prompts?"
query = Message(
    role_name="human", role_type="user", input_query=query_content,
    doc_engine_name=kb_name, score_threshold=1.0, top_k=3
    )

output_message, output_memory = phase.step(query)
print(output_memory.to_str_messages(return_all=True, content_key="parsed_output_list"))
`,paraId:15,tocIndex:9},{value:"example 2",paraId:16,tocIndex:9},{value:`## Customized register demo
from muagent.tools import DocRetrieval
class BaseDocRetrieval(IMRertrieval):

    def __init__(self, knowledge_base_name: str, search_top=5, score_threshold=1.0, embed_config: EmbedConfig=EmbedConfig(), kb_root_path: str=KB_ROOT_PATH):
        self.knowledge_base_name = knowledge_base_name
        self.search_top = search_top
        self.score_threshold = score_threshold
        self.embed_config = embed_config
        self.kb_root_path = kb_root_path

    def run(self, query: str, search_top=None, score_threshold=None, ):
        docs = DocRetrieval.run(
            query=query, knowledge_base_name=self.knowledge_base_name,
            search_top=search_top or self.search_top,
            score_threshold=score_threshold or self.score_threshold,
            embed_config=self.embed_config,
            kb_root_path=self.kb_root_path
        )
        return docs


doc_retrieval = BaseDocRetrieval(knowledge_base_name=kb_name, score_threshold=1.0, search_top=3, embed_config=embed_config)

# set chat phase
phase_name = "docChatPhase"
phase = BasePhase(
    phase_name, embed_config=embed_config, llm_config=llm_config, kb_root_path=KB_ROOT_PATH,
    doc_retrieval=doc_retrieval
)

# round-1
query_content = "What modules does langchain have?"
query = Message(
    role_name="human", role_type="user", input_query=query_content,
)
output_message, output_memory = phase.step(query)
print(output_memory.to_str_messages(return_all=True, content_key="parsed_output_list"))


# round-2
query_content = "What is the use of prompts?"
query = Message(
    role_name="human", role_type="user", input_query=query_content,
)
output_message, output_memory = phase.step(query)
print(output_memory.to_str_messages(return_all=True, content_key="parsed_output_list"))
`,paraId:17,tocIndex:9},{value:"The code construction Phase in metagpt",paraId:18,tocIndex:10},{value:`# log level, print prompt, and llm predict
os.environ["log_verbose"] = "2"
phase_name = "metagpt_code_development"

phase = BasePhase(
    phase_name, embed_config=embed_config, llm_config=llm_config
)

query_content = "create a snake game"
query = Message(role_name="human", role_type="user", input_query=query_content)
output_message, output_memory = phase.step(query)
print(output_memory.to_str_messages(return_all=True, content_key="parsed_output_list"))
`,paraId:19,tocIndex:10},{value:"Fixed scenario chain, search first then directly answer based on LLM",paraId:20,tocIndex:11},{value:`# log-level\uFF0Cprint prompt\u548Cllm predict
os.environ["log_verbose"] = "2"

# This can be configured when the duckduckgo connection is not available
os.environ["DUCKDUCKGO_PROXY"] = os.environ.get("DUCKDUCKGO_PROXY") or "socks5h://127.0.0.1:13659"
phase_name = "searchChatPhase"
phase = BasePhase(
    phase_name, embed_config=embed_config, llm_config=llm_config
)


# round-1
query_content1 = "Who is the current President of the United States?"
query = Message(
    role_name="human", role_type="user", input_query=query_content1,
    search_engine_name="duckduckgo", score_threshold=1.0, top_k=3
)
output_message, output_memory = phase.step(query)
print(output_memory.to_str_messages(return_all=True, content_key="parsed_output_list"))

# round-2
query_content2 = "Who was the previous president of the United States, and do these two people have any relationship?"
query = Message(
    role_name="human", role_type="user", input_query=query_content2,
    search_engine_name="duckduckgo", score_threshold=1.0, top_k=3
)
output_message, output_memory = phase.step(query)
print(output_memory.to_str_messages(return_all=True, content_key="parsed_output_list"))
`,paraId:21,tocIndex:11},{value:"The tool invocation scene based on the React template",paraId:22,tocIndex:12},{value:`# log-level\uFF0Cprint prompt\u548Cllm predict
os.environ["log_verbose"] = "2"
phase_name = "toolReactPhase"

phase = BasePhase(
    phase_name, embed_config=embed_config, llm_config=llm_config
)

# round-1
tools = toLangchainTools([TOOL_DICT[i] for i in TOOL_SETS if i in TOOL_DICT])
query_content = "Please help me check if there were any issues with the server at 127.0.0.1 at 10 o'clock, I need your assistance in determining this."
query = Message(
    role_name="human", role_type="user", tools=tools, input_query=query_content,
)

# phase.pre_print(query)
output_message, output_memory = phase.step(query)
print(output_memory.to_str_messages(return_all=True, content_key="parsed_output_list"))
`,paraId:23,tocIndex:12}]},66786:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"Doc Retrieval",paraId:0,tocIndex:0},{value:" is the document vector database, which is the most mainstream method for knowledge base construction nowadays. It uses Text Embedding models to vectorize documents and stores them in a vector database. In the future, we will also support querying based on knowledge graph and automatically extracting entities and relationships through large models to explore various complex relationships in data.",paraId:0,tocIndex:0},{value:"Code Retrieval",paraId:1,tocIndex:0},{value:" LLM faces challenges in tasks such as code generation, repair, and component understanding, including lagging code training data and the inability to perceive the dependency structure of code context. During development, understanding existing codebases and dependencies, retrieving related code, querying metadata, etc., can take a significant amount of time. Therefore, we hope to support LLM with code outside of its knowledge system through code structure analysis and code retrieval.",paraId:1,tocIndex:0},{value:"Search Retrieval",paraId:2,tocIndex:0},{value:" In addition to existing document and code knowledge bases, in daily practice, we browse a large amount of web content to acquire more knowledge, helping us understand emerging scenarios, businesses, technologies, etc., hence we integrated duckduckgo search, an open-source search tool, to provide LLM with content beyond its knowledge reserve.",paraId:2,tocIndex:0},{value:`class IMRertrieval:
    def __init__(self,):
        '''
        init your personal attributes
        '''
        pass

    def run(self, ):
        '''
        execute interface, and can use init' attributes
        '''
        pass


class BaseDocRetrieval(IMRertrieval):

    def __init__(self, knowledge_base_name: str, search_top=5, score_threshold=1.0, embed_config: EmbedConfig=EmbedConfig(), kb_root_path: str=KB_ROOT_PATH):
        self.knowledge_base_name = knowledge_base_name
        self.search_top = search_top
        self.score_threshold = score_threshold
        self.embed_config = embed_config
        self.kb_root_path = kb_root_path

    def run(self, query: str, search_top=None, score_threshold=None, ):
        docs = DocRetrieval.run(
            query=query, knowledge_base_name=self.knowledge_base_name,
            search_top=search_top or self.search_top,
            score_threshold=score_threshold or self.score_threshold,
            embed_config=self.embed_config,
            kb_root_path=self.kb_root_path
        )
        return docs
`,paraId:3,tocIndex:1},{value:"import dependcy and set config",paraId:4,tocIndex:2},{value:`# retrieval your customized register demo
from muagent.tools import DocRetrieval
from muagent.base_configs.env_config import JUPYTER_WORK_PATH
from muagent.connector.agents import BaseAgent, ReactAgent, ExecutorAgent, SelectorAgent
from muagent.connector.chains import BaseChain
from muagent.connector.phase import BasePhase
from muagent.connector.schema import Role, Message, ChainConfig
from muagent.llm_models.llm_config import EmbedConfig, LLMConfig
from muagent.base_configs.env_config import KB_ROOT_PATH
from muagent.retrieval.base_retrieval import IMRertrieval


# set your config
api_key = ""
api_base_url= ""
model_name = ""
embed_model = ""
embed_model_path = ""
`,paraId:5,tocIndex:2},{value:"Customed retrieval",paraId:6,tocIndex:2},{value:`# retrieval your customized register demo
from muagent.tools import DocRetrieval
class BaseDocRetrieval(IMRertrieval):

    def __init__(self, knowledge_base_name: str, search_top=5, score_threshold=1.0, embed_config: EmbedConfig=EmbedConfig(), kb_root_path: str=KB_ROOT_PATH):
        self.knowledge_base_name = knowledge_base_name
        self.search_top = search_top
        self.score_threshold = score_threshold
        self.embed_config = embed_config
        self.kb_root_path = kb_root_path

    def run(self, query: str, search_top=None, score_threshold=None, ):
        docs = DocRetrieval.run(
            query=query, knowledge_base_name=self.knowledge_base_name,
            search_top=search_top or self.search_top,
            score_threshold=score_threshold or self.score_threshold,
            embed_config=self.embed_config,
            kb_root_path=self.kb_root_path
        )
        return docs
`,paraId:7,tocIndex:2},{value:"llm&embedding Config",paraId:8,tocIndex:2},{value:`#
llm_config = LLMConfig(
    model_name=model_name, api_key=api_key,  api_base_url=api_base_url, temperature=0.3,
    stop="**Observation:**"
)

embed_config = EmbedConfig(
    embed_engine="model", embed_model=embed_model, embed_model_path=embed_model_path
)

`,paraId:9,tocIndex:2},{value:"Load Document into local",paraId:10,tocIndex:2},{value:`
# create your knowledge base
from muagent.service.kb_api import create_kb, upload_files2kb
from muagent.utils.server_utils import run_async
from muagent.orm import create_tables


# use to test, don't create some directory
create_tables()
# create a knowledge base
kb_name = "example_test"
run_async(create_kb(knowledge_base_name=kb_name, vector_store_type="faiss", embed_config=embed_config, kb_root_path=KB_ROOT_PATH))
# add doc to knowledge base
file = os.path.join("D://project/gitlab/llm/external/ant_code/Codefuse-chatbot/sources/docs/langchain_text_10.jsonl")
files = [file]
upload_files2kb(files, kb_name, embed_config, kb_root_path=KB_ROOT_PATH)
`,paraId:11,tocIndex:2},{value:"Doc RAG QA",paraId:12,tocIndex:2},{value:`doc_retrieval = BaseDocRetrieval(knowledge_base_name=kb_name, score_threshold=1.0, search_top=3, embed_config=embed_config)


llm_config = LLMConfig(
    model_name=model_name, api_key=api_key,  api_base_url=api_base_url, temperature=0.3,
    stop="**Observation:**"
)

embed_config = EmbedConfig(
    embed_engine="model", embed_model=embed_model, embed_model_path=embed_model_path
)


# set chat phase
phase_name = "docChatPhase"
phase = BasePhase(
    phase_name, embed_config=embed_config, llm_config=llm_config, kb_root_path=KB_ROOT_PATH,
    doc_retrieval=doc_retrieval
)

# round-1
query_content = "What modules does langchain have?"
query = Message(
    role_name="human", role_type="user", input_query=query_content,
)
output_message, output_memory = phase.step(query)
print(output_memory.to_str_messages(return_all=True, content_key="parsed_output_list"))

# round-2
query_content = "What is the use of prompts?"
query = Message(
    role_name="human", role_type="user", input_query=query_content,
)
output_message, output_memory = phase.step(query)
print(output_memory.to_str_messages(return_all=True, content_key="parsed_output_list"))
`,paraId:13,tocIndex:2}]},8435:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"In MuAgent, it also supports the registration of Tools by Agents. By registering the BaseToolModel class with Python and writing",paraId:0,tocIndex:0},{value:"Tool_name",paraId:1,tocIndex:0},{value:"Tool_description",paraId:1,tocIndex:0},{value:"ToolInputArgs",paraId:1,tocIndex:0},{value:"ToolOutputArgs",paraId:1,tocIndex:0},{value:"run",paraId:1,tocIndex:0},{value:"and other relevant properties and methods, the quick integration of tools can be achieved. It also supports the direct use of the langchain Tool interface. For example, functions like the aforementioned XXRetrieval can also be registered as a Tool, to be ultimately called by an LLM.",paraId:2,tocIndex:0},{value:`from langchain.agents import Tool
from pydantic import BaseModel, Field
from typing import List, Dict
import json


class BaseToolModel:
    name = "BaseToolModel"
    description = "Tool Description"

    class ToolInputArgs(BaseModel):
        """
        Input for MoveFileTool.
        Tips:
            default control Required, e.g.  key1 is not Required/key2 is Required
        """

        key1: str = Field(default=None, description="hello world!")
        key2: str = Field(..., description="hello world!!")

    class ToolOutputArgs(BaseModel):
        """
        Input for MoveFileTool.
        Tips:
            default control Required, e.g.  key1 is not Required/key2 is Required
        """

        key1: str = Field(default=None, description="hello world!")
        key2: str = Field(..., description="hello world!!")

    @classmethod
    def run(cls, tool_input_args: ToolInputArgs) -> ToolOutputArgs:
        """excute your tool!"""
        pass
`,paraId:3,tocIndex:1},{value:`from pydantic import BaseModel, Field
from typing import List, Dict
import requests
from loguru import logger

class Multiplier(BaseToolModel):
    """
    Tips:
        default control Required, e.g.  key1 is not Required/key2 is Required
    """

    name: str = "Multiplier"
    description: str = """useful for when you need to multiply two numbers together. \\
    The input to this tool should be a comma separated list of numbers of length two, representing the two numbers you want to multiply together. \\
    For example, \`1,2\` would be the input if you wanted to multiply 1 by 2."""

    class ToolInputArgs(BaseModel):
        """Input for Multiplier."""

        # key: str = Field(..., description="\u7528\u6237\u5728\u9AD8\u5FB7\u5730\u56FE\u5B98\u7F51\u7533\u8BF7web\u670D\u52A1API\u7C7B\u578BKEY")
        a: int = Field(..., description="num a")
        b: int = Field(..., description="num b")

    class ToolOutputArgs(BaseModel):
        """Output for Multiplier."""

        res: int = Field(..., description="the result of two nums")

    @staticmethod
    def run(a, b):
        return a * b
`,paraId:4,tocIndex:2},{value:`from langchain.tools import StructuredTool
from muagent.tools import (
    WeatherInfo, Multiplier, toLangchainTools,
    TOOL_DICT, TOOL_SETS
)

# Function exec
tools =  [
    StructuredTool(
            name=Multiplier.name,
            func=Multiplier.run,
            description=Multiplier.description,
            args_schema=Multiplier.ToolInputArgs,
        ),
        StructuredTool(
            name=WeatherInfo.name,
            func=WeatherInfo.run,
            description=WeatherInfo.description,
            args_schema=WeatherInfo.ToolInputArgs,
        )
        ]

tools = toLangchainTools([TOOL_DICT["Multiplier"]])

# tool run Test
print(tools[0].func(1,2))
`,paraId:5,tocIndex:3}]},38122:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"This project is an open-source AI intelligent assistant, specifically designed for the entire lifecycle of software development, covering design, coding, testing, deployment, and operations. Through knowledge retrieval, tool utilization, and sandbox execution, Codefuse-ChatBot can not only answer professional questions you encounter during the development process but also coordinate multiple independent, dispersed platforms through a conversational interface.",paraId:0},{value:"\u{1F91D} Introduction",paraId:1,tocIndex:0},{value:"\u{1F9ED} Technical Route",paraId:2,tocIndex:0},{value:"\u{1F4A1} The aim of this project is to construct an AI intelligent assistant for the entire lifecycle of software development, covering design, coding, testing, deployment, and operations, through Retrieval Augmented Generation (RAG), Tool Learning, and sandbox environments. It transitions gradually from the traditional development and operations mode of querying information from various sources and operating on standalone, disparate platforms to an intelligent development and operations mode based on large-model Q&A, changing people's development and operations habits.",paraId:3,tocIndex:1},{value:"\u{1F9E0} Intelligent Scheduling Core:",paraId:4,tocIndex:1},{value:" Constructed a well-integrated scheduling core system that supports multi-mode one-click configuration, simplifying the operational process.",paraId:4,tocIndex:1},{value:"Use Introduction",paraId:5,tocIndex:1},{value:"\u{1F4BB} Comprehensive Code Repository Analysis:",paraId:4,tocIndex:1},{value:" Achieved in-depth understanding at the repository level and coding and generation at the project file level, enhancing development efficiency.",paraId:4,tocIndex:1},{value:"\u{1F4C4} Enhanced Document Analysis:",paraId:4,tocIndex:1},{value:" Integrated document knowledge bases with knowledge graphs, providing deeper support for document analysis through enhanced retrieval and reasoning.",paraId:4,tocIndex:1},{value:"\u{1F527} Industry-Specific Knowledge:",paraId:4,tocIndex:1},{value:" Tailored a specialized knowledge base for the DevOps domain, supporting the self-service one-click construction of industry-specific knowledge bases for convenience and practicality.",paraId:4,tocIndex:1},{value:"\u{1F916} Compatible Models for Specific Verticals:",paraId:4,tocIndex:1},{value:" Designed small models specifically for the DevOps field, ensuring compatibility with related DevOps platforms and promoting the integration of the technological ecosystem.",paraId:4,tocIndex:1},{value:"\u{1F30D} Relying on open-source LLM and Embedding models, this project can achieve offline private deployments based on open-source models. Additionally, this project also supports the use of the OpenAI API.",paraId:6,tocIndex:1},{value:"Access Demo",paraId:7,tocIndex:1},{value:'\u{1F465} The core development team has been long-term focused on research in the AIOps + NLP domain. We initiated the CodefuseGPT project, hoping that everyone could contribute high-quality development and operations documents widely, jointly perfecting this solution to achieve the goal of "Making Development Seamless for Everyone."',paraId:8,tocIndex:1},{value:`
  `,paraId:9},{value:"\u{1F30D} Relying on open-source LLM and Embedding models, this project can achieve offline private deployments based on open-source models. Additionally, this project also supports the use of the OpenAI API.",paraId:10,tocIndex:1},{value:'\u{1F465} The core development team has been long-term focused on research in the AIOps + NLP domain. We initiated the DevOpsGPT project, hoping that everyone could contribute high-quality development and operations documents widely, jointly perfecting this solution to achieve the goal of "Making Development Seamless for Everyone."',paraId:11,tocIndex:1},{value:`
  `,paraId:9},{value:"\u{1F9E0} ",paraId:12,tocIndex:2},{value:"Multi-Agent Schedule Core:",paraId:12,tocIndex:2},{value:" Easily configurable to create interactive intelligent agents.",paraId:12,tocIndex:2},{value:"\u{1F577}\uFE0F ",paraId:12,tocIndex:2},{value:"Multi Source Web Crawl:",paraId:12,tocIndex:2},{value:" Offers the capability to crawl specified URLs for collecting the required information.",paraId:12,tocIndex:2},{value:"\u{1F5C2}\uFE0F ",paraId:12,tocIndex:2},{value:"Data Processor:",paraId:12,tocIndex:2},{value:" Effortlessly handles document loading, data cleansing, and text segmentation, integrating data from different sources.",paraId:12,tocIndex:2},{value:"\u{1F524} ",paraId:12,tocIndex:2},{value:"Text Embedding & Index:",paraId:12,tocIndex:2},{value:"\uFF1AUsers can easily upload files for document retrieval, optimizing the document analysis process.",paraId:12,tocIndex:2},{value:"\u{1F5C4}\uFE0F ",paraId:12,tocIndex:2},{value:"Vector Database & Graph Database:",paraId:12,tocIndex:2},{value:" Provides flexible and powerful data management solutions.",paraId:12,tocIndex:2},{value:"\u{1F4DD} ",paraId:12,tocIndex:2},{value:"Prompt Control & Management:",paraId:12,tocIndex:2},{value:"\uFF1APrecisely defines the contextual environment for intelligent agents.",paraId:12,tocIndex:2},{value:"\u{1F6A7} ",paraId:12,tocIndex:2},{value:"SandBox:",paraId:12,tocIndex:2},{value:"\uFF1ASafely executes code compilation and actions.",paraId:12,tocIndex:2},{value:"\u{1F4AC} ",paraId:12,tocIndex:2},{value:"LLM:",paraId:12,tocIndex:2},{value:"\uFF1ASupports various open-source models and LLM interfaces.",paraId:12,tocIndex:2},{value:"\u{1F6E0}\uFE0F ",paraId:12,tocIndex:2},{value:"API Management:\uFF1A",paraId:12,tocIndex:2},{value:" Enables rapid integration of open-source components and operational platforms.",paraId:12,tocIndex:2},{value:"For implementation details, see: ",paraId:13,tocIndex:2},{value:"Technical Route Details",paraId:14,tocIndex:2}]},8320:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"Leveraging open-source LLMs (Large Language Models) and Embedding models, this project enables offline private deployment based on open-source models.",paraId:0,tocIndex:0},{value:"In addition, the project supports the invocation of OpenAI API.",paraId:1,tocIndex:0},{value:"Example of model address configuration, modification of the model_config.py configuration:",paraId:2,tocIndex:1},{value:`# Recommendation: Use Hugging Face models, preferably the chat models, and avoid using base models, which may not produce correct outputs.
# Note: When both \`llm_model_dict\` and \`VLLM_MODEL_DICT\` are present, the model configuration in \`VLLM_MODEL_DICT\` takes precedence.
# Example of \`llm_model_dict\` configuration:

# 1. If the model is placed under the ~/codefuse-chatbot/llm_models path
# Suppose the model address is as follows
model_dir: ~/codefuse-chatbot/llm_models/THUDM/chatglm-6b

# The reference configuration is as follows
llm_model_dict = {
    "chatglm-6b": {
        "local_model_path": "THUDM/chatglm-6b",
        "api_base_url": "http://localhost:8888/v1",  # "name"\u4FEE\u6539\u4E3Afastchat\u670D\u52A1\u4E2D\u7684"api_base_url"
        "api_key": "EMPTY"
    }
}

VLLM_MODEL_DICT = {
 'chatglm2-6b':  "THUDM/chatglm-6b",
}

# or If the model address is as follows
model_dir: ~/codefuse-chatbot/llm_models/chatglm-6b
llm_model_dict = {
    "chatglm-6b": {
        "local_model_path": "chatglm-6b",
        "api_base_url": "http://localhost:8888/v1",  # "name"\u4FEE\u6539\u4E3Afastchat\u670D\u52A1\u4E2D\u7684"api_base_url"
        "api_key": "EMPTY"
    }
}

VLLM_MODEL_DICT = {
 'chatglm2-6b':  "chatglm-6b",
}

# 2. If you do not wish to move the model to ~/codefuse-chatbot/llm_models
# Also, delete the related code below \`Model Path Reset\`, see model_config.py for details.
# Suppose the model address is as follows
model_dir: ~/THUDM/chatglm-6b
# The reference configuration is as follows
llm_model_dict = {
    "chatglm-6b": {
        "local_model_path": "your personl dir/THUDM/chatglm-6b",
        "api_base_url": "http://localhost:8888/v1",  # "name"\u4FEE\u6539\u4E3Afastchat\u670D\u52A1\u4E2D\u7684"api_base_url"
        "api_key": "EMPTY"
    }
}

VLLM_MODEL_DICT = {
 'chatglm2-6b':  "your personl dir/THUDM/chatglm-6b",
}
`,paraId:3,tocIndex:1},{value:`# 3. Specify the model service to be launched, keeping both consistent
LLM_MODEL = "chatglm-6b"
LLM_MODELs = ["chatglm-6b"]
`,paraId:4,tocIndex:1},{value:`# Modification of server_config.py configuration, if LLM_MODELS does not have multiple model configurations, no additional settings are needed.
# Modify the configuration of server_config.py#FSCHAT_MODEL_WORKERS
"model_name": {'host': DEFAULT_BIND_HOST, 'port': 20057}
`,paraId:5,tocIndex:1},{value:"\u91CF\u5316\u6A21\u578B\u63A5\u5165",paraId:6,tocIndex:1},{value:`# If you need to support the codellama-34b-int4 model, you need to patch fastchat
cp examples/gptq.py ~/site-packages/fastchat/modules/gptq.py
# If you need to support the qwen-72b-int4 model, you need to patch fastchat
cp examples/gptq.py ~/site-packages/fastchat/modules/gptq.py

# Quantization requires modification of the llm_api.py configuration
# Uncomment \`kwargs["gptq_wbits"] = 4\` in examples/llm_api.py#559
`,paraId:7,tocIndex:1},{value:`# Modification of model_config.py configuration
# ONLINE_LLM_MODEL
# Other interface development comes from the langchain-chatchat project, untested due to lack of relevant accounts.
# Specify the model service to be launched, keeping both consistent
LLM_MODEL = "gpt-3.5-turbo"
LLM_MODELs = ["gpt-3.5-turbo"]
`,paraId:8,tocIndex:2},{value:"\u5916\u90E8\u5927\u6A21\u578B\u63A5\u53E3\u63A5\u5165\u793A\u4F8B",paraId:9,tocIndex:2},{value:`# 1. Implement a new model access class
# Refer to ~/examples/model_workers/openai.py#ExampleWorker
# Implementing the do_chat function will enable the use of LLM capabilities

class XXWorker(ApiModelWorker):
    def __init__(
            self,
            *,
            controller_addr: str = None,
            worker_addr: str = None,
            model_names: List[str] = ["gpt-3.5-turbo"],
            version: str = "gpt-3.5",
            **kwargs,
    ):
        kwargs.update(model_names=model_names, controller_addr=controller_addr, worker_addr=worker_addr)
        kwargs.setdefault("context_len", 16384) #TODO 16K\u6A21\u578B\u9700\u8981\u6539\u621016384
        super().__init__(**kwargs)
        self.version = version

    def do_chat(self, params: ApiChatParams) -> Dict:
        '''
        \u6267\u884CChat\u7684\u65B9\u6CD5\uFF0C\u9ED8\u8BA4\u4F7F\u7528\u6A21\u5757\u91CC\u9762\u7684chat\u51FD\u6570\u3002
        :params.messages : [
            {"role": "user", "content": "hello"},
            {"role": "assistant", "content": "hello"}
            ]
        :params.xx: \u8BE6\u60C5\u89C1 ApiChatParams
        \u8981\u6C42\u8FD4\u56DE\u5F62\u5F0F\uFF1A{"error_code": int, "text": str}
        '''
        return {"error_code": 500, "text": f"{self.model_names[0]}\u672A\u5B9E\u73B0chat\u529F\u80FD"}


# Finally, complete the registration in ~/examples/model_workers/__init__.py
# from .xx import XXWorker

# 2. Complete access through an existing model access class
# Or directly use the existing relevant large model class for use (lacking relevant account testing, community contributions after testing are welcome)
`,paraId:10,tocIndex:2},{value:`# Modification of model_config.py#ONLINE_LLM_MODEL configuration
# Enter exclusive model details: version, api_base_url, api_key, provider (consistent with the class name above)
ONLINE_LLM_MODEL = {
    # Online models. Please set different ports for each online API in server_config.
    "openai-api": {
        "model_name": "gpt-3.5-turbo",
        "api_base_url": "https://api.openai.com/v1",
        "api_key": "",
        "openai_proxy": "",
    },
    "example": {
        "version": "gpt-3.5",  # Using openai interface as an example
        "api_base_url": "https://api.openai.com/v1",
        "api_key": "",
        "provider": "ExampleWorker",
    },
}
`,paraId:11,tocIndex:2},{value:`# start llm-service (optional)  - Launch the large model service separately
python examples/llm_api.py
`,paraId:12,tocIndex:3},{value:`# Test
import openai
# openai.api_key = "EMPTY" # Not support yet
openai.api_base = "http://127.0.0.1:8888/v1"
# Select the model you launched
model = "example"
# create a chat completion
completion = openai.ChatCompletion.create(
    model=model,
    messages=[{"role": "user", "content": "Hello! What is your name? "}],
    max_tokens=100,
)
# print the completion
print(completion.choices[0].message.content)
# Once the correct output is confirmed, LLM can be accessed normally.
`,paraId:13,tocIndex:3},{value:"or",paraId:14,tocIndex:3},{value:`# model_config.py#USE_FASTCHAT - Determine whether to integrate local models via fastchat
USE_FASTCHAT = "gpt" not in LLM_MODEL
python start.py #221 Automatically executes python llm_api.py
`,paraId:15,tocIndex:3}]},77186:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:`To deploy private models, please install the NVIDIA driver by yourself.
This project has been tested on Python 3.9.18 and CUDA 11.7 environments, as well as on Windows and macOS systems with x86 architecture.
For Docker installation, private LLM access, and related startup issues, see: `,paraId:0,tocIndex:0},{value:"Start-detail...",paraId:1,tocIndex:0},{value:"Preparation of Python environment",paraId:2,tocIndex:0},{value:"It is recommended to use conda to manage the python environment (optional)",paraId:3,tocIndex:0},{value:`# Prepare conda environment
conda create --name Codefusegpt python=3.9
conda activate Codefusegpt
`,paraId:4,tocIndex:0},{value:"Install related dependencies",paraId:5,tocIndex:0},{value:`cd Codefuse-ChatBot
# python=3.9\uFF0Cuse notebook-latest\uFF0Cpython=3.8 use notebook==6.5.5
pip install -r requirements.txt
`,paraId:6,tocIndex:0},{value:"Start the Service",paraId:7,tocIndex:0},{value:`# After configuring server_config.py, you can start with just one click.
cd examples
bash start.sh
# you can config your llm model and embedding model, then choose the "\u542F\u52A8\u5BF9\u8BDD\u670D\u52A1"
`,paraId:8,tocIndex:0},{value:`
  `,paraId:9},{value:"Or ",paraId:10,tocIndex:0},{value:"python start.py",paraId:10,tocIndex:0},{value:" by ",paraId:10,tocIndex:0},{value:"old version to start",paraId:11,tocIndex:0},{value:`
More details about accessing LLM Moldes`,paraId:10,tocIndex:0},{value:"More Details...",paraId:12,tocIndex:0}]},23039:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:`
  `,paraId:0},{value:"Roadmap Overview",paraId:1,tocIndex:0},{value:` Sandbox Environment
`,paraId:2,tocIndex:0},{value:" Isolated sandbox environment for code execution",paraId:3,tocIndex:0},{value:" File upload and download",paraId:3,tocIndex:0},{value:" Support for Java execution environment",paraId:3,tocIndex:0},{value:` Vector Database & Retrieval
`,paraId:2,tocIndex:0},{value:" Task retrieval",paraId:4,tocIndex:0},{value:" Tool retrieval",paraId:4,tocIndex:0},{value:" Prompt Management",paraId:2,tocIndex:0},{value:" Memory Management",paraId:2,tocIndex:0},{value:` Multi Agent Framework
`,paraId:2,tocIndex:0},{value:" PRD (Product Requirement Document), system analysis, interface design",paraId:5,tocIndex:0},{value:" Generate code based on requirement documents, system analysis, and interface design",paraId:5,tocIndex:0},{value:" Automated testing, automated debugger",paraId:5,tocIndex:0},{value:" Operations process integration (ToolLearning)",paraId:5,tocIndex:0},{value:" Fully automated end-to-end process",paraId:5,tocIndex:0},{value:" Integration with LLM based on fastchat",paraId:2,tocIndex:0},{value:" Integration with Text Embedding based on sentencebert",paraId:2,tocIndex:0},{value:" Improved vector loading speed",paraId:2,tocIndex:0},{value:` Connector
`,paraId:2,tocIndex:0},{value:" React Mode based on langchain",paraId:6,tocIndex:0},{value:" Tool retrieval completed with langchain",paraId:6,tocIndex:0},{value:` General Capability for Web Crawl
`,paraId:2,tocIndex:0},{value:" Technical documentation: Zhihu, CSDN, Alibaba Cloud Developer Forum, Tencent Cloud Developer Forum, etc.",paraId:7,tocIndex:0},{value:" Issue document",paraId:7,tocIndex:0},{value:" SDK Library Document",paraId:7,tocIndex:0},{value:"v0.0",paraId:8,tocIndex:0},{value:` Sandbox Environment
`,paraId:9,tocIndex:0},{value:" Isolated sandbox environment for code execution",paraId:10,tocIndex:0},{value:" Integration with LLM based on fastchat",paraId:9,tocIndex:0},{value:" Integration with Text Embedding based on sentencebert",paraId:9,tocIndex:0},{value:" General Capability for Web Crawl: Technical documentation: Zhihu, CSDN, Alibaba Cloud Developer Forum, Tencent Cloud Developer Forum, etc.",paraId:9,tocIndex:0},{value:`Done
`,paraId:11,tocIndex:0},{value:"v0.1",paraId:12,tocIndex:0},{value:" Sandbox Environment: File upload and download",paraId:13,tocIndex:0},{value:` Vector Database & Retrieval
`,paraId:13,tocIndex:0},{value:" Task retrieval",paraId:14,tocIndex:0},{value:" Tool retrieval",paraId:14,tocIndex:0},{value:` Connector
`,paraId:13,tocIndex:0},{value:" React Mode based on langchain",paraId:15,tocIndex:0},{value:" Integration with Text Embedding based on sentencebert: Improved vector loading speed",paraId:13,tocIndex:0},{value:`Done
`,paraId:16,tocIndex:0},{value:"v0.2",paraId:17,tocIndex:0},{value:" Prompt Management",paraId:18,tocIndex:0},{value:" Memory Management",paraId:18,tocIndex:0},{value:" Vector Database & Retrieval",paraId:18,tocIndex:0},{value:`Done
`,paraId:19,tocIndex:0},{value:"v0.3",paraId:20,tocIndex:0},{value:` Sandbox Environment
`,paraId:21,tocIndex:0},{value:" Support for Java execution environment",paraId:22,tocIndex:0},{value:` Multi Agent
`,paraId:21,tocIndex:0},{value:" PRD (Product Requirement Document), system analysis, interface design",paraId:23,tocIndex:0},{value:" Generate code based on requirement documents, system analysis, and interface design",paraId:23,tocIndex:0},{value:" Automated testing, automated debugger",paraId:23,tocIndex:0},{value:" Operations process integration (ToolLearning)",paraId:23,tocIndex:0},{value:" Fully automated end-to-end process",paraId:23,tocIndex:0},{value:` General Capability for Web Crawl
`,paraId:21,tocIndex:0},{value:" Issue document",paraId:24,tocIndex:0},{value:" SDK Library Document",paraId:24,tocIndex:0},{value:`DDL\uFF1A 2024.12.31
`,paraId:25,tocIndex:0}]},74150:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"If you need to deploy a privatized model, please install the NVIDIA driver yourself.",paraId:0},{value:"It is recommended to use conda to manage the python environment (optional)",paraId:1,tocIndex:0},{value:`# Prepare conda environment
conda create --name Codefusegpt python=3.9
conda activate Codefusegpt
`,paraId:2,tocIndex:0},{value:"Install related dependencies",paraId:3,tocIndex:0},{value:`cd Codefuse-ChatBot
pip install -r requirements.txt
`,paraId:4,tocIndex:0},{value:`Windows Docker installation:
`,paraId:5,tocIndex:1},{value:"Docker Desktop for Windows",paraId:5,tocIndex:1},{value:" supports 64-bit versions of Windows 10 Pro with Hyper-V enabled (Hyper-V is not required for versions v1903 and above), or 64-bit versions of Windows 10 Home v1903 and above.",paraId:5,tocIndex:1},{value:"\u3010\u5168\u9762\u8BE6\u7EC6\u3011Windows10 Docker \u5B89\u88C5\u8BE6\u7EC6\u6559\u7A0B",paraId:6,tocIndex:1},{value:"Docker \u4ECE\u5165\u95E8\u5230\u5B9E\u8DF5",paraId:6,tocIndex:1},{value:"Handling 'Docker Desktop requires the Server service to be enabled'",paraId:6,tocIndex:1},{value:"\u5B89\u88C5 wsl \u6216\u8005\u7B49\u62A5\u9519\u63D0\u793A",paraId:6,tocIndex:1},{value:`Linux Docker installation\uFF1A
Linux installation is relatively simple, please search Baidu/Google for installation guides.`,paraId:7,tocIndex:1},{value:"Mac Docker installation",paraId:8,tocIndex:1},{value:"Docker \u4ECE\u5165\u95E8\u5230\u5B9E\u8DF5",paraId:9,tocIndex:1},{value:`# Build the image for the sandbox environment, see above for notebook version issues
bash docker_build.sh
`,paraId:10,tocIndex:1},{value:`If you need to use open-source LLM and Embedding models, you can download them from HuggingFace.
Here we take THUDM/chatglm2-6b and text2vec-base-chinese as examples:`,paraId:11,tocIndex:2},{value:`# install git-lfs
git lfs install

# install LLM-model
git lfs clone https://huggingface.co/THUDM/chatglm2-6b
cp ~/THUDM/chatglm2-6b ~/codefuse-chatbot/llm_models/

# install Embedding-model
git lfs clone https://huggingface.co/shibing624/text2vec-base-chinese
cp ~/shibing624/text2vec-base-chinese ~/codefuse-chatbot/embedding_models/
`,paraId:12,tocIndex:2},{value:`# Modify the basic configuration for service startup
cd configs
cp model_config.py.example model_config.py
cp server_config.py.example server_config.py

# model_config#11~12 If you need to use the OpenAI interface, the OpenAI interface key
os.environ["OPENAI_API_KEY"] = "sk-xxx"
# Replace with the api_base_url you need
os.environ["API_BASE_URL"] = "https://api.openai.com/v1"

# vi model_config#LLM_MODEL The language model you need to choose
LLM_MODEL = "gpt-3.5-turbo"
LLM_MODELs = ["gpt-3.5-turbo"]

# vi model_config#EMBEDDING_MODEL The private vector model you need to choose
EMBEDDING_ENGINE = 'model'
EMBEDDING_MODEL = "text2vec-base"

# Example of vector model access, modify model_config#embedding_model_dict
# If the model directory is:
model_dir: ~/codefuse-chatbot/embedding_models/shibing624/text2vec-base-chinese
# Configure as follows
"text2vec-base": "shibing624/text2vec-base-chinese"


# vi server_config#8~14, It's recommended to use a container to start the service to prevent environment conflicts when installing other dependencies using the codeInterpreter feature
DOCKER_SERVICE = True
# Whether to use a container sandbox
SANDBOX_DO_REMOTE = True
`,paraId:13,tocIndex:3},{value:"By default, only the webui-related services are started, and fastchat is not started (optional).",paraId:14,tocIndex:4},{value:`# If you need to support the codellama-34b-int4 model, you need to patch fastchat
# cp examples/gptq.py ~/site-packages/fastchat/modules/gptq.py
# Modify examples/llm_api.py#258 to kwargs={"gptq_wbits": 4},

# start llm-service (optional)
python examples/llm_api.py
`,paraId:15,tocIndex:4},{value:"For more LLM integration methods, see",paraId:16,tocIndex:4},{value:"more details...",paraId:17,tocIndex:4},{value:`# After completing the server_config.py configuration, you can start with one click
cd examples
python start.py
`,paraId:18,tocIndex:4}]},52910:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:" ",paraId:0},{value:"       ",paraId:0},{value:"DevOps-Eval is a comprehensive evaluation suite specifically designed for foundation models in the DevOps field. We hope DevOps-Eval could help developers, especially in the DevOps field, track the progress and analyze the important strengths/shortcomings of their models.",paraId:1},{value:"\u{1F4DA} This repo contains questions and exercises related to DevOps, including the AIOps, ToolLearning;",paraId:2},{value:"\u{1F4A5}\uFE0F There are currently ",paraId:3},{value:"7486",paraId:3},{value:" multiple-choice questions spanning 8 diverse general categories, as shown ",paraId:3},{value:"below",paraId:3},{value:".",paraId:3},{value:"\u{1F525} There are a total of ",paraId:4},{value:"2840",paraId:4},{value:" samples in the AIOps subcategory, covering scenarios such as ",paraId:4},{value:"log parsing",paraId:4},{value:", ",paraId:4},{value:"time series anomaly detection",paraId:4},{value:", ",paraId:4},{value:"time series classification",paraId:4},{value:", ",paraId:4},{value:"time series forecasting",paraId:4},{value:", and ",paraId:4},{value:"root cause analysis",paraId:4},{value:".",paraId:4},{value:"\u{1F527} There are a total of ",paraId:5},{value:"1509",paraId:5},{value:" samples in the ToolLearning subcategory, covering 239 tool scenes across 59 fields.",paraId:5},{value:" ",paraId:6},{value:" ",paraId:6},{value:"Below are zero-shot and five-shot accuracies from the models that we evaluate in the initial release. We note that five-shot performance is better than zero-shot for many instruction-tuned models.",paraId:7,tocIndex:0},{value:"ModelName",paraId:8,tocIndex:2},{value:"plan",paraId:8,tocIndex:2},{value:"code",paraId:8,tocIndex:2},{value:"build",paraId:8,tocIndex:2},{value:"test",paraId:8,tocIndex:2},{value:"release",paraId:8,tocIndex:2},{value:"deploy",paraId:8,tocIndex:2},{value:"operate",paraId:8,tocIndex:2},{value:"monitor",paraId:8,tocIndex:2},{value:"AVG",paraId:8,tocIndex:2},{value:"DevOpsPal-14B-Chat",paraId:8,tocIndex:2},{value:"60.61",paraId:8,tocIndex:2},{value:"78.35",paraId:8,tocIndex:2},{value:"84.86",paraId:8,tocIndex:2},{value:"84.65",paraId:8,tocIndex:2},{value:"87.26",paraId:8,tocIndex:2},{value:"82.75",paraId:8,tocIndex:2},{value:"69.89",paraId:8,tocIndex:2},{value:"79.17",paraId:8,tocIndex:2},{value:"78.23",paraId:8,tocIndex:2},{value:"DevOpsPal-14B-Base",paraId:8,tocIndex:2},{value:"54.55",paraId:8,tocIndex:2},{value:"77.82",paraId:8,tocIndex:2},{value:"83.49",paraId:8,tocIndex:2},{value:"85.96",paraId:8,tocIndex:2},{value:"86.32",paraId:8,tocIndex:2},{value:"81.96",paraId:8,tocIndex:2},{value:"71.18",paraId:8,tocIndex:2},{value:"82.41",paraId:8,tocIndex:2},{value:"78.23",paraId:8,tocIndex:2},{value:"Qwen-14B-Chat",paraId:8,tocIndex:2},{value:"60.61",paraId:8,tocIndex:2},{value:"75.4",paraId:8,tocIndex:2},{value:"85.32",paraId:8,tocIndex:2},{value:"84.21",paraId:8,tocIndex:2},{value:"89.62",paraId:8,tocIndex:2},{value:"82.75",paraId:8,tocIndex:2},{value:"69.57",paraId:8,tocIndex:2},{value:"80.56",paraId:8,tocIndex:2},{value:"77.18",paraId:8,tocIndex:2},{value:"Qwen-14B-Base",paraId:8,tocIndex:2},{value:"57.58",paraId:8,tocIndex:2},{value:"73.81",paraId:8,tocIndex:2},{value:"84.4",paraId:8,tocIndex:2},{value:"85.53",paraId:8,tocIndex:2},{value:"86.32",paraId:8,tocIndex:2},{value:"81.18",paraId:8,tocIndex:2},{value:"70.05",paraId:8,tocIndex:2},{value:"80.09",paraId:8,tocIndex:2},{value:"76.19",paraId:8,tocIndex:2},{value:"Baichuan2-13B-Base",paraId:8,tocIndex:2},{value:"60.61",paraId:8,tocIndex:2},{value:"69.42",paraId:8,tocIndex:2},{value:"79.82",paraId:8,tocIndex:2},{value:"79.82",paraId:8,tocIndex:2},{value:"82.55",paraId:8,tocIndex:2},{value:"81.18",paraId:8,tocIndex:2},{value:"70.37",paraId:8,tocIndex:2},{value:"83.8",paraId:8,tocIndex:2},{value:"73.73",paraId:8,tocIndex:2},{value:"Baichuan2-13B-Chat",paraId:8,tocIndex:2},{value:"60.61",paraId:8,tocIndex:2},{value:"68.43",paraId:8,tocIndex:2},{value:"77.98",paraId:8,tocIndex:2},{value:"80.7",paraId:8,tocIndex:2},{value:"81.6",paraId:8,tocIndex:2},{value:"83.53",paraId:8,tocIndex:2},{value:"67.63",paraId:8,tocIndex:2},{value:"84.72",paraId:8,tocIndex:2},{value:"72.9",paraId:8,tocIndex:2},{value:"DevOpsPal-7B-Chat",paraId:8,tocIndex:2},{value:"54.55",paraId:8,tocIndex:2},{value:"69.11",paraId:8,tocIndex:2},{value:"83.94",paraId:8,tocIndex:2},{value:"82.02",paraId:8,tocIndex:2},{value:"76.89",paraId:8,tocIndex:2},{value:"80",paraId:8,tocIndex:2},{value:"64.73",paraId:8,tocIndex:2},{value:"77.78",paraId:8,tocIndex:2},{value:"71.92",paraId:8,tocIndex:2},{value:"DevOpsPal-7B-Base",paraId:8,tocIndex:2},{value:"54.55",paraId:8,tocIndex:2},{value:"68.96",paraId:8,tocIndex:2},{value:"82.11",paraId:8,tocIndex:2},{value:"78.95",paraId:8,tocIndex:2},{value:"80.66",paraId:8,tocIndex:2},{value:"76.47",paraId:8,tocIndex:2},{value:"65.54",paraId:8,tocIndex:2},{value:"78.7",paraId:8,tocIndex:2},{value:"71.69",paraId:8,tocIndex:2},{value:"Qwen-7B-Base",paraId:8,tocIndex:2},{value:"53.03",paraId:8,tocIndex:2},{value:"68.13",paraId:8,tocIndex:2},{value:"78.9",paraId:8,tocIndex:2},{value:"75.44",paraId:8,tocIndex:2},{value:"80.19",paraId:8,tocIndex:2},{value:"80",paraId:8,tocIndex:2},{value:"65.06",paraId:8,tocIndex:2},{value:"80.09",paraId:8,tocIndex:2},{value:"71.09",paraId:8,tocIndex:2},{value:"Qwen-7B-Chat",paraId:8,tocIndex:2},{value:"57.58",paraId:8,tocIndex:2},{value:"66.01",paraId:8,tocIndex:2},{value:"80.28",paraId:8,tocIndex:2},{value:"79.82",paraId:8,tocIndex:2},{value:"76.89",paraId:8,tocIndex:2},{value:"77.65",paraId:8,tocIndex:2},{value:"62.64",paraId:8,tocIndex:2},{value:"79.17",paraId:8,tocIndex:2},{value:"69.75",paraId:8,tocIndex:2},{value:"Baichuan2-7B-Chat",paraId:8,tocIndex:2},{value:"54.55",paraId:8,tocIndex:2},{value:"63.66",paraId:8,tocIndex:2},{value:"77.98",paraId:8,tocIndex:2},{value:"76.32",paraId:8,tocIndex:2},{value:"71.7",paraId:8,tocIndex:2},{value:"73.33",paraId:8,tocIndex:2},{value:"59.42",paraId:8,tocIndex:2},{value:"79.63",paraId:8,tocIndex:2},{value:"66.97",paraId:8,tocIndex:2},{value:"Internlm-7B-Chat",paraId:8,tocIndex:2},{value:"60.61",paraId:8,tocIndex:2},{value:"62.15",paraId:8,tocIndex:2},{value:"77.06",paraId:8,tocIndex:2},{value:"76.32",paraId:8,tocIndex:2},{value:"66.98",paraId:8,tocIndex:2},{value:"74.51",paraId:8,tocIndex:2},{value:"60.39",paraId:8,tocIndex:2},{value:"78.24",paraId:8,tocIndex:2},{value:"66.27",paraId:8,tocIndex:2},{value:"Baichuan2-7B-Base",paraId:8,tocIndex:2},{value:"56.06",paraId:8,tocIndex:2},{value:"62.45",paraId:8,tocIndex:2},{value:"75.69",paraId:8,tocIndex:2},{value:"70.61",paraId:8,tocIndex:2},{value:"74.06",paraId:8,tocIndex:2},{value:"69.8",paraId:8,tocIndex:2},{value:"61.67",paraId:8,tocIndex:2},{value:"75.93",paraId:8,tocIndex:2},{value:"66.21",paraId:8,tocIndex:2},{value:"Internlm-7B-Base",paraId:8,tocIndex:2},{value:"54.55",paraId:8,tocIndex:2},{value:"58.29",paraId:8,tocIndex:2},{value:"79.36",paraId:8,tocIndex:2},{value:"78.95",paraId:8,tocIndex:2},{value:"77.83",paraId:8,tocIndex:2},{value:"70.59",paraId:8,tocIndex:2},{value:"65.86",paraId:8,tocIndex:2},{value:"75.93",paraId:8,tocIndex:2},{value:"65.99",paraId:8,tocIndex:2},{value:"ModelName",paraId:9,tocIndex:3},{value:"plan",paraId:9,tocIndex:3},{value:"code",paraId:9,tocIndex:3},{value:"build",paraId:9,tocIndex:3},{value:"test",paraId:9,tocIndex:3},{value:"release",paraId:9,tocIndex:3},{value:"deploy",paraId:9,tocIndex:3},{value:"operate",paraId:9,tocIndex:3},{value:"monitor",paraId:9,tocIndex:3},{value:"AVG",paraId:9,tocIndex:3},{value:"DevOpsPal-14B-Chat",paraId:9,tocIndex:3},{value:"63.64",paraId:9,tocIndex:3},{value:"79.49",paraId:9,tocIndex:3},{value:"81.65",paraId:9,tocIndex:3},{value:"85.96",paraId:9,tocIndex:3},{value:"86.79",paraId:9,tocIndex:3},{value:"86.67",paraId:9,tocIndex:3},{value:"72.95",paraId:9,tocIndex:3},{value:"81.48",paraId:9,tocIndex:3},{value:"79.69",paraId:9,tocIndex:3},{value:"DevOpsPal-14B-Base",paraId:9,tocIndex:3},{value:"62.12",paraId:9,tocIndex:3},{value:"80.55",paraId:9,tocIndex:3},{value:"82.57",paraId:9,tocIndex:3},{value:"85.53",paraId:9,tocIndex:3},{value:"85.85",paraId:9,tocIndex:3},{value:"84.71",paraId:9,tocIndex:3},{value:"71.98",paraId:9,tocIndex:3},{value:"80.09",paraId:9,tocIndex:3},{value:"79.63",paraId:9,tocIndex:3},{value:"Qwen-14B-Chat",paraId:9,tocIndex:3},{value:"65.15",paraId:9,tocIndex:3},{value:"76",paraId:9,tocIndex:3},{value:"82.57",paraId:9,tocIndex:3},{value:"85.53",paraId:9,tocIndex:3},{value:"84.91",paraId:9,tocIndex:3},{value:"84.31",paraId:9,tocIndex:3},{value:"70.85",paraId:9,tocIndex:3},{value:"81.48",paraId:9,tocIndex:3},{value:"77.81",paraId:9,tocIndex:3},{value:"Qwen-14B-Base",paraId:9,tocIndex:3},{value:"66.67",paraId:9,tocIndex:3},{value:"76.15",paraId:9,tocIndex:3},{value:"84.4",paraId:9,tocIndex:3},{value:"85.53",paraId:9,tocIndex:3},{value:"86.32",paraId:9,tocIndex:3},{value:"80.39",paraId:9,tocIndex:3},{value:"72.46",paraId:9,tocIndex:3},{value:"80.56",paraId:9,tocIndex:3},{value:"77.56",paraId:9,tocIndex:3},{value:"Baichuan2-13B-Base",paraId:9,tocIndex:3},{value:"63.64",paraId:9,tocIndex:3},{value:"71.39",paraId:9,tocIndex:3},{value:"80.73",paraId:9,tocIndex:3},{value:"82.46",paraId:9,tocIndex:3},{value:"81.13",paraId:9,tocIndex:3},{value:"84.31",paraId:9,tocIndex:3},{value:"73.75",paraId:9,tocIndex:3},{value:"85.19",paraId:9,tocIndex:3},{value:"75.8",paraId:9,tocIndex:3},{value:"Qwen-7B-Base",paraId:9,tocIndex:3},{value:"75.76",paraId:9,tocIndex:3},{value:"72.52",paraId:9,tocIndex:3},{value:"78.9",paraId:9,tocIndex:3},{value:"81.14",paraId:9,tocIndex:3},{value:"83.96",paraId:9,tocIndex:3},{value:"81.18",paraId:9,tocIndex:3},{value:"70.37",paraId:9,tocIndex:3},{value:"81.94",paraId:9,tocIndex:3},{value:"75.36",paraId:9,tocIndex:3},{value:"Baichuan2-13B-Chat",paraId:9,tocIndex:3},{value:"62.12",paraId:9,tocIndex:3},{value:"69.95",paraId:9,tocIndex:3},{value:"76.61",paraId:9,tocIndex:3},{value:"84.21",paraId:9,tocIndex:3},{value:"83.49",paraId:9,tocIndex:3},{value:"79.61",paraId:9,tocIndex:3},{value:"71.98",paraId:9,tocIndex:3},{value:"80.56",paraId:9,tocIndex:3},{value:"74.12",paraId:9,tocIndex:3},{value:"DevOpsPal-7B-Chat",paraId:9,tocIndex:3},{value:"66.67",paraId:9,tocIndex:3},{value:"69.95",paraId:9,tocIndex:3},{value:"83.94",paraId:9,tocIndex:3},{value:"81.14",paraId:9,tocIndex:3},{value:"80.19",paraId:9,tocIndex:3},{value:"82.75",paraId:9,tocIndex:3},{value:"68.6",paraId:9,tocIndex:3},{value:"76.85",paraId:9,tocIndex:3},{value:"73.61",paraId:9,tocIndex:3},{value:"DevOpsPal-7B-Base",paraId:9,tocIndex:3},{value:"69.7",paraId:9,tocIndex:3},{value:"69.49",paraId:9,tocIndex:3},{value:"82.11",paraId:9,tocIndex:3},{value:"81.14",paraId:9,tocIndex:3},{value:"82.55",paraId:9,tocIndex:3},{value:"82.35",paraId:9,tocIndex:3},{value:"67.15",paraId:9,tocIndex:3},{value:"79.17",paraId:9,tocIndex:3},{value:"73.35",paraId:9,tocIndex:3},{value:"Qwen-7B-Chat",paraId:9,tocIndex:3},{value:"65.15",paraId:9,tocIndex:3},{value:"66.54",paraId:9,tocIndex:3},{value:"82.57",paraId:9,tocIndex:3},{value:"81.58",paraId:9,tocIndex:3},{value:"81.6",paraId:9,tocIndex:3},{value:"81.18",paraId:9,tocIndex:3},{value:"65.38",paraId:9,tocIndex:3},{value:"81.02",paraId:9,tocIndex:3},{value:"71.69",paraId:9,tocIndex:3},{value:"Baichuan2-7B-Base",paraId:9,tocIndex:3},{value:"60.61",paraId:9,tocIndex:3},{value:"67.22",paraId:9,tocIndex:3},{value:"76.61",paraId:9,tocIndex:3},{value:"75",paraId:9,tocIndex:3},{value:"77.83",paraId:9,tocIndex:3},{value:"78.43",paraId:9,tocIndex:3},{value:"67.31",paraId:9,tocIndex:3},{value:"79.63",paraId:9,tocIndex:3},{value:"70.8",paraId:9,tocIndex:3},{value:"Internlm-7B-Chat",paraId:9,tocIndex:3},{value:"60.61",paraId:9,tocIndex:3},{value:"63.06",paraId:9,tocIndex:3},{value:"79.82",paraId:9,tocIndex:3},{value:"80.26",paraId:9,tocIndex:3},{value:"67.92",paraId:9,tocIndex:3},{value:"75.69",paraId:9,tocIndex:3},{value:"60.06",paraId:9,tocIndex:3},{value:"77.31",paraId:9,tocIndex:3},{value:"69.21",paraId:9,tocIndex:3},{value:"Baichuan2-7B-Chat",paraId:9,tocIndex:3},{value:"60.61",paraId:9,tocIndex:3},{value:"64.95",paraId:9,tocIndex:3},{value:"81.19",paraId:9,tocIndex:3},{value:"75.88",paraId:9,tocIndex:3},{value:"71.23",paraId:9,tocIndex:3},{value:"75.69",paraId:9,tocIndex:3},{value:"64.9",paraId:9,tocIndex:3},{value:"79.17",paraId:9,tocIndex:3},{value:"69.05",paraId:9,tocIndex:3},{value:"Internlm-7B-Base",paraId:9,tocIndex:3},{value:"62.12",paraId:9,tocIndex:3},{value:"65.25",paraId:9,tocIndex:3},{value:"77.52",paraId:9,tocIndex:3},{value:"80.7",paraId:9,tocIndex:3},{value:"74.06",paraId:9,tocIndex:3},{value:"78.82",paraId:9,tocIndex:3},{value:"63.45",paraId:9,tocIndex:3},{value:"75.46",paraId:9,tocIndex:3},{value:"67.17",paraId:9,tocIndex:3},{value:"ModelName",paraId:10,tocIndex:5},{value:"LogParsing",paraId:10,tocIndex:5},{value:"RootCauseAnalysis",paraId:10,tocIndex:5},{value:"TimeSeriesAnomalyDetection",paraId:10,tocIndex:5},{value:"TimeSeriesClassification",paraId:10,tocIndex:5},{value:"TimeSeriesForecasting",paraId:10,tocIndex:5},{value:"AVG",paraId:10,tocIndex:5},{value:"Qwen-14B-Base",paraId:10,tocIndex:5},{value:"66.29",paraId:10,tocIndex:5},{value:"58.8",paraId:10,tocIndex:5},{value:"25.33",paraId:10,tocIndex:5},{value:"43.5",paraId:10,tocIndex:5},{value:"62.5",paraId:10,tocIndex:5},{value:"52.25",paraId:10,tocIndex:5},{value:"DevOpsPal-14B\u2014Base",paraId:10,tocIndex:5},{value:"63.14",paraId:10,tocIndex:5},{value:"53.6",paraId:10,tocIndex:5},{value:"23.33",paraId:10,tocIndex:5},{value:"43.5",paraId:10,tocIndex:5},{value:"64.06",paraId:10,tocIndex:5},{value:"50.49",paraId:10,tocIndex:5},{value:"Qwen-14B-Chat",paraId:10,tocIndex:5},{value:"64.57",paraId:10,tocIndex:5},{value:"51.6",paraId:10,tocIndex:5},{value:"22.67",paraId:10,tocIndex:5},{value:"36",paraId:10,tocIndex:5},{value:"62.5",paraId:10,tocIndex:5},{value:"48.94",paraId:10,tocIndex:5},{value:"DevOpsPal-14B\u2014Chat",paraId:10,tocIndex:5},{value:"60",paraId:10,tocIndex:5},{value:"56",paraId:10,tocIndex:5},{value:"24",paraId:10,tocIndex:5},{value:"43",paraId:10,tocIndex:5},{value:"57.81",paraId:10,tocIndex:5},{value:"48.8",paraId:10,tocIndex:5},{value:"Qwen-7B-Base",paraId:10,tocIndex:5},{value:"50",paraId:10,tocIndex:5},{value:"39.2",paraId:10,tocIndex:5},{value:"22.67",paraId:10,tocIndex:5},{value:"54",paraId:10,tocIndex:5},{value:"43.75",paraId:10,tocIndex:5},{value:"41.48",paraId:10,tocIndex:5},{value:"DevOpsPal-7B\u2014Chat",paraId:10,tocIndex:5},{value:"56.57",paraId:10,tocIndex:5},{value:"30.4",paraId:10,tocIndex:5},{value:"25.33",paraId:10,tocIndex:5},{value:"45",paraId:10,tocIndex:5},{value:"44.06",paraId:10,tocIndex:5},{value:"40.92",paraId:10,tocIndex:5},{value:"Baichuan2-13B-Chat",paraId:10,tocIndex:5},{value:"64",paraId:10,tocIndex:5},{value:"18",paraId:10,tocIndex:5},{value:"21.33",paraId:10,tocIndex:5},{value:"37.5",paraId:10,tocIndex:5},{value:"46.88",paraId:10,tocIndex:5},{value:"39.3",paraId:10,tocIndex:5},{value:"Qwen-7B-Chat",paraId:10,tocIndex:5},{value:"57.43",paraId:10,tocIndex:5},{value:"38.8",paraId:10,tocIndex:5},{value:"22.33",paraId:10,tocIndex:5},{value:"39.5",paraId:10,tocIndex:5},{value:"25.31",paraId:10,tocIndex:5},{value:"36.97",paraId:10,tocIndex:5},{value:"Internlm-7B\u2014Chat",paraId:10,tocIndex:5},{value:"58.86",paraId:10,tocIndex:5},{value:"8.8",paraId:10,tocIndex:5},{value:"22.33",paraId:10,tocIndex:5},{value:"28.5",paraId:10,tocIndex:5},{value:"51.25",paraId:10,tocIndex:5},{value:"36.34",paraId:10,tocIndex:5},{value:"Baichuan2-7B-Chat",paraId:10,tocIndex:5},{value:"60.86",paraId:10,tocIndex:5},{value:"10",paraId:10,tocIndex:5},{value:"28",paraId:10,tocIndex:5},{value:"34.5",paraId:10,tocIndex:5},{value:"39.06",paraId:10,tocIndex:5},{value:"36.34",paraId:10,tocIndex:5},{value:"Baichuan2-7B-Base",paraId:10,tocIndex:5},{value:"53.43",paraId:10,tocIndex:5},{value:"12.8",paraId:10,tocIndex:5},{value:"27.67",paraId:10,tocIndex:5},{value:"36.5",paraId:10,tocIndex:5},{value:"40.31",paraId:10,tocIndex:5},{value:"35.49",paraId:10,tocIndex:5},{value:"Baichuan2-13B-Base",paraId:10,tocIndex:5},{value:"54",paraId:10,tocIndex:5},{value:"12.4",paraId:10,tocIndex:5},{value:"23",paraId:10,tocIndex:5},{value:"34.5",paraId:10,tocIndex:5},{value:"42.81",paraId:10,tocIndex:5},{value:"34.86",paraId:10,tocIndex:5},{value:"DevOpsPal-7B\u2014Base",paraId:10,tocIndex:5},{value:"46.57",paraId:10,tocIndex:5},{value:"20.8",paraId:10,tocIndex:5},{value:"25",paraId:10,tocIndex:5},{value:"34",paraId:10,tocIndex:5},{value:"38.75",paraId:10,tocIndex:5},{value:"33.94",paraId:10,tocIndex:5},{value:"Internlm-7B\u2014Base",paraId:10,tocIndex:5},{value:"48.57",paraId:10,tocIndex:5},{value:"18.8",paraId:10,tocIndex:5},{value:"23.33",paraId:10,tocIndex:5},{value:"37.5",paraId:10,tocIndex:5},{value:"33.75",paraId:10,tocIndex:5},{value:"33.1",paraId:10,tocIndex:5},{value:"ModelName",paraId:11,tocIndex:6},{value:"LogParsing",paraId:11,tocIndex:6},{value:"RootCauseAnalysis",paraId:11,tocIndex:6},{value:"TimeSeriesAnomalyDetection",paraId:11,tocIndex:6},{value:"TimeSeriesClassification",paraId:11,tocIndex:6},{value:"TimeSeriesForecasting",paraId:11,tocIndex:6},{value:"AVG",paraId:11,tocIndex:6},{value:"DevOpsPal-14B\u2014Chat",paraId:11,tocIndex:6},{value:"66.29",paraId:11,tocIndex:6},{value:"80.8",paraId:11,tocIndex:6},{value:"23.33",paraId:11,tocIndex:6},{value:"44.5",paraId:11,tocIndex:6},{value:"56.25",paraId:11,tocIndex:6},{value:"54.44",paraId:11,tocIndex:6},{value:"DevOpsPal-14B\u2014Base",paraId:11,tocIndex:6},{value:"60",paraId:11,tocIndex:6},{value:"74",paraId:11,tocIndex:6},{value:"25.33",paraId:11,tocIndex:6},{value:"43.5",paraId:11,tocIndex:6},{value:"52.5",paraId:11,tocIndex:6},{value:"51.13",paraId:11,tocIndex:6},{value:"Qwen-14B-Base",paraId:11,tocIndex:6},{value:"64.29",paraId:11,tocIndex:6},{value:"74.4",paraId:11,tocIndex:6},{value:"28",paraId:11,tocIndex:6},{value:"48.5",paraId:11,tocIndex:6},{value:"40.31",paraId:11,tocIndex:6},{value:"50.77",paraId:11,tocIndex:6},{value:"Qwen-7B-Base",paraId:11,tocIndex:6},{value:"56",paraId:11,tocIndex:6},{value:"60.8",paraId:11,tocIndex:6},{value:"27.67",paraId:11,tocIndex:6},{value:"44",paraId:11,tocIndex:6},{value:"57.19",paraId:11,tocIndex:6},{value:"49.44",paraId:11,tocIndex:6},{value:"Qwen-14B-Chat",paraId:11,tocIndex:6},{value:"49.71",paraId:11,tocIndex:6},{value:"65.6",paraId:11,tocIndex:6},{value:"28.67",paraId:11,tocIndex:6},{value:"48",paraId:11,tocIndex:6},{value:"42.19",paraId:11,tocIndex:6},{value:"46.13",paraId:11,tocIndex:6},{value:"Baichuan2-13B-Base",paraId:11,tocIndex:6},{value:"56",paraId:11,tocIndex:6},{value:"43.2",paraId:11,tocIndex:6},{value:"24.33",paraId:11,tocIndex:6},{value:"41",paraId:11,tocIndex:6},{value:"46.88",paraId:11,tocIndex:6},{value:"42.89",paraId:11,tocIndex:6},{value:"Baichuan2-7B-Chat",paraId:11,tocIndex:6},{value:"58.57",paraId:11,tocIndex:6},{value:"31.6",paraId:11,tocIndex:6},{value:"27",paraId:11,tocIndex:6},{value:"31.5",paraId:11,tocIndex:6},{value:"51.88",paraId:11,tocIndex:6},{value:"41.83",paraId:11,tocIndex:6},{value:"DevOpsPal-7B\u2014Base",paraId:11,tocIndex:6},{value:"52.86",paraId:11,tocIndex:6},{value:"44.4",paraId:11,tocIndex:6},{value:"28",paraId:11,tocIndex:6},{value:"44.5",paraId:11,tocIndex:6},{value:"36.25",paraId:11,tocIndex:6},{value:"41.2",paraId:11,tocIndex:6},{value:"Baichuan2-7B-Base",paraId:11,tocIndex:6},{value:"48.29",paraId:11,tocIndex:6},{value:"40.4",paraId:11,tocIndex:6},{value:"27",paraId:11,tocIndex:6},{value:"42",paraId:11,tocIndex:6},{value:"40.94",paraId:11,tocIndex:6},{value:"39.86",paraId:11,tocIndex:6},{value:"Qwen-7B-Chat",paraId:11,tocIndex:6},{value:"54.57",paraId:11,tocIndex:6},{value:"52",paraId:11,tocIndex:6},{value:"29.67",paraId:11,tocIndex:6},{value:"26.5",paraId:11,tocIndex:6},{value:"27.19",paraId:11,tocIndex:6},{value:"38.73",paraId:11,tocIndex:6},{value:"Baichuan2-13B-Chat",paraId:11,tocIndex:6},{value:"57.43",paraId:11,tocIndex:6},{value:"44.4",paraId:11,tocIndex:6},{value:"25",paraId:11,tocIndex:6},{value:"25.5",paraId:11,tocIndex:6},{value:"30.63",paraId:11,tocIndex:6},{value:"37.75",paraId:11,tocIndex:6},{value:"DevOpsPal-7B\u2014Chat",paraId:11,tocIndex:6},{value:"56.57",paraId:11,tocIndex:6},{value:"27.2",paraId:11,tocIndex:6},{value:"25.33",paraId:11,tocIndex:6},{value:"41.5",paraId:11,tocIndex:6},{value:"33.44",paraId:11,tocIndex:6},{value:"37.46",paraId:11,tocIndex:6},{value:"Internlm-7B\u2014Chat",paraId:11,tocIndex:6},{value:"62.57",paraId:11,tocIndex:6},{value:"12.8",paraId:11,tocIndex:6},{value:"22.33",paraId:11,tocIndex:6},{value:"21",paraId:11,tocIndex:6},{value:"50.31",paraId:11,tocIndex:6},{value:"36.69",paraId:11,tocIndex:6},{value:"Internlm-7B\u2014Base",paraId:11,tocIndex:6},{value:"48",paraId:11,tocIndex:6},{value:"33.2",paraId:11,tocIndex:6},{value:"29",paraId:11,tocIndex:6},{value:"35",paraId:11,tocIndex:6},{value:"31.56",paraId:11,tocIndex:6},{value:"35.85",paraId:11,tocIndex:6},{value:"FuncCall-Filler",paraId:12,tocIndex:7},{value:"dataset_name",paraId:12,tocIndex:7},{value:"fccr",paraId:12,tocIndex:7},{value:"1-fcffr",paraId:12,tocIndex:7},{value:"1-fcfnr",paraId:12,tocIndex:7},{value:"1-fcfpr",paraId:12,tocIndex:7},{value:"1-fcfnir",paraId:12,tocIndex:7},{value:"aar",paraId:12,tocIndex:7},{value:"Qwen-14b-chat",paraId:12,tocIndex:7},{value:"luban",paraId:12,tocIndex:7},{value:"61",paraId:12,tocIndex:7},{value:"100",paraId:12,tocIndex:7},{value:"97.68",paraId:12,tocIndex:7},{value:"63.32",paraId:12,tocIndex:7},{value:"100",paraId:12,tocIndex:7},{value:"69.46",paraId:12,tocIndex:7},{value:"Qwen-7b-chat",paraId:12,tocIndex:7},{value:"luban",paraId:12,tocIndex:7},{value:"50.58",paraId:12,tocIndex:7},{value:"100",paraId:12,tocIndex:7},{value:"98.07",paraId:12,tocIndex:7},{value:"52.51",paraId:12,tocIndex:7},{value:"100",paraId:12,tocIndex:7},{value:"63.59",paraId:12,tocIndex:7},{value:"Baichuan-7b-chat",paraId:12,tocIndex:7},{value:"luban",paraId:12,tocIndex:7},{value:"60.23",paraId:12,tocIndex:7},{value:"100",paraId:12,tocIndex:7},{value:"97.3",paraId:12,tocIndex:7},{value:"62.93",paraId:12,tocIndex:7},{value:"99.61",paraId:12,tocIndex:7},{value:"61.12",paraId:12,tocIndex:7},{value:"Internlm-chat-7b",paraId:12,tocIndex:7},{value:"luban",paraId:12,tocIndex:7},{value:"47.88",paraId:12,tocIndex:7},{value:"100",paraId:12,tocIndex:7},{value:"96.14",paraId:12,tocIndex:7},{value:"51.74",paraId:12,tocIndex:7},{value:"99.61",paraId:12,tocIndex:7},{value:"61.85",paraId:12,tocIndex:7},{value:"Qwen-14b-chat",paraId:12,tocIndex:7},{value:"fc_data",paraId:12,tocIndex:7},{value:"98.37",paraId:12,tocIndex:7},{value:"99.73",paraId:12,tocIndex:7},{value:"99.86",paraId:12,tocIndex:7},{value:"98.78",paraId:12,tocIndex:7},{value:"100",paraId:12,tocIndex:7},{value:"81.58",paraId:12,tocIndex:7},{value:"Qwen-7b-chat",paraId:12,tocIndex:7},{value:"fc_data",paraId:12,tocIndex:7},{value:"99.46",paraId:12,tocIndex:7},{value:"99.86",paraId:12,tocIndex:7},{value:"100",paraId:12,tocIndex:7},{value:"99.59",paraId:12,tocIndex:7},{value:"100",paraId:12,tocIndex:7},{value:"79.25",paraId:12,tocIndex:7},{value:"Baichuan-7b-chat",paraId:12,tocIndex:7},{value:"fc_data",paraId:12,tocIndex:7},{value:"97.96",paraId:12,tocIndex:7},{value:"99.32",paraId:12,tocIndex:7},{value:"100",paraId:12,tocIndex:7},{value:"98.64",paraId:12,tocIndex:7},{value:"100",paraId:12,tocIndex:7},{value:"89.53",paraId:12,tocIndex:7},{value:"Internlm-chat-7b",paraId:12,tocIndex:7},{value:"fc_data",paraId:12,tocIndex:7},{value:"94.29",paraId:12,tocIndex:7},{value:"95.78",paraId:12,tocIndex:7},{value:"100",paraId:12,tocIndex:7},{value:"98.5",paraId:12,tocIndex:7},{value:"100",paraId:12,tocIndex:7},{value:"88.19",paraId:12,tocIndex:7},{value:"CodeLLaMa-7b",paraId:12,tocIndex:7},{value:"fc_data",paraId:12,tocIndex:7},{value:"98.78",paraId:12,tocIndex:7},{value:"99.73",paraId:12,tocIndex:7},{value:"100",paraId:12,tocIndex:7},{value:"99.05",paraId:12,tocIndex:7},{value:"100",paraId:12,tocIndex:7},{value:"94.7",paraId:12,tocIndex:7},{value:"CodeLLaMa-7b-16",paraId:12,tocIndex:7},{value:"fc_data",paraId:12,tocIndex:7},{value:"98.1",paraId:12,tocIndex:7},{value:"99.87",paraId:12,tocIndex:7},{value:"99.73",paraId:12,tocIndex:7},{value:"98.5",paraId:12,tocIndex:7},{value:"100",paraId:12,tocIndex:7},{value:"93.14",paraId:12,tocIndex:7},{value:"CodeFuse-7b-4k",paraId:12,tocIndex:7},{value:"fc_data",paraId:12,tocIndex:7},{value:"98.91",paraId:12,tocIndex:7},{value:"99.87",paraId:12,tocIndex:7},{value:"99.87",paraId:12,tocIndex:7},{value:"99.18",paraId:12,tocIndex:7},{value:"100",paraId:12,tocIndex:7},{value:"89.5",paraId:12,tocIndex:7}]},76067:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"Method 1: Download the zip file (you can also simply open the following link with the browser):",paraId:0,tocIndex:1},{value:`wget https://huggingface.co/datasets/codefuse-admin/devopseval-exam/resolve/main/devopseval-exam.zip
`,paraId:1,tocIndex:1},{value:"then unzip it and you may load the data with pandas:",paraId:2,tocIndex:1},{value:`import os
import pandas as pd

File_Dir="devopseval-exam"
test_df=pd.read_csv(os.path.join(File_Dir,"test","UnitTesting.csv"))
`,paraId:3,tocIndex:1},{value:"Method 2: Directly load the dataset using ",paraId:4,tocIndex:1},{value:"Hugging Face datasets",paraId:4,tocIndex:1},{value:":",paraId:4,tocIndex:1},{value:`from datasets import load_dataset
dataset=load_dataset(r"DevOps-Eval/devopseval-exam",name="UnitTesting")

print(dataset['val'][0])
# {"id": 1, "question": "\u5355\u5143\u6D4B\u8BD5\u5E94\u8BE5\u8986\u76D6\u4EE5\u4E0B\u54EA\u4E9B\u65B9\u9762\uFF1F", "A": "\u6B63\u5E38\u8DEF\u5F84", "B": "\u5F02\u5E38\u8DEF\u5F84", "C": "\u8FB9\u754C\u503C\u6761\u4EF6"\uFF0C"D": \u6240\u6709\u4EE5\u4E0A\uFF0C"answer": "D", "explanation": ""}  \`\`\`

`,paraId:5,tocIndex:1},{value:"Method 3: Use ",paraId:6,tocIndex:1},{value:"modelscope",paraId:6,tocIndex:1},{value:" download all datas\u3002Examples\uFF1A",paraId:6,tocIndex:1},{value:"from modelscope.msdatasets import MsDataset\nMsDataset.clone_meta(dataset_work_dir='./xxx', dataset_id='codefuse-ai/devopseval-exam')```\n",paraId:7,tocIndex:1},{value:"To facilitate usage, we have organized the category name handlers and English/Chinese names corresponding to 55 subcategories. Please refer to ",paraId:8,tocIndex:2},{value:"category_mapping.json",paraId:9,tocIndex:2},{value:" for details. The format is:",paraId:8,tocIndex:2},{value:`{
  "UnitTesting.csv": [
    "unit testing",
    "\u5355\u5143\u6D4B\u8BD5",
    {"dev": 5, "test": 32}
    "TEST"
  ],
  ...
  "file_name":[
  "English Name",
  "Chinese Name",
  "Sample Number",
  "Supercatagory Label(PLAN,CODE,BUILD,TEST,RELEASE,DEPOLY,OPERATE,MONITOR choose 1 out of 8)"
  ]
}
`,paraId:10,tocIndex:2},{value:"Each subcategory consists of two splits: dev and test. The dev set per subcategory consists of five exemplars with explanations for few-shot evaluation. And the test set is for model evaluation. Labels on the test split are also released.",paraId:11,tocIndex:2},{value:"Below is a dev example from 'version control':",paraId:12,tocIndex:2},{value:`id: 4
question: \u5982\u4F55\u627E\u5230Git\u7279\u5B9A\u63D0\u4EA4\u4E2D\u5DF2\u66F4\u6539\u7684\u6587\u4EF6\u5217\u8868\uFF1F
A: \u4F7F\u7528\u547D\u4EE4 \`git diff --name-only SHA\`
B: \u4F7F\u7528\u547D\u4EE4 \`git log --name-only SHA\`
C: \u4F7F\u7528\u547D\u4EE4 \`git commit --name-only SHA\`
D: \u4F7F\u7528\u547D\u4EE4 \`git clone --name-only SHA\`
answer: A
explanation:
\u5206\u6790\u539F\u56E0\uFF1A
git diff --name-only SHA\u547D\u4EE4\u4F1A\u663E\u793A\u4E0ESHA\u53C2\u6570\u5BF9\u5E94\u7684\u63D0\u4EA4\u4E2D\u5DF2\u4FEE\u6539\u7684\u6587\u4EF6\u5217\u8868\u3002\u53C2\u6570--name-only\u8BA9\u547D\u4EE4\u53EA\u8F93\u51FA\u6587\u4EF6\u540D\uFF0C\u800C\u5FFD\u7565\u5176\u4ED6\u4FE1\u606F\u3002\u5176\u5B83\u9009\u9879\u4E2D\u7684\u547D\u4EE4\u5E76\u4E0D\u80FD\u5B9E\u73B0\u6B64\u529F\u80FD\u3002
`,paraId:13,tocIndex:2},{value:"\u{1F440} \u{1F440} Taking ",paraId:14,tocIndex:3},{value:"log parsing",paraId:14,tocIndex:3},{value:" and ",paraId:14,tocIndex:3},{value:"time series anomaly detection",paraId:14,tocIndex:3},{value:" as examples, here is a brief showcase of the AIOps samples:",paraId:14,tocIndex:3},{value:"LogParsing",paraId:15,tocIndex:3},{value:`id: 0
question:
Here are some running logs
 0 04:21:15,429 WARN Cannot open channel to 2 at election address /10.10.34.12:3888
 1 19:18:56,377 WARN ******* GOODBYE /10.10.34.11:52703 ********
 2 19:13:46,128 WARN ******* GOODBYE /10.10.34.11:52308 ********
 3 19:16:26,268 WARN ******* GOODBYE /10.10.34.11:52502 ********
 4 09:11:16,012 WARN Cannot open channel to 3 at election address /10.10.34.13:3888
 5 16:37:13,837 WARN Cannot open channel to 2 at election address /10.10.34.12:3888
 6 09:09:16,008 WARN Cannot open channel to 3 at election address /10.10.34.13:3888
 7 15:27:03,681 WARN Cannot open channel to 3 at election address /10.10.34.13:3888
The first three parts of the log are index, timestamp, and log level. Without considering these three parts, Here we assume that the variables in the logs are represented as '<*>', separated by spaces between tokens. What is the specific log template for the above logs?
A: Notification time out: <*> \u548C Connection broken for id <*>, my id = <*>, error =
B: Send worker leaving thread \u548C Connection broken for id <*>, my id = <*>, error =
C: Received connection request /<*>:<*> \u548C Interrupting SendWorker
D: Cannot open channel to <*> at election address /<*>:<*> \u548C ******* GOODBYE /<*>:<*> ********
answer: D
explanation: The log includes the fixed template fragments "Cannot open channel to <> at election address /<>:<>" and "****** GOODBYE /<>:<> ********," both of which appear in option D. Meanwhile, the template fragments in the other options do not match the content in the log. Therefore, option D is the most consistent with the log template.
`,paraId:16,tocIndex:3},{value:"TimeSeriesAnomalyDetection",paraId:17,tocIndex:3},{value:`id: 0
question:
Analyze the following time series
[50,62,74,84,92,97,99,98,94,87,77,65,265,40,28,17,8,3,0,0,4,10,20,31,43,56,68,79,89,95,99,99,96,91,82,71,59,46,34,22,12,5,1,0,2,7,15,25,37,49]
Please identify the indices of obvious outlier points. Outlier points generally refer to points that significantly deviate from the overall trend of the data.
A: 46
B: 0
C: 37
D: 12
answer: D
explanation: According to the analysis, the value 265 in the given time series at 12 o'clock is significantly larger than the surrounding data, indicating a sudden increase phenomenon. Therefore, selecting option D is correct.
`,paraId:18,tocIndex:3},{value:"\u{1F440} \u{1F440}The data format of ToolLearning samples is compatible with OpenAI's Function Calling.",paraId:19,tocIndex:4},{value:"Please refer to ",paraId:20,tocIndex:4},{value:"tool_learning_info.md",paraId:21,tocIndex:4},{value:` for details.
Tool Learning Data Evalution see `,paraId:20,tocIndex:4},{value:"tool_learning_evalution.md",paraId:22,tocIndex:4},{value:`\u3002
`,paraId:20,tocIndex:4}]},62229:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"If you need to test your own huggingface-formatted model, the overall steps are as follows:",paraId:0,tocIndex:0},{value:"Write the loader function for the model.",paraId:1,tocIndex:0},{value:"Write the context_builder function for the model.",paraId:1,tocIndex:0},{value:"Register the model in the configuration file.",paraId:1,tocIndex:0},{value:`Run the testing script.
If the model does not require any special processing after loading, and the input does not need to be converted to a specific format (e.g. chatml format or other human-bot formats), you can directly proceed to step 4 to initiate the testing.`,paraId:1,tocIndex:0},{value:"If the model requires additional processing after loading (e.g. adjusting the tokenizer), you need to inherit the ",paraId:2,tocIndex:1},{value:"ModelAndTokenizerLoader",paraId:2,tocIndex:1},{value:" class in ",paraId:2,tocIndex:1},{value:"src.context_builder.context_builder_family.py",paraId:2,tocIndex:1},{value:" and override the corresponding ",paraId:2,tocIndex:1},{value:"load_model",paraId:2,tocIndex:1},{value:" and ",paraId:2,tocIndex:1},{value:"load_tokenizer",paraId:2,tocIndex:1},{value:" functions. You can refer to the following example:",paraId:2,tocIndex:1},{value:`class QwenModelAndTokenizerLoader(ModelAndTokenizerLoader):
    def __init__(self):
        super().__init__()
        pass

    @override
    def load_model(self, model_path: str):
    # Implementation of the method
        pass

    @override
    def load_tokenizer(self, model_path: str):
    # Implementation of the method
        pass
`,paraId:3,tocIndex:1},{value:"If the input needs to be converted to a specific format (e.g. chatml format or other human-bot formats), you need to inherit the ContextBuilder class in ",paraId:4,tocIndex:2},{value:"src.context_builder.context_builder_family",paraId:4,tocIndex:2},{value:" and override the make_context function. This function is used to convert the input to the corresponding required format. An example is shown below:",paraId:4,tocIndex:2},{value:`class QwenChatContextBuilder(ContextBuilder):
    def __init__(self):
        super().__init__()

    @override
    def make_context(self, model, tokenizer, query: str, system: str = "hello\uFF01"):
    # Implementation of the method
        pass
`,paraId:5,tocIndex:2},{value:"Go to the ",paraId:6,tocIndex:3},{value:"model_conf.json",paraId:6,tocIndex:3},{value:" file in the conf directory and register the corresponding model name and the loader and context_builder that will be used for this model. Simply write the class names defined in the first and second steps for the loader and context_builder. Here is an example:",paraId:6,tocIndex:3},{value:`{
  "Qwen-Chat": {
    "loader": "QwenModelAndTokenizerLoader",
    "context_builder": "QwenChatContextBuilder"
  }
}
`,paraId:7,tocIndex:3},{value:"Run the following code to initiate the test:",paraId:8,tocIndex:4},{value:`python src/run_eval.py \\
--model_path path_to_model \\
--model_name model_name_in_conf \\
--model_conf_path path_to_model_conf \\
--eval_dataset_list all \\
--eval_dataset_fp_conf_path path_to_dataset_conf \\
--eval_dataset_type test \\
--data_path path_to_downloaded_devops_eval_data \\
--k_shot 0
`,paraId:9,tocIndex:4},{value:"\u{1F440} \u{1F440} The specific evaluation process is as follows \u{1F4D6} ",paraId:10,tocIndex:4},{value:"Evaluate Tutorial",paraId:11,tocIndex:4}]},63334:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"To test on your own Huggingface-format model, the overall steps are divided into the following:",paraId:0,tocIndex:1},{value:"Write the ",paraId:1,tocIndex:1},{value:"create_prompts",paraId:1,tocIndex:1},{value:" function in ",paraId:1,tocIndex:1},{value:"~/evals/FuncCallEvaluation",paraId:1,tocIndex:1},{value:"Write the related functions in ",paraId:1,tocIndex:1},{value:"~/models/base_model",paraId:1,tocIndex:1},{value:"Register the model and evaluation functions",paraId:1,tocIndex:1},{value:`Execute the test script
If the model does not require special handling after loading, and the input does not need to be converted into a specific format (e.g. ChatML format or other human-bot formats), please skip directly to step four and start testing.`,paraId:1,tocIndex:1},{value:"If the model needs additional processing after loading (e.g., tokenizer adjustments), you will need to inherit the ",paraId:2,tocIndex:2},{value:"ModelAndTokenizerLoader",paraId:2,tocIndex:2},{value:" class in ",paraId:2,tocIndex:2},{value:"src.context_builder.context_builder_family.py",paraId:2,tocIndex:2},{value:" and override the corresponding ",paraId:2,tocIndex:2},{value:"load_model",paraId:2,tocIndex:2},{value:" and ",paraId:2,tocIndex:2},{value:"load_tokenizer",paraId:2,tocIndex:2},{value:" functions.",paraId:2,tocIndex:2},{value:"An example is as follows:",paraId:3,tocIndex:2},{value:`class FuncCallEvalution(ToolEvalution):

    def create_prompts(self, func_call_datas):
        '''
        datas: [
            {
                "instruction": history[his_idx],
                "input": "",
                "output": output,
                "history": [(human_content, ai_content), (), ()],
                "functions": tools
            }
        ]
        '''
        system_content = '''CodeFuse\u662F\u4E00\u4E2A\u9762\u5411\u7814\u53D1\u9886\u57DF\u7684\u667A\u80FD\u52A9\u624B\uFF0C\u65E8\u5728\u4E2D\u7ACB\u7684\u3001\u65E0\u5BB3\u7684\u5E2E\u52A9\u7528\u6237\u89E3\u51B3\u5F00\u53D1\u76F8\u5173\u7684\u95EE\u9898\uFF0C\u6240\u6709\u7684\u56DE\u7B54\u5747\u4F7F\u7528Markdown\u683C\u5F0F\u8FD4\u56DE\u3002
        \u4F60\u80FD\u5229\u7528\u8BB8\u591A\u5DE5\u5177\u548C\u529F\u80FD\u6765\u5B8C\u6210\u7ED9\u5B9A\u7684\u4EFB\u52A1\uFF0C\u5728\u6BCF\u4E00\u6B65\u4E2D\uFF0C\u4F60\u9700\u8981\u5206\u6790\u5F53\u524D\u72B6\u6001\uFF0C\u5E76\u901A\u8FC7\u6267\u884C\u51FD\u6570\u8C03\u7528\u6765\u786E\u5B9A\u4E0B\u4E00\u6B65\u7684\u884C\u52A8\u65B9\u5411\u3002\u4F60\u53EF\u4EE5\u8FDB\u884C\u591A\u6B21\u5C1D\u8BD5\u3002\u5982\u679C\u4F60\u8BA1\u5212\u8FDE\u7EED\u5C1D\u8BD5\u4E0D\u540C\u7684\u6761\u4EF6\uFF0C\u8BF7\u6BCF\u6B21\u5C1D\u8BD5\u4E00\u79CD\u6761\u4EF6\u3002\u82E5\u7ED9\u5B9A\u4E86Finish\u51FD\u6570,\u5219\u4EE5Finish\u8C03\u7528\u7ED3\u675F\uFF0C\u82E5\u6CA1\u63D0\u4F9BFinish\u51FD\u6570\uFF0C\u5219\u4EE5\u4E0D\u5E26function_call\u7684\u5BF9\u8BDD\u7ED3\u675F\u3002'''
        function_format = '''You are ToolGPT, you have access to the following APIs:\\n{tools}'''

        func_call_train_datas = []
        history_error_cnt = 0
        funccall_error_cnt = 0

        for data in func_call_datas:
            tools = data["functions"]
            chatrounds = data["chatrounds"]

            function_content = ""
            if len(tools) > 0:
                function_content = function_format.format(tools=json.dumps(tools, ensure_ascii=False, sort_keys=True))

            history = []
            for i in chatrounds:
                if i["role"]=="system":
                    continue

                if i["role"]=="user":
                    history.append(("user", i["content"]))

                if i["role"] == "assistant":
                    if "function_call" in i:
                        if not isinstance(i["function_call"], dict):
                            funccall_error_cnt+=1
                            continue
                        content  = "#function" + json.dumps({**{"content": i["content"]}, **i["function_call"]}, ensure_ascii=False)
                    else:
                        content = i["content"]
                    history.append(("assistant", content))


                if i["role"] == "function":
                    content  = json.dumps({**{"content": i["content"]}, **{"name": i["name"]}}, ensure_ascii=False)
                    history.append(("user", content))


            history = [i[1] for i in history]
            history[0] = "\\n".join([system_content,function_content, history[0]])

            for his_idx in range(0, len(history), 2):
                output = history[his_idx+1]

                if "#function" in output:
                    output = output.split("#function")[-1]

                try:
                    output = json.loads(output)
                except:
                    output = {"content": output}


                func_call_train_datas.append(
                    {
                        "instruction": history[his_idx],
                        "input": "",
                        "output": output,
                        "history": [history[:his_idx+2][i:i+2] for i in range(0, len(history[:his_idx]), 2)],
                        "functions": tools
                    },
                )
        return func_call_train_datas
`,paraId:4,tocIndex:2},{value:"If the input needs to be converted to a specific format (e.g., ChatML format or other human-bot formats), then you will need to go to ",paraId:5,tocIndex:3},{value:"src.context_builder.context_builder_family",paraId:5,tocIndex:3},{value:" and inherit the ",paraId:5,tocIndex:3},{value:"ContextBuilder",paraId:5,tocIndex:3},{value:" class to override the ",paraId:5,tocIndex:3},{value:"make_context",paraId:5,tocIndex:3},{value:" function. This function is used to convert the input to the corresponding required output. An example is as follows:",paraId:5,tocIndex:3},{value:`class ToolModel:
    def __init__(self, model_path: str, template: str, trust_remote_code=True, tensor_parallel_size=1, gpu_memory_utilization=0.25):
        self.model_path = model_path
        self.trust_remote_code = trust_remote_code
        self.tensor_parallel_size = tensor_parallel_size
        self.gpu_memory_utilization = gpu_memory_utilization
        self.load_model(self.model_path, self.trust_remote_code, self.tensor_parallel_size, self.gpu_memory_utilization)

    def generate(self, prompts: str, template: str = None, generate_configs: GenerateConfigs = None) -> list:
        '''\u4EA7\u51FA\u5BF9\u5E94\u7ED3\u679C'''
        pass

    def generate_params(
        self, generate_configs: GenerateConfigs,
    ):
        '''generate param'''
        kargs = generate_configs.dict()
        return kargs

    def load_model(self, model_path, trust_remote_code=True, tensor_parallel_size=1, gpu_memory_utilization=0.25):
        '''\u52A0\u8F7D\u6A21\u578B'''
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=trust_remote_code)
        self.model = AutoModelForCausalLM.from_pretrained(self.model_path, device_map="auto", trust_remote_code=trust_remote_code).eval()

        # self.model = LLM(model=model_path, trust_remote_code=trust_remote_code, tensor_parallel_size=tensor_parallel_size, gpu_memory_utilization=gpu_memory_utilization)
`,paraId:6,tocIndex:3},{value:"Registration can be done in ",paraId:7,tocIndex:4},{value:"~/models/__init__.py",paraId:7,tocIndex:4},{value:`from .base_model import ToolModel
__all__ = [
    "ToolModel",
]
`,paraId:8,tocIndex:4},{value:"Registration can also be done in ",paraId:9,tocIndex:4},{value:"~/evals/__init__.py",paraId:9,tocIndex:4},{value:`from .base_evaluation import ToolEvaluation
from .toolfill_evaluation import ToolFillEvaluation
from .toolparser_evaluation import ToolParserEvaluation
from .toolsummary_evaluation import ToolSummaryEvaluation
from .func_call_evaluation import FuncCallEvaluation
__all__ = [
    "ToolEvaluation", "ToolFillEvaluation", "ToolParserEvaluation", "ToolSummaryEvaluation", "FuncCallEvaluation"
]
`,paraId:10,tocIndex:4},{value:"Modify ",paraId:11,tocIndex:5},{value:"~/src/qwen_eval_main.py# datainfos",paraId:11,tocIndex:5},{value:" and ",paraId:11,tocIndex:5},{value:"model_infos",paraId:11,tocIndex:5},{value:`model_infos = [
    {"model_name": "", "template": "chatml", "model_path": "",
     "peft_path": "", "model_class": QwenModel}]
datainfos = [
    {"dataset_path": "~/fcdata_luban_zh_test.jsonl", "dataset_name": "fcdata_luban_zh", "tool_task": "func_call"},
    {"dataset_path": "~/test_datas/fcdata_zh_test_v1.jsonl", "dataset_name": "fcdata_zh", "tool_task": "func_call"},
]
`,paraId:12,tocIndex:5},{value:"To execute, run the following command:",paraId:13,tocIndex:5},{value:`python qwen_eval_main.py
`,paraId:14,tocIndex:5},{value:"To test on your own Huggingface-format model, the overall steps are divided into the following:",paraId:15,tocIndex:6},{value:"Write related code in ",paraId:16,tocIndex:6},{value:"~/getAssistantAns.py",paraId:16,tocIndex:6},{value:"Execute the test script",paraId:16,tocIndex:6},{value:"getAssistantAns",paraId:17},{value:`class GetAssistantAns():
    # \u6309\u7167\u81EA\u5DF1\u63A8\u7406\u9700\u6C42\u81EA\u5DF1\u4FEE\u6539\u4EE3\u7801

    def __init__(self, gpu_num=1):
        model = AutoModelForCausalLM.from_pretrained(model_name)
        device_list = []
        for gpu_idx in range(gpu_num):
            device_list.append(torch.device("cuda:0"))

        # \u5C06\u6A21\u578B\u79FB\u52A8\u5230\u6307\u5B9A\u7684GPU\u8BBE\u5907
        model.to(device)


    def gen_answer(self, chat_dict, gpu_index):
        # \u8FD9\u91CC\u5B9E\u9645\u6839\u636E\u81EA\u5DF1\u63A8\u7406\u903B\u8F91 \u7136\u540E\u8F6C\u4E3A\u6807\u51C6\u683C\u5F0F\u8FD4\u56DE
        # \u4EE5\u4E0B\u4EC5\u4EC5\u662F\u6837\u4F8B
        import time
        print(os.environ["CUDA_VISIBLE_DEVICES"])
        time.sleep(1)
        rtn_dict1 = {
                "role": "assistant",
                "content": None,
                "function_call":
                {
                    "name": "get_fudan_university_scoreline",
                    "arguments": "{\\n  \\"year\\": \\"2020\\"\\n}"
                }
            }

        rtn_dict2 =  {
                "role": "assistant",
                "content": "2020\u5E74\u590D\u65E6\u5927\u5B66\u7684\u5206\u6570\u7EBF\u5982\u4E0B\uFF1A\\n\\n- \u6587\u79D1\u4E00\u6279\uFF1A630\u5206\\n- \u6587\u79D1\u4E8C\u6279\uFF1A610\u5206\\n- \u7406\u79D1\u4E00\u6279\uFF1A650\u5206\\n- \u7406\u79D1\u4E8C\u6279\uFF1A630\u5206"
            }

        return random.choice([rtn_dict1, rtn_dict2])
`,paraId:18,tocIndex:7},{value:"Modify ",paraId:19,tocIndex:8},{value:"~/src/opensource_functioncall_evaluation.py # test_ans_file_list",paraId:19,tocIndex:8},{value:`test_ans_file_list = [
        "fcdata_zh_test.jsonl"
        ]
`,paraId:20,tocIndex:8},{value:"To execute, run the following command:",paraId:21,tocIndex:8},{value:`python opensource_functioncall_evaluation.py
`,paraId:22,tocIndex:8}]},97299:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:`We are fully compatible with OpenAI Function Calling in terms of data, with the format as follows:
`,paraId:0,tocIndex:0},{value:"Data format for Function Call",paraId:0,tocIndex:0},{value:"Input Key",paraId:1,tocIndex:0},{value:"Input Type",paraId:1,tocIndex:0},{value:"Input Description",paraId:1,tocIndex:0},{value:"functions",paraId:1,tocIndex:0},{value:"List[Swagger]",paraId:1,tocIndex:0},{value:"Collection of tools",paraId:1,tocIndex:0},{value:"chatrounds",paraId:1,tocIndex:0},{value:"List[chatround]",paraId:1,tocIndex:0},{value:"Multi-round dialogue data",paraId:1,tocIndex:0},{value:"Data format for chatrounds",paraId:2,tocIndex:0},{value:"Input Key",paraId:3,tocIndex:0},{value:"Input Type",paraId:3,tocIndex:0},{value:"Input Description",paraId:3,tocIndex:0},{value:"role",paraId:3,tocIndex:0},{value:"string",paraId:3,tocIndex:0},{value:"Role name, includes three categories: user, assistant, function",paraId:3,tocIndex:0},{value:"name",paraId:3,tocIndex:0},{value:"string",paraId:3,tocIndex:0},{value:"If the role is function, then the name field exists, which is the name of the function",paraId:3,tocIndex:0},{value:"content",paraId:3,tocIndex:0},{value:"string",paraId:3,tocIndex:0},{value:"Content returned by the role",paraId:3,tocIndex:0},{value:"function_call",paraId:3,tocIndex:0},{value:"dict",paraId:3,tocIndex:0},{value:"Tool invocation",paraId:3,tocIndex:0},{value:`{
    "functions":
    [
        {
            "name": "get_fudan_university_scoreline",
            "description": "Query the past years' cut-off scores for Fudan University, for example: querying the 2020 cut-off scores for Fudan University",
            "parameters":
            {
                "type": "object",
                "properties":
                {
                    "year":
                    {
                        "type": "string",
                        "description": "Year, for example: 2020, 2019, 2018"
                    }
                },
                "required":
                [
                    "year"
                ]
            }
        }
    ],
    "chatrounds":
    [
        {
            "role": "system",
            "content": "CodeFuse is an intelligent assistant targeted at the R&D sector, aiming to help users solve development-related issues in a neutral and harmless manner. All responses are returned in Markdown format.\\nYou can utilize many tools and functions to complete the given tasks. In each step, you need to analyze the current state and determine the next course of action through function calls. You can attempt multiple times. If you plan to continuously try different conditions, please try one condition at a time. If a Finish function is provided, it ends with a Finish call, otherwise, it concludes with a dialogue without a function_call."
        },
        {
            "role": "user",
            "content": "Query the 2020 cut-off scores for Fudan University"
        },
        {
            "role": "assistant",
            "content": null,
            "function_call":
            {
                "name": "get_fudan_university_scoreline",
                "arguments": "{\\n  \\"year\\": \\"2020\\"\\n}"
            }
        },
        {
            "role": "function",
            "name": "get_fudan_university_scoreline",
            "content": "{\\n    \\"scoreline\\":{\\n        \\"Liberal Arts first tier\\": 630,    \\n        \\"Liberal Arts second tier\\": 610,  \\n        \\"Science first tier\\": 650,  \\n        \\"Science second tier\\": 630  \\n    }\\n}"
        },
        {
            "role": "assistant",
            "content": "The 2020 cut-off scores for Fudan University are as follows:\\n\\n- Liberal Arts first tier: 630 points\\n- Liberal Arts second tier: 610 points\\n- Science first tier: 650 points\\n- Science second tier: 630 points"
        }
    ]
}
`,paraId:4,tocIndex:0},{value:"The above data example of Function Call is used to answer users' queries about the admission scores of a certain university after a specific set of tools is provided.",paraId:5,tocIndex:0},{value:`Since general models generally lack the capability of tool invocation, it is necessary to fine-tune the general model before performing Tool Learn-Eval evaluation, to teach the model the basic paradigm of tool usage.
Below, we define several metrics for assessing the use of tools:`,paraId:6,tocIndex:1},{value:"The sum of \u2461\u2462\u2463\u2464 represents the total number of tool invocation failures, with \u2464 being a special case of tool name recognition failure.",paraId:7,tocIndex:1}]},2621:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"If you need to test your own huggingface-formatted model, the overall steps are as follows:",paraId:0,tocIndex:1},{value:"Write the loader function for the model.",paraId:1,tocIndex:1},{value:"Write the context_builder function for the model.",paraId:1,tocIndex:1},{value:"Register the model in the configuration file.",paraId:1,tocIndex:1},{value:`Run the testing script.
If the model does not require any special processing after loading, and the input does not need to be converted to a specific format (e.g. chatml format or other human-bot formats), you can directly proceed to step 4 to initiate the testing.`,paraId:1,tocIndex:1},{value:"If the model requires additional processing after loading (e.g. adjusting the tokenizer), you need to inherit the ",paraId:2,tocIndex:2},{value:"ModelAndTokenizerLoader",paraId:2,tocIndex:2},{value:" class in ",paraId:2,tocIndex:2},{value:"src.context_builder.context_builder_family.py",paraId:2,tocIndex:2},{value:" and override the corresponding ",paraId:2,tocIndex:2},{value:"load_model",paraId:2,tocIndex:2},{value:" and ",paraId:2,tocIndex:2},{value:"load_tokenizer",paraId:2,tocIndex:2},{value:" functions. You can refer to the following example:",paraId:2,tocIndex:2},{value:`class QwenModelAndTokenizerLoader(ModelAndTokenizerLoader):
    def __init__(self):
      super().__init__()
      pass

    def load_model(self, model_path: str):
        model = super().load_model(model_path)
        model.generation_config = GenerationConfig.from_pretrained(model_path)
        return model

    def load_tokenizer(self, model_path: str):
        tokenizer = super().load_tokenizer(model_path)

        # read generation config
        with open(model_path + '/generation_config.json', 'r') as f:
        generation_config = json.load(f)
        tokenizer.pad_token_id = generation_config['pad_token_id']
        tokenizer.eos_token_id = generation_config['eos_token_id']
        return tokenizer
`,paraId:3,tocIndex:2},{value:"If the input needs to be converted to a specific format (e.g. chatml format or other human-bot formats), you need to inherit the ContextBuilder class in ",paraId:4,tocIndex:3},{value:"src.context_builder.context_builder_family",paraId:4,tocIndex:3},{value:" and override the make_context function. This function is used to convert the input to the corresponding required format. An example is shown below:",paraId:4,tocIndex:3},{value:`class QwenChatContextBuilder(ContextBuilder):
    def __init__(self):
        super().__init__()

    def make_context(
        self,
        model,
        tokenizer,
        query: str,
        system: str = "you are a helpful assistant"
    ):
        '''
        model: PretrainedModel
        tokenizer: PretrainedTokenzier
        query: Input string
        system: System prompt if needed
        '''
        im_start, im_end = "<|im_start|>", "<|im_end|>"
        im_start_tokens = [tokenizer.im_start_id]
        im_end_tokens = [tokenizer.im_end_id]
        nl_tokens = tokenizer.encode("\\n")

        def _tokenize_str(role, content):
            return f"{role}\\n{content}", tokenizer.encode(
                role, allowed_special=set()
            ) + nl_tokens + tokenizer.encode(content, allowed_special=set())

        system_text, system_tokens_part = _tokenize_str("system", system)
        system_tokens = im_start_tokens + system_tokens_part + im_end_tokens

        raw_text = ""
        context_tokens = []

        context_tokens = system_tokens + context_tokens
        raw_text = f"{im_start}{system_text}{im_end}" + raw_text
        context_tokens += (
            nl_tokens
            + im_start_tokens
            + _tokenize_str("user", query)[1]
            + im_end_tokens
            + nl_tokens
            + im_start_tokens
            + tokenizer.encode("assistant")
            + nl_tokens
        )
        raw_text += f"\\n{im_start}user\\n{query}{im_end}\\n{im_start}assistant\\n"
        return raw_text, context_tokens
`,paraId:5,tocIndex:3},{value:"Go to the ",paraId:6,tocIndex:4},{value:"model_conf.json",paraId:6,tocIndex:4},{value:" file in the conf directory and register the corresponding model name and the loader and context_builder that will be used for this model. Simply write the class names defined in the first and second steps for the loader and context_builder. Here is an example:",paraId:6,tocIndex:4},{value:`{
  "Qwen-Chat": {
    "loader": "QwenModelAndTokenizerLoader",
    "context_builder": "QwenChatContextBuilder"
  }
}
`,paraId:7,tocIndex:4},{value:"Run the following code to initiate the test:",paraId:8,tocIndex:5},{value:`# model_path: path to the model for testing
# model_name: the model name corresponding to the model in the configuration file, default is Default, which represents using the default loader and context_builder
# model_conf_path: path to the model configuration file, usually the devopseval_dataset_fp.json file in the conf directory
# eval_dataset_list: the names of the datasets to be tested, default is all to test all datasets, if you need to test one or more datasets, use the # symbol to connect them, for example: dataset1#dataset2
# eval_dataset_fp_conf_path: path to the dataset configuration file
# eval_dataset_type: the type of testing, only supports the default test type of test dataset
# data_path: path to the evaluation dataset, fill in the downloaded dataset address
# k_shot: supports 0-5, represents the number of example prefixes added for few-shot

python src/run_eval.py \\
--model_path path_to_model \\
--model_name model_name_in_conf \\
--model_conf_path path_to_model_conf \\
--eval_dataset_list all \\
--eval_dataset_fp_conf_path path_to_dataset_conf \\
--eval_dataset_type test \\
--data_path path_to_downloaded_devops_eval_data \\
--k_shot 0
`,paraId:9,tocIndex:5},{value:"For example, if the evaluation dataset is downloaded to ",paraId:10,tocIndex:5},{value:"folder1",paraId:10,tocIndex:5},{value:", the code is placed in ",paraId:10,tocIndex:5},{value:"folder2",paraId:10,tocIndex:5},{value:", and the model is in ",paraId:10,tocIndex:5},{value:"folder3",paraId:10,tocIndex:5},{value:", and the model does not require custom loader and context_builder, and all zero-shot scores of all datasets need to be tested, you can use the following script to initiate the test:",paraId:10,tocIndex:5},{value:`python folder2/src/run_eval.py \\
--model_path folder3 \\
--model_name Default \\
--model_conf_path folder1/conf/model_conf.json \\
--eval_dataset_list all \\
--eval_dataset_fp_conf_path folder1/conf/devopseval_dataset_fp.json \\
--eval_dataset_type test \\
--data_path folder2 \\
--k_shot 0
`,paraId:11,tocIndex:5}]},14690:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:`DevOps-Model is a large language model for the Chinese DevOps field jointly released by Ant Group and Peking University. By collecting professional data related to the DevOps domain and conducting additional training and alignment on the model, a large model has been produced to help engineers enhance efficiency throughout the entire development and operations lifecycle. This fills the current gap in large models within the DevOps domain, with the aim to provide solutions to any problems by asking DevOps-Model!
We have now open-sourced two versions of the model, the Base model with additional training and the Chat model after alignment, in both 7B and 14B specifications, as well as the corresponding training code. We welcome everyone to collaborate and contribute!`,paraId:0,tocIndex:0},{value:"GitHub Address: ",paraId:1,tocIndex:1},{value:"https://github.com/codefuse-ai/CodeFuse-DevOps-Model/tree/main",paraId:1,tocIndex:1},{value:`
ModelScope Address:`,paraId:1,tocIndex:1},{value:"DevOps-Model-7B-Base: ",paraId:2,tocIndex:1},{value:"https://modelscope.cn/models/codefuse-ai/CodeFuse-DevOps-Model-7B-Base/summary",paraId:2,tocIndex:1},{value:"DevOps-Model-7B-Chat: ",paraId:2,tocIndex:1},{value:"https://modelscope.cn/models/codefuse-ai/CodeFuse-DevOps-Model-7B-Chat/summary",paraId:2,tocIndex:1},{value:"DevOps-Model-14B-Base: ",paraId:2,tocIndex:1},{value:"https://modelscope.cn/models/codefuse-ai/CodeFuse-DevOps-Model-14B-Base/summary",paraId:2,tocIndex:1},{value:"DevOps-Model-14B-Chat: ",paraId:2,tocIndex:1},{value:"https://modelscope.cn/models/codefuse-ai/CodeFuse-DevOps-Model-14B-Chat/summary",paraId:2,tocIndex:1},{value:"For model evaluation, there was initially no benchmark for testing in the DevOps domain, so we first selected some domain-related multiple-choice questions from general open-source tests for evaluation. The specific test data is as follows:",paraId:3,tocIndex:2},{value:"Dataset",paraId:4,tocIndex:2},{value:"Subject",paraId:4,tocIndex:2},{value:"Total Questions",paraId:4,tocIndex:2},{value:"CMMLU",paraId:4,tocIndex:2},{value:"Computer science 204",paraId:4,tocIndex:2},{value:"Computer",paraId:4,tocIndex:2},{value:"security",paraId:4,tocIndex:2},{value:"171",paraId:4,tocIndex:2},{value:"Machine",paraId:4,tocIndex:2},{value:"learning",paraId:4,tocIndex:2},{value:"122",paraId:4,tocIndex:2},{value:"CEval",paraId:4,tocIndex:2},{value:"college programming",paraId:4,tocIndex:2},{value:"37",paraId:4,tocIndex:2},{value:"CEval",paraId:4,tocIndex:2},{value:"computer_architecture",paraId:4,tocIndex:2},{value:"21",paraId:4,tocIndex:2},{value:"CEval",paraId:4,tocIndex:2},{value:"computer_network",paraId:4,tocIndex:2},{value:"19",paraId:4,tocIndex:2},{value:"\u603B\u8BA1",paraId:4,tocIndex:2},{value:"\u603B\u8BA1\u9898\u76EE\u6570",paraId:4,tocIndex:2},{value:"574",paraId:4,tocIndex:2},{value:"Since all are multiple-choice questions, we adopted the method of selecting the highest-scoring Token among the four option Tokens in the first Token produced by the model as the model's answer to the question. We also tested Zero-shot and Five-shot results.",paraId:5,tocIndex:3},{value:"The specific scores are shown in the table below:",paraId:6,tocIndex:4},{value:"Scale of Parameters",paraId:7,tocIndex:4},{value:"Model",paraId:7,tocIndex:4},{value:"Model Size",paraId:7,tocIndex:4},{value:"Zero-shot Score",paraId:7,tocIndex:4},{value:"Five-shot Score",paraId:7,tocIndex:4},{value:"10+ B",paraId:7,tocIndex:4},{value:"DevOps-Model-14B-Base",paraId:7,tocIndex:4},{value:"14B",paraId:7,tocIndex:4},{value:"70.73",paraId:7,tocIndex:4},{value:"73.00",paraId:7,tocIndex:4},{value:"10+ B",paraId:7,tocIndex:4},{value:"Qwen-14B-Base",paraId:7,tocIndex:4},{value:"14B",paraId:7,tocIndex:4},{value:"69.16",paraId:7,tocIndex:4},{value:"71.25",paraId:7,tocIndex:4},{value:"10+ B",paraId:7,tocIndex:4},{value:"Baichuan2-13B-Base",paraId:7,tocIndex:4},{value:"13B",paraId:7,tocIndex:4},{value:"55.75",paraId:7,tocIndex:4},{value:"61.15",paraId:7,tocIndex:4},{value:"10+ B",paraId:7,tocIndex:4},{value:"DevOps-Model-14B-Chat",paraId:7,tocIndex:4},{value:"14B",paraId:7,tocIndex:4},{value:"74.04",paraId:7,tocIndex:4},{value:"75.96",paraId:7,tocIndex:4},{value:"10+ B",paraId:7,tocIndex:4},{value:"Qwen-14B-Chat",paraId:7,tocIndex:4},{value:"14B",paraId:7,tocIndex:4},{value:"69.16",paraId:7,tocIndex:4},{value:"70.03",paraId:7,tocIndex:4},{value:"10+ B",paraId:7,tocIndex:4},{value:"Baichuan2-13B-Chat",paraId:7,tocIndex:4},{value:"13B",paraId:7,tocIndex:4},{value:"52.79",paraId:7,tocIndex:4},{value:"55.23",paraId:7,tocIndex:4},{value:"7B",paraId:7,tocIndex:4},{value:"DevOps-Model-7B-Base",paraId:7,tocIndex:4},{value:"7B",paraId:7,tocIndex:4},{value:"62.72",paraId:7,tocIndex:4},{value:"62.02",paraId:7,tocIndex:4},{value:"7B",paraId:7,tocIndex:4},{value:"Qwen-7B-Base",paraId:7,tocIndex:4},{value:"7B",paraId:7,tocIndex:4},{value:"55.75",paraId:7,tocIndex:4},{value:"56.0",paraId:7,tocIndex:4},{value:"7B",paraId:7,tocIndex:4},{value:"Baichuan2-7B-Base",paraId:7,tocIndex:4},{value:"7B",paraId:7,tocIndex:4},{value:"49.30",paraId:7,tocIndex:4},{value:"55.4",paraId:7,tocIndex:4},{value:"7B",paraId:7,tocIndex:4},{value:"Internlm-7B-Base",paraId:7,tocIndex:4},{value:"7B",paraId:7,tocIndex:4},{value:"47.56",paraId:7,tocIndex:4},{value:"52.6",paraId:7,tocIndex:4},{value:"7B",paraId:7,tocIndex:4},{value:"DevOps-Model-7B-Chat",paraId:7,tocIndex:4},{value:"7B",paraId:7,tocIndex:4},{value:"62.20",paraId:7,tocIndex:4},{value:"64.11",paraId:7,tocIndex:4},{value:"7B",paraId:7,tocIndex:4},{value:"Qwen-7B-Chat",paraId:7,tocIndex:4},{value:"7B",paraId:7,tocIndex:4},{value:"46.00",paraId:7,tocIndex:4},{value:"52.44",paraId:7,tocIndex:4},{value:"7B",paraId:7,tocIndex:4},{value:"Baichuan2-7B-Chat",paraId:7,tocIndex:4},{value:"7B",paraId:7,tocIndex:4},{value:"52.26",paraId:7,tocIndex:4},{value:"54.46",paraId:7,tocIndex:4},{value:"7B",paraId:7,tocIndex:4},{value:"Internlm-7B-Chat",paraId:7,tocIndex:4},{value:"7B",paraId:7,tocIndex:4},{value:"52.61",paraId:7,tocIndex:4},{value:"55.75",paraId:7,tocIndex:4}]},78099:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"Please install the packages listed in the requirements.txt file from the GitHub address first. You can refer to the following code:",paraId:0,tocIndex:0},{value:`pip install -r requirements.txt
`,paraId:1,tocIndex:0},{value:"Model download information is as follows:",paraId:2,tocIndex:1},{value:"\u{1F917} Huggingface Address",paraId:3,tocIndex:1},{value:"-",paraId:4,tocIndex:1},{value:"Base Model",paraId:4,tocIndex:1},{value:"Aligned Model",paraId:4,tocIndex:1},{value:"7B",paraId:4,tocIndex:1},{value:"DevOps-Model-7B-Base",paraId:4,tocIndex:1},{value:"DevOps-Model-7B-Chat",paraId:4,tocIndex:1},{value:"14B",paraId:4,tocIndex:1},{value:"DevOps-Model-14B-Base",paraId:4,tocIndex:1},{value:"DevOps-Model-14B-Chat",paraId:4,tocIndex:1},{value:"\u{1F916} ModelScope Address",paraId:5,tocIndex:1},{value:"-",paraId:6,tocIndex:1},{value:"Base Model",paraId:6,tocIndex:1},{value:"Aligned Model",paraId:6,tocIndex:1},{value:"7B",paraId:6,tocIndex:1},{value:"DevOps-Model-7B-Base",paraId:6,tocIndex:1},{value:"DevOps-Model-7B-Chat",paraId:6,tocIndex:1},{value:"14B",paraId:6,tocIndex:1},{value:"DevOps-Model-14B-Base",paraId:6,tocIndex:1},{value:"DevOps-Model-14B-Chat",paraId:6,tocIndex:1},{value:"Find the version of the Chat model you want to download; currently, 7B and 14B models are provided.",paraId:7,tocIndex:1},{value:"Interact with the Chat model using the following code:",paraId:8,tocIndex:2},{value:`from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

tokenizer = AutoTokenizer.from_pretrained("path_to_DevOps-Model-Chat", trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained("path_to_DevOps-Model-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()

# \u6307\u5B9A generation_config
model.generation_config = GenerationConfig.from_pretrained("path_to_DevOps-Model-Chat", trust_remote_code=True)

# First round of conversation
resp, hist = model.chat(query='\u4F60\u662F\u8C01', tokenizer=tokenizer, history=None)
print(resp)
# \u6211\u662F DevOps-Model\uFF0C\u4E00\u4E2A\u7531\u8682\u8681\u96C6\u56E2\u5E73\u53F0\u6280\u672F\u4E8B\u4E1A\u7FA4\u98CE\u9669\u667A\u80FD\u56E2\u961F\u548C\u5317\u4EAC\u5927\u5B66\u8054\u5408\u7814\u53D1\u7684\u4EBA\u5DE5\u667A\u80FD\u673A\u5668\u4EBA\uFF0C\u53EF\u4EE5\u4E0E\u7528\u6237\u8FDB\u884C\u81EA\u7136\u8BED\u8A00\u4EA4\u4E92\uFF0C\u5E76\u534F\u52A9\u89E3\u7B54 DevOps \u5168\u751F\u547D\u5468\u671F\u4E2D\u7684\u5404\u79CD\u95EE\u9898\u3002\u5982\u679C\u60A8\u6709\u4EFB\u4F55\u9700\u8981\u534F\u52A9\u7684\u95EE\u9898\u6216\u8005\u60F3\u8981\u8FDB\u884C\u95F2\u804A\uFF0C\u90FD\u53EF\u4EE5\u548C\u6211\u4EA4\u6D41\u54E6\u3002

# Second round of conversation
resp2, hist2 = model.chat(query='Java \u4E2D HashMap \u548C Hashtable \u6709\u4EC0\u4E48\u533A\u522B', tokenizer=tokenizer, history=hist)
print(resp2)
# HashMap \u548C Hashtable \u90FD\u662F Java \u4E2D\u5E38\u7528\u7684\u54C8\u5E0C\u8868\u5B9E\u73B0\uFF0C\u5B83\u4EEC\u7684\u4E3B\u8981\u533A\u522B\u5728\u4E8E\uFF1A
# 1. Hashtable \u662F\u7EBF\u7A0B\u5B89\u5168\u7684\uFF0C\u800C HashMap \u4E0D\u662F\u7EBF\u7A0B\u5B89\u5168\u7684\uFF0C\u56E0\u6B64\u5728\u591A\u7EBF\u7A0B\u73AF\u5883\u4E0B\uFF0CHashtable \u7684\u6027\u80FD\u66F4\u7A33\u5B9A\u3002
# 2. Hashtable \u4E2D\u7684\u65B9\u6CD5\u90FD\u662F\u540C\u6B65\u7684\uFF0C\u800C HashMap \u7684\u65B9\u6CD5\u4E0D\u662F\u540C\u6B65\u7684\uFF0C\u56E0\u6B64\u5728\u591A\u7EBF\u7A0B\u73AF\u5883\u4E0B\uFF0CHashtable \u7684\u6027\u80FD\u66F4\u597D\u3002
# 3. Hashtable \u4E2D\u7684 key \u548C value \u90FD\u5FC5\u987B\u5B9E\u73B0 Serializable \u63A5\u53E3\uFF0C\u800C HashMap \u4E2D\u7684 key \u548C value \u53EF\u4EE5\u662F\u4EFB\u4F55\u5BF9\u8C61\uFF0C\u5305\u62EC\u57FA\u672C\u6570\u636E\u7C7B\u578B\u3002
# 4. Hashtable \u7684\u521D\u59CB\u5BB9\u91CF\u662F 11\uFF0C\u800C HashMap \u7684\u521D\u59CB\u5BB9\u91CF\u662F 16\u3002
# \u603B\u4E4B\uFF0C\u5982\u679C\u9700\u8981\u5728\u591A\u7EBF\u7A0B\u73AF\u5883\u4E0B\u4F7F\u7528\u54C8\u5E0C\u8868\uFF0C\u5E76\u4E14\u9700\u8981\u4FDD\u8BC1\u7EBF\u7A0B\u5B89\u5168\uFF0C\u90A3\u4E48\u5E94\u8BE5\u4F7F\u7528 Hashtable\uFF1B\u5982\u679C\u4E0D\u9700\u8981\u8003\u8651\u7EBF\u7A0B\u5B89\u5168\uFF0C\u6216\u8005\u9700\u8981\u5FEB\u901F\u5730\u8FDB\u884C\u54C8\u5E0C\u8868\u64CD\u4F5C\uFF0C\u90A3\u4E48\u5E94\u8BE5\u4F7F\u7528 HashMap\u3002

# Third round of conversation
resp3, hist3 = model.chat(query='\u7EBF\u7A0B\u5B89\u5168\u4EE3\u8868\u4EC0\u4E48', tokenizer=tokenizer, history=hist2)
print(resp3)
# \u7EBF\u7A0B\u5B89\u5168\u662F\u6307\u5728\u591A\u7EBF\u7A0B\u73AF\u5883\u4E0B\uFF0C\u7A0B\u5E8F\u80FD\u591F\u6B63\u786E\u5730\u5904\u7406\u5E76\u53D1\u8BBF\u95EE\uFF0C\u5E76\u4E14\u4E0D\u4F1A\u51FA\u73B0\u6570\u636E\u7ADE\u4E89\u3001\u6B7B\u9501\u3001\u9965\u997F\u7B49\u5F02\u5E38\u60C5\u51B5\u3002\u7EBF\u7A0B\u5B89\u5168\u7684\u7A0B\u5E8F\u53EF\u4EE5\u4FDD\u8BC1\u5728\u4E0D\u540C\u7684\u7EBF\u7A0B\u4E4B\u95F4\u5171\u4EAB\u540C\u4E00\u4E2A\u6570\u636E\u7ED3\u6784\u65F6\uFF0C\u6570\u636E\u7684\u6B63\u786E\u6027\u548C\u4E00\u81F4\u6027\u3002\u7EBF\u7A0B\u5B89\u5168\u7684\u5B9E\u73B0\u901A\u5E38\u9700\u8981\u4F7F\u7528\u540C\u6B65\u673A\u5236\uFF0C\u5982\u9501\u3001\u539F\u5B50\u64CD\u4F5C\u7B49\uFF0C\u6765\u4FDD\u8BC1\u5BF9\u5171\u4EAB\u6570\u636E\u7684\u8BBF\u95EE\u662F\u7EBF\u7A0B\u5B89\u5168\u7684\u3002\u5728 Java \u4E2D\uFF0C\u53EF\u4EE5\u901A\u8FC7 synchronized \u5173\u952E\u5B57\u3001Lock \u63A5\u53E3\u7B49\u673A\u5236\u6765\u5B9E\u73B0\u7EBF\u7A0B\u5B89\u5168\u3002
`,paraId:9,tocIndex:2}]},90378:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"According to the literature review, it is known that most domain models are based on conversational models and undergo knowledge infusion through Supervised Fine-Tuning (SFT). However, the QA corpus required for SFT fine-tuning largely comes from ChatGPT generation, which may not fully cover domain knowledge.",paraId:0,tocIndex:0},{value:"Therefore, the DevOps-Model adopts a pre-training plus training followed by SFT fine-tuning approach, as illustrated in Figure 2.1. We believe that for large domain models, additional pre-training is necessary. This can inject some domain knowledge into the large model during the pre-training phase. If this knowledge has not been covered during the general large model's pre-training, it will allow the large model to learn new information; if it has been covered, it will further reinforce the model's knowledge. The second step is model alignment, aiming to enable the large model to provide the most appropriate content in response to questions.",paraId:1,tocIndex:0},{value:"The model is positioned as a large Chinese DevOps domain model, so we collect pre-training and QA data related to Chinese DevOps.",paraId:2,tocIndex:2},{value:`The pre-training data mainly comes from the internet, including technical blogs, documentation, and books, amounting to over 50GB of pre-training corpus data.
For the QA data, our goal is not only to align the model with general Q&A capabilities but also to learn how to answer questions better in the DevOps domain. Therefore, we collected both general single-turn and multi-turn dialogue data and generated domain-specific QA data for the DevOps field through crawling and using ChatGPT. Ultimately, we carefully selected around 200K pieces of QA data for SFT fine-tuning training, as shown in the table below.`,paraId:3,tocIndex:2},{value:"Data Type",paraId:4,tocIndex:2},{value:"Volume",paraId:4,tocIndex:2},{value:"General Single-turn QA",paraId:4,tocIndex:2},{value:"50K",paraId:4,tocIndex:2},{value:"General Multi-turn QA",paraId:4,tocIndex:2},{value:"20K",paraId:4,tocIndex:2},{value:"DevOps Domain QA",paraId:4,tocIndex:2},{value:"130K",paraId:4,tocIndex:2},{value:"Since most of the pre-training data is collected from the internet, the quality can be uneven. As data is the most crucial component in large model training, we established a cleaning Pipeline as shown above to thoroughly filter the quality of the collected data.",paraId:5,tocIndex:3},{value:`First, experts and manual screening have summarized a set of heuristic filtering rules at the document level, primarily to filter out those documents of very poor quality.
Then, even within an article of slightly lower quality, there may still be some valuable domain knowledge, which we need to collect as much as possible. Here, we split the article into paragraphs.
Next, the split paragraphs are filtered again using the rules from step 1, yielding a batch of paragraphs that have passed rule-based filtering.
We then picked out 1000 paragraphs for labeling by experienced professional developers to obtain high-quality labeled data.
Finally, we trained a scoring model based on the labeling results to score the quality of paragraphs. The vector model for paragraphs was the pre-trained Chinese version of Sentence-Bert, and the scoring algorithm was logistic regression. To avoid errors in the scoring model, we used the Pareto distribution to decide whether to filter a paragraph based on its quality score.
After this Pipeline, we finally settled on approximately 15GB of data for the pre-training plus training of the large model.`,paraId:6,tocIndex:3}]},46532:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:`CodeFuse-VLM is a Multimodal LLM(MLLM) framework that provides users with multiple vision encoders, multimodal alignment adapters, and LLMs. Through CodeFuse-VLM framework, users are able to customize their own MLLM model to adapt their own tasks.
As more and more models are published on Huggingface community, there will be more open-source vision encoders and LLMs. Each of these models has their own specialties, e.g. Code-LLama is good at code-related tasks but has poor performance for Chinese tasks. Therefore, we built CodeFuse-VLM framework to support multiple vision encoders, multimodal alignment adapters, and LLMs to adapt different types of tasks.
`,paraId:0,tocIndex:0},{value:`Under CodeFuse-VLM framework, we use cross attention multimodal adapter, Qwen-14B LLM, and Qwen-VL's vision encoder to train CodeFuse-VLM-14B model. On multiple benchmarks, our CodeFuse-VLM-14B shows superior performances over Qwen-VL and LLAVA-1.5.
`,paraId:1,tocIndex:0},{value:"Here is the table for different MLLM model's performance on benchmarks",paraId:2,tocIndex:0},{value:"Model",paraId:3,tocIndex:0},{value:"MMBench",paraId:3,tocIndex:0},{value:"MMBench-CN",paraId:3,tocIndex:0},{value:"VqaV2",paraId:3,tocIndex:0},{value:"GQA",paraId:3,tocIndex:0},{value:"TextVQA",paraId:3,tocIndex:0},{value:"Vizwiz",paraId:3,tocIndex:0},{value:"LLAVA-1.5",paraId:3,tocIndex:0},{value:"67.7",paraId:3,tocIndex:0},{value:"63.6",paraId:3,tocIndex:0},{value:"80.0",paraId:3,tocIndex:0},{value:"63.3",paraId:3,tocIndex:0},{value:"61.3",paraId:3,tocIndex:0},{value:"53.6",paraId:3,tocIndex:0},{value:"Qwen-VL",paraId:3,tocIndex:0},{value:"60.6",paraId:3,tocIndex:0},{value:"56.7",paraId:3,tocIndex:0},{value:"78.2",paraId:3,tocIndex:0},{value:"57.5",paraId:3,tocIndex:0},{value:"63.8",paraId:3,tocIndex:0},{value:"38.9",paraId:3,tocIndex:0},{value:"CodeFuse-VLM-14B",paraId:3,tocIndex:0},{value:"75.7",paraId:3,tocIndex:0},{value:"69.8",paraId:3,tocIndex:0},{value:"79.3",paraId:3,tocIndex:0},{value:"59.4",paraId:3,tocIndex:0},{value:"63.9",paraId:3,tocIndex:0},{value:"45.3",paraId:3,tocIndex:0},{value:"Our model achieved high ranking on MMBenchmark: ",paraId:4,tocIndex:0},{value:"https://mmbench.opencompass.org.cn/leaderboard",paraId:4,tocIndex:0},{value:"Here's our model's demo video",paraId:5,tocIndex:0},{value:`
 `,paraId:6}]},80406:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"Install",paraId:0,tocIndex:0},{value:"Datasets",paraId:1,tocIndex:0},{value:"Multimodal Alignment",paraId:2,tocIndex:0},{value:"Visual Instruction Tuning",paraId:3,tocIndex:0},{value:"Evaluation",paraId:4,tocIndex:0},{value:"Please run sh init_env.sh",paraId:5,tocIndex:1},{value:"Here's the table of datasets we used to train CodeFuse-VLM-14B:",paraId:6,tocIndex:2},{value:"Dataset",paraId:7,tocIndex:2},{value:"Task Type",paraId:7,tocIndex:2},{value:"Number of Samples",paraId:7,tocIndex:2},{value:"synthdog-en",paraId:7,tocIndex:2},{value:"OCR",paraId:7,tocIndex:2},{value:"800,000",paraId:7,tocIndex:2},{value:"synthdog-zh",paraId:7,tocIndex:2},{value:"OCR",paraId:7,tocIndex:2},{value:"800,000",paraId:7,tocIndex:2},{value:"cc3m(downsampled)",paraId:7,tocIndex:2},{value:"Image Caption",paraId:7,tocIndex:2},{value:"600,000",paraId:7,tocIndex:2},{value:"cc3m(downsampled)",paraId:7,tocIndex:2},{value:"Image Caption",paraId:7,tocIndex:2},{value:"600,000",paraId:7,tocIndex:2},{value:"SBU",paraId:7,tocIndex:2},{value:"Image Caption",paraId:7,tocIndex:2},{value:"850,000",paraId:7,tocIndex:2},{value:"Visual Genome VQA (Downsampled)",paraId:7,tocIndex:2},{value:"Visual Question Answer(VQA)",paraId:7,tocIndex:2},{value:"500,000",paraId:7,tocIndex:2},{value:"Visual Genome Region descriptions (Downsampled)",paraId:7,tocIndex:2},{value:"Reference Grouding",paraId:7,tocIndex:2},{value:"500,000",paraId:7,tocIndex:2},{value:"Visual Genome objects (Downsampled)",paraId:7,tocIndex:2},{value:"Grounded Caption",paraId:7,tocIndex:2},{value:"500,000",paraId:7,tocIndex:2},{value:"OCR VQA (Downsampled)",paraId:7,tocIndex:2},{value:"OCR and VQA",paraId:7,tocIndex:2},{value:"500,000",paraId:7,tocIndex:2},{value:"Please download these datasets on their own official websites.",paraId:8,tocIndex:2},{value:"Please run sh scripts/pretrain.sh or sh scripts/pretrain_multinode.sh",paraId:9,tocIndex:3},{value:"Please run sh scripts/finetune.sh or sh scripts/finetune_multinode.sh",paraId:10,tocIndex:4},{value:"Please run python scripts in directory llava/eval/. Our pre-trained CodeFuse-VLM-14B can be loaded with the following code:",paraId:11,tocIndex:5},{value:`import os
from llava.model.builder import load_mixed_pretrained_model

model_path = '/pretrained/model/path'
tokenizer, model, image_processor, context_len = load_mixed_pretrained_model(model_path, None, 'qwen-vl-14b', os.path.join(model_path, 'Qwen-VL-visual'), 'cross_attn', os.path.join(model_path, 'mm_projector/mm_projector.bin'))
`,paraId:12,tocIndex:5},{value:"You can also run scripts/merge_qwen_vl_weights.sh first and load the merged model by the following code:",paraId:13,tocIndex:5},{value:`from llava.model import LlavaQWenForCausalLM

model = LlavaQWenForCausalLM.from_pretrained('/path/to/our/pretrained/model')
`,paraId:14,tocIndex:5},{value:"Here's the demo video of front-end code copilot backed by our VLM model",paraId:15,tocIndex:6},{value:`
 `,paraId:16}]},51790:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"news",paraId:0,tocIndex:0},{value:"Introduction",paraId:1,tocIndex:0},{value:"Modules",paraId:2,tocIndex:0},{value:"Acknowledgements",paraId:3,tocIndex:0},{value:"Contributing",paraId:4,tocIndex:0},{value:"\u{1F525}\u{1F525}[2023.12.10] we integrate LLM embedding frameworks such as 'llmEmb', 'ONNX', 'PaddleNLP', 'FastText', alone with the image embedding framework 'timm', to bolster embedding functionality.",paraId:5,tocIndex:1},{value:"\u{1F525}\u{1F525}[2023.11.20] codefuse-ModelCache has integrated local storage, such as sqlite and faiss, providing users with the convenience of quickly initiating tests.",paraId:5,tocIndex:1},{value:"[2023.08.26] codefuse-ModelCache...",paraId:5,tocIndex:1},{value:"Codefuse-ModelCache is a semantic cache for large language models (LLMs). By caching pre-generated model results, it reduces response time for similar requests and improves user experience. ",paraId:6,tocIndex:2},{value:"This project aims to optimize services by introducing a caching mechanism. It helps businesses and research institutions reduce the cost of inference deployment, improve model performance and efficiency, and provide scalable services for large models. Through open-source, we aim to share and exchange technologies related to large model semantic cache.",paraId:6,tocIndex:2},{value:"This project has referenced the following open-source projects. We would like to express our gratitude to the projects and their developers for their contributions and research.",paraId:7,tocIndex:4},{value:"GPTCache",paraId:7,tocIndex:4},{value:"ModelCache is a captivating and invaluable project, whether you are an experienced developer or a novice just starting out, your contributions to this project are warmly welcomed. Your involvement in this project, be it through raising issues, providing suggestions, writing code, or documenting and creating examples, will enhance the project's quality and make a significant contribution to the open-source community.",paraId:8,tocIndex:5}]},79531:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"Python version: 3.8 or higher",paraId:0,tocIndex:0},{value:"To install dependencies: pip install requirements.txt",paraId:0,tocIndex:0},{value:"Before starting the service, the following environment configurations should be performed:",paraId:1,tocIndex:1},{value:"Install relational database MySQL, import SQL to create tables, SQL file: reference_doc/create_table.sql",paraId:1,tocIndex:1},{value:"Install vector database Milvus",paraId:1,tocIndex:1},{value:`Add database access information to the configuration files, which are:
`,paraId:1,tocIndex:1},{value:"modelcache/config/milvus_config.ini",paraId:2,tocIndex:1},{value:"modelcache/config/mysql_config.ini",paraId:2,tocIndex:1},{value:"Download offline model bin files, refer to: ",paraId:1,tocIndex:1},{value:"https://huggingface.co/shibing624/text2vec-base-chinese/tree/main",paraId:1,tocIndex:1},{value:", and place the downloaded bin files into the model/text2vec-base-chinese folder",paraId:1,tocIndex:1},{value:"Start the backend service using the flask4modelcache.py script.",paraId:1,tocIndex:1}]},84479:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"From a functional standpoint, to address Huggingface network issues and improve inference speed, local inference capabilities for embeddings have been added. Given some limitations in the SQLAlchemy framework, we have rewritten the relational database interaction module for more flexible database operations. In practice, large model products need to interface with multiple users and models; thus, support for multi-tenancy has been added to ModelCache, as well as preliminary compatibility with system commands and multi-turn conversations.",paraId:0},{value:"Below is a feature comparison table for ModelCache and GPTCache modules:",paraId:1},{value:`

  `,paraId:2},{value:`
  `,paraId:3},{value:`
    `,paraId:3},{value:"Module",paraId:3},{value:`
    `,paraId:3},{value:"Function",paraId:3},{value:`
  `,paraId:3},{value:`
    `,paraId:3},{value:"ModelCache",paraId:3},{value:`
    `,paraId:3},{value:"GPTCache",paraId:3},{value:`
  `,paraId:3},{value:`
  `,paraId:3},{value:`
    `,paraId:3},{value:"Basic Interface",paraId:3},{value:`
    `,paraId:3},{value:"Data query interface",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
  `,paraId:3},{value:`
  `,paraId:3},{value:`
    `,paraId:3},{value:"Data writing interface",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
  `,paraId:3},{value:`
  `,paraId:3},{value:`
    `,paraId:3},{value:"Embedding",paraId:3},{value:`
    `,paraId:3},{value:"Embedding model configuration",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
  `,paraId:3},{value:`
  `,paraId:3},{value:`
    `,paraId:3},{value:"Large model embedding layer",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
    `,paraId:3},{value:`
  `,paraId:3},{value:`
  `,paraId:3},{value:`
    `,paraId:3},{value:"BERT model long text processing",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
    `,paraId:3},{value:`
  `,paraId:3},{value:`
  `,paraId:3},{value:`
    `,paraId:3},{value:"Large model invocation",paraId:3},{value:`
    `,paraId:3},{value:"Decoupling from large models",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
    `,paraId:3},{value:`
  `,paraId:3},{value:`
  `,paraId:3},{value:`
    `,paraId:3},{value:"Local loading of embedding model",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
    `,paraId:3},{value:`
  `,paraId:3},{value:`
  `,paraId:3},{value:`
    `,paraId:3},{value:"Data isolation",paraId:3},{value:`
    `,paraId:3},{value:"Model data isolation",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
  `,paraId:3},{value:`
  `,paraId:3},{value:`
    `,paraId:3},{value:"Hyperparameter isolation",paraId:3},{value:`
    `,paraId:3},{value:`
    `,paraId:3},{value:`
  `,paraId:3},{value:`
  `,paraId:3},{value:`
    `,paraId:3},{value:"Databases",paraId:3},{value:`
    `,paraId:3},{value:"MySQL",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
  `,paraId:3},{value:`
  `,paraId:3},{value:`
    `,paraId:3},{value:"Milvus",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
  `,paraId:3},{value:`
  `,paraId:3},{value:`
    `,paraId:3},{value:"OceanBase",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
    `,paraId:3},{value:`
  `,paraId:3},{value:`
  `,paraId:3},{value:`
    `,paraId:3},{value:"Session management",paraId:3},{value:`
    `,paraId:3},{value:"Single-turn dialogue",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
  `,paraId:3},{value:`
  `,paraId:3},{value:`
    `,paraId:3},{value:"System commands",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
    `,paraId:3},{value:`
  `,paraId:3},{value:`
  `,paraId:3},{value:`
    `,paraId:3},{value:"Multi-turn dialogue",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
    `,paraId:3},{value:`
  `,paraId:3},{value:`
  `,paraId:3},{value:`
    `,paraId:3},{value:"Data management",paraId:3},{value:`
    `,paraId:3},{value:"Data persistence",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
  `,paraId:3},{value:`
  `,paraId:3},{value:`
    `,paraId:3},{value:"One-click cache clearance",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
    `,paraId:3},{value:`
  `,paraId:3},{value:`
  `,paraId:3},{value:`
    `,paraId:3},{value:"Tenant management",paraId:3},{value:`
    `,paraId:3},{value:"Support for multi-tenancy",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
    `,paraId:3},{value:`
  `,paraId:3},{value:`
  `,paraId:3},{value:`
    `,paraId:3},{value:"Milvus multi-collection capability",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
    `,paraId:3},{value:`
  `,paraId:3},{value:`
  `,paraId:3},{value:`
    `,paraId:3},{value:"Other",paraId:3},{value:`
    `,paraId:3},{value:"Long-short dialogue distinction",paraId:3},{value:`
    `,paraId:3},{value:"\u2611 ",paraId:3},{value:`
    `,paraId:3},{value:`
  `,paraId:3},{value:"In ModelCache, the main ideas of GPTCache are carried forward, including a series of core modules: adapter, embedding, similarity, and data_manager. The adapter module's main function is to handle the business logic for various tasks and connect modules like embedding, similarity, and data_manager; the embedding module is responsible for converting text into semantic vector representations, transforming user queries into vectors for recall or storage; the rank module ranks and evaluates the similarity of recalled vectors; the data_manager module manages the database. To better industrialize, we've made architectural and functional upgrades as follows:",paraId:4,tocIndex:0},{value:" Architectural Adjustment (Lightweight Integration): Embedded in large model products in a cache mode similar to Redis, it provides semantic caching capabilities without interfering with LLM invocation and security audits, adaptable toall large model services.",paraId:5,tocIndex:0},{value:" Multiple Model Loading Schemes:",paraId:6,tocIndex:0},{value:"Support for loading local embedding models to resolve Huggingface connectivity issues.",paraId:7,tocIndex:0},{value:"Support for loading various pre-trained model embedding layers.",paraId:7,tocIndex:0},{value:" Data Isolation Capabilities:",paraId:8,tocIndex:0},{value:"Environmental Isolation: Depending on the environment, different database configurations can be pulled to achieve isolation (development, staging, production).",paraId:9,tocIndex:0},{value:"Multi-Tenant Data Isolation: Dynamically create collections according to the model to isolate data, addressing data isolation issues for multiple models/services in large model products.",paraId:9,tocIndex:0},{value:" Support for System Commands: Using concatenation to solve system command issues within the prompt paradigm.",paraId:10,tocIndex:0},{value:" Distinguishing Long and Short Texts: Long texts pose more challenges to similarity assessment, so the differentiation between long and short texts has been enhanced, allowing separate configuration of judgment thresholds.",paraId:11,tocIndex:0},{value:` Performance Optimization for Milvus: Adjusting Milvus's consistency_level to "Session" level for better performance.`,paraId:12,tocIndex:0},{value:" Data Management Capabilities:",paraId:13,tocIndex:0},{value:"One-click cache clearing ability for data management after model upgrades.",paraId:14,tocIndex:0},{value:"Recall hit queries for subsequent data analysis and model iteration reference.",paraId:14,tocIndex:0},{value:"Asynchronous log write-back capability for data analysis and statistics.",paraId:14,tocIndex:0},{value:"Added model fields and data statistics fields for feature expansion.",paraId:14,tocIndex:0},{value:"Future features that will continue to be built upon include:",paraId:14,tocIndex:0},{value:" Data isolation based on hyperparameters.",paraId:15,tocIndex:0},{value:" System prompt partitioned storage capability to improve the accuracy and efficiency of similarity matching.",paraId:16,tocIndex:0},{value:" More versatile embedding models and similarity evaluation algorithms.",paraId:17,tocIndex:0}]},84518:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"ModelCache is easy to use, and you can build a cache testing demo in just one step.",paraId:0},{value:"Download the embedding model bin file from the following address: ",paraId:1,tocIndex:1},{value:"https://huggingface.co/shibing624/text2vec-base-chinese/tree/main",paraId:1,tocIndex:1},{value:". Place the downloaded bin file in the model/text2vec-base-chinese folder.",paraId:1,tocIndex:1},{value:"Start the backend service using the flask4modelcache_dome.py script.",paraId:1,tocIndex:1},{value:`cd CodeFuse-ModelCache
`,paraId:2,tocIndex:1},{value:`python flask4modelcache_demo.py
`,paraId:3,tocIndex:1},{value:"Before starting the service, the following environment configurations should be performed:",paraId:4,tocIndex:2},{value:"Install the relational database MySQL and import the SQL file to create the data tables. The SQL file can be found at: ",paraId:5,tocIndex:2},{value:"reference_doc/create_table.sql",paraId:5,tocIndex:2},{value:"Install the vector database Milvus.",paraId:5,tocIndex:2},{value:`Add the database access information to the configuration files:
`,paraId:5,tocIndex:2},{value:"modelcache/config/milvus_config.ini ",paraId:6,tocIndex:2},{value:"modelcache/config/mysql_config.ini",paraId:6,tocIndex:2},{value:"Download the embedding model bin file from the following address: ",paraId:5,tocIndex:2},{value:"https://huggingface.co/shibing624/text2vec-base-chinese/tree/main",paraId:5,tocIndex:2},{value:". Place the downloaded bin file in the model/text2vec-base-chinese folder.",paraId:5,tocIndex:2},{value:"Start the backend service using the flask4modelcache.py script.",paraId:5,tocIndex:2},{value:"The current service provides three core functionalities through RESTful API.: Cache-Writing, Cache-Querying, and Cache-Clearing. Demos:",paraId:7,tocIndex:3},{value:`import json
import requests
url = 'http://127.0.0.1:5000/modelcache'
type = 'insert'
scope = {"model": "CODEGPT-1008"}
chat_info = [{"query": [{"role": "system", "content": "You are an AI code assistant and you must provide neutral and harmless answers to help users solve code-related problems."}, {"role": "user", "content": "\u4F60\u662F\u8C01?"}],
                  "answer": "Hello, I am an intelligent assistant. How can I assist you?"}]
data = {'type': type, 'scope': scope, 'chat_info': chat_info}
headers = {"Content-Type": "application/json"}
res = requests.post(url, headers=headers, json=json.dumps(data))
`,paraId:8,tocIndex:4},{value:`import json
import requests
url = 'http://127.0.0.1:5000/modelcache'
type = 'query'
scope = {"model": "CODEGPT-1008"}
query = [{"role": "system", "content": "You are an AI code assistant and you must provide neutral and harmless answers to help users solve code-related problems."}, {"role": "user", "content": "Who are you?"}]
data = {'type': type, 'scope': scope, 'query': query}

headers = {"Content-Type": "application/json"}
res = requests.post(url, headers=headers, json=json.dumps(data))
`,paraId:9,tocIndex:5},{value:`import json
import requests
url = 'http://127.0.0.1:5000/modelcache'
type = 'remove'
scope = {"model": "CODEGPT-1008"}
remove_type = 'truncate_by_model'
data = {'type': type, 'scope': scope, 'remove_type': remove_type}

headers = {"Content-Type": "application/json"}
res = requests.post(url, headers=headers, json=json.dumps(data))
`,paraId:10,tocIndex:6}]},28540:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"\u65F6\u95F4",paraId:0},{value:"\u529F\u80FD",paraId:0},{value:"\u7248\u672C\u53F7",paraId:0},{value:"20230430",paraId:0},{value:"Completed GPTCache research, open-source process running through OpenAI interface, single-node form",paraId:0},{value:"\u65E0",paraId:0},{value:"20230509",paraId:0},{value:"1. Completed technology selection and upstream/downstream interaction scheme",paraId:0},{value:"2. Redeveloped database module, replaced SQLAlchemy framework",paraId:0},{value:"3. Refactored llm_handler module, compatible with codegpt, adapted codegpt model parameters \u6570",paraId:0},{value:"V0.1.0",paraId:0},{value:"20230519",paraId:0},{value:"1. Dynamically selected codegpt service mode based on environment",paraId:0},{value:"2. Capability for local model loading and pre-loading",paraId:0},{value:"3. Added dynamic loading capability for local paths based on environment",paraId:0},{value:"V0.1.1",paraId:0},{value:"20230522",paraId:0},{value:"1. Architecture optimized, adjusted to a Redis-like structure, decoupled large model invocation",paraId:0},{value:"2. Switched relational database from SQLite to OceanBase",paraId:0},{value:"3. Switched vector database from FAISS to Milvus",paraId:0},{value:"4. Model data isolation capability",paraId:0},{value:"5. Added core modules adapter_query, adapter_insert",paraId:0},{value:"V0.2.0",paraId:0},{value:"20230531",paraId:0},{value:"1. Online environment launched with dynamic sensing capability",paraId:0},{value:"2. Embedding model evaluation and selection",paraId:0},{value:"3. Added staging environment and data isolation capability",paraId:0},{value:"4. Added exposure capability for the original query field",paraId:0},{value:"V0.2.1",paraId:0},{value:"20230607",paraId:0},{value:"1. Optimized relational database access performance",paraId:0},{value:"2. Optimized environment and model isolation capabilities",paraId:0},{value:"V0.2.2",paraId:0},{value:"20230630",paraId:0},{value:"1. Added large model embedding layer adaptation module in modelCache",paraId:0},{value:"2. Added adoption rate statistical capability",paraId:0},{value:"V0.2.3",paraId:0},{value:"20230730",paraId:0},{value:"1. Added cache statistics feature",paraId:0},{value:"2. Added data deletion function interface",paraId:0},{value:"3. One-click cache clearing capability launched",paraId:0},{value:"4. Developed multi-turn conversation ability, supporting system commands and multi-turn dialogues",paraId:0},{value:"v0.3.0",paraId:0},{value:"20230830",paraId:0},{value:"1. Added asynchronous processing capability, performance improved by over 20%",paraId:0},{value:"2. Architecture change, decoupled embedding inference and business processing logic",paraId:0},{value:"3. Blacklist filtering feature",paraId:0},{value:"V0.3.1",paraId:0}]},19867:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"With the increasing popularity of large-scale software development, the demand for scalable and adaptable static code analysis techniques is growing. Traditional static analysis tools such as Clang Static Analyzer (CSA) or PMD have shown good results in checking programming rules or style issues. However, these tools are often designed for specific objectives and are unable to meet the diverse and changing needs of modern software development environments. These needs may relate to Quality of Service (QoS), various programming languages, different algorithmic requirements, and various performance needs. For example, a security team might need sophisticated algorithms like context-sensitive taint analysis to review smaller codebases, while project managers might need a lighter algorithm, such as one that calculates cyclomatic complexity, to measure developer productivity on larger codebases.",paraId:0,tocIndex:0},{value:"These diversified needs, coupled with the common computational resource constraints in large organizations, pose a significant challenge. Traditional tools, with their problem-specific computation methods, often fail to scale in such environments. This is why we introduced CodeQuery, a centralized data platform specifically designed for large-scale static analysis.",paraId:1,tocIndex:0},{value:`
In implementing CodeQuery, we treat source code and analysis results as data, and the execution process as big data processing, a significant departure from traditional tool-centric approaches. We leverage common systems in large organizations, such as data warehouses, data computation facilities like MaxCompute and Hive, OSS object storage, and flexible computing resources like Kubernetes, allowing CodeQuery to integrate seamlessly into these systems. This approach makes CodeQuery highly maintainable and scalable, capable of supporting diverse needs and effectively addressing changing demands. Furthermore, CodeQuery's open architecture encourages interoperability between various internal systems, facilitating seamless interaction and data exchange. This level of integration and interaction not only increases the degree of automation within the organization but also improves efficiency and reduces the likelihood of manual errors. By breaking down information silos and fostering a more interconnected, automated environment, CodeQuery significantly enhances the overall productivity and efficiency of the software development process.`,paraId:1,tocIndex:0},{value:`
Moreover, CodeQuery's data-centric approach offers unique advantages when addressing domain-specific challenges in static source code analysis. For instance, source code is typically a highly structured and interconnected dataset, with strong informational and relational ties to other code and configuration files. By treating code as data, CodeQuery can adeptly handle these issues, making it especially suitable for use in large organizations where codebases evolve continuously but incrementally, with most code undergoing minor changes daily while remaining stable. CodeQuery also supports use cases like code-data based Business Intelligence (BI), generating reports and dashboards to aid in monitoring and decision-making processes. Additionally, CodeQuery plays an important role in analyzing training data for large language models (LLMs), providing deep insights to enhance the overall effectiveness of these models.`,paraId:1,tocIndex:0},{value:"In the current field of static analysis, CodeQuery introduces a new paradigm. It not only meets the needs of analyzing large, complex codebases but is also adaptable to the ever-changing and diversified scenarios of static analysis. CodeQuery's data-centric approach gives it a unique advantage in dealing with code analysis issues in big data environments. Designed to address static analysis problems in large-scale software development settings, it views both source code and analysis results as data, allowing it to integrate flexibly into various systems within large organizations. This approach not only enables efficient handling of large codebases but can also accommodate various complex analysis needs, thereby making static analysis work more effective and accurate.",paraId:2,tocIndex:0},{value:"The characteristics and advantages of CodeQuery can be summarized as follows:",paraId:3,tocIndex:0},{value:"Highly Scalable",paraId:4,tocIndex:0},{value:": CodeQuery can handle large codebases and adapt to different analysis needs. This high level of scalability makes CodeQuery particularly valuable in large organizations.",paraId:4,tocIndex:0},{value:"Data-Centric",paraId:4,tocIndex:0},{value:": By treating source code and analysis results as data, CodeQuery's data-centric approach gives it a distinct edge in addressing code analysis problems in big data environments.",paraId:4,tocIndex:0},{value:"Highly Integrated",paraId:4,tocIndex:0},{value:": CodeQuery can integrate seamlessly into various systems within large organizations, including data warehouses, data computation facilities, object storage, and flexible computing resources. This high level of integration makes the use of CodeQuery in large organizations more convenient and efficient.",paraId:4,tocIndex:0},{value:"Supports Diverse Needs",paraId:4,tocIndex:0},{value:": CodeQuery can process large codebases and accommodate various complex analysis needs, including QoS analysis, cross-language analysis, algorithmic needs, and performance requirements.",paraId:4,tocIndex:0},{value:"CodeQuery is a powerful static code analysis platform, suitable for large-scale, complex codebase analysis scenarios. Its data-centric approach and high scalability give it a unique advantage in the modern software development environment. As static code analysis technology continues to evolve, CodeQuery is expected to play an increasingly important role in this field.",paraId:5,tocIndex:0}]},44078:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"G\xF6delScript Basic Concepts and Syntax",paraId:0,tocIndex:1},{value:"Introduction",paraId:1,tocIndex:1},{value:"Basic Program Structure",paraId:2,tocIndex:1},{value:"Fundamental Types and Compiler Built-in Functions",paraId:3,tocIndex:1},{value:"Functions",paraId:4,tocIndex:1},{value:"Statements",paraId:5,tocIndex:1},{value:"Schema",paraId:6,tocIndex:1},{value:"Database",paraId:7,tocIndex:1},{value:"Trait",paraId:8,tocIndex:1},{value:"Import",paraId:9,tocIndex:1},{value:"Query",paraId:10,tocIndex:1},{value:"Ungrounded Error: Unassigned/Unbound Error",paraId:11,tocIndex:1},{value:"Query Examples",paraId:12,tocIndex:1},{value:"Java",paraId:13,tocIndex:1},{value:"Python",paraId:14,tocIndex:1},{value:"JavaScript",paraId:15,tocIndex:1},{value:"XML",paraId:16,tocIndex:1},{value:"Go",paraId:17,tocIndex:1},{value:"Query Debugging and Optimization Tips",paraId:18,tocIndex:1},{value:"Schema Arguments Causing Excessively Large Cartesian Products",paraId:19,tocIndex:1},{value:"Multiple Layers of ",paraId:20,tocIndex:1},{value:"for",paraId:20,tocIndex:1},{value:" Causing Excessively Large Cartesian Products",paraId:20,tocIndex:1},{value:"Avoid Misusing ",paraId:21,tocIndex:1},{value:"@inline",paraId:21,tocIndex:1},{value:" and Strategies for Necessary Inline Optimization",paraId:21,tocIndex:1},{value:"Using Query Scripts on a Local Machine",paraId:22,tocIndex:1},{value:`// script
fn hello(greeting: string) -> bool {
    return greeting = "hello world!"
}

fn main() {
    output(hello())
}
`,paraId:23,tocIndex:3},{value:"G\xF6delScript\uFF0C the G\xF6del query language, is a domain-specific language (DSL) for querying and data processing used by CodeQuery. G\xF6delScript uses syntax similar to Rust, providing strict type checking, convenient type inference, and user-friendly error messages, allowing users to get started quickly.",paraId:24,tocIndex:3},{value:"Main use cases for the G\xF6delScript compiler include:",paraId:25,tocIndex:3},{value:"Writing simple or complex queries for users, offering more convenient syntax to improve query writing efficiency.",paraId:26,tocIndex:3},{value:"Providing strict type checking and type inference, offering smarter code modification suggestions.",paraId:26,tocIndex:3},{value:"Offering strict ",paraId:26,tocIndex:3},{value:"ungrounded",paraId:27,tocIndex:3},{value:" detection to avoid triggering the common Souffl\xE9 Ungrounded Error.",paraId:26,tocIndex:3},{value:"Support for Language Server and IDE Extension.",paraId:26,tocIndex:3},{value:"A G\xF6delScript program may include:",paraId:28,tocIndex:5},{value:"Module and symbol import statements",paraId:29,tocIndex:5},{value:"Schema type declarations",paraId:30,tocIndex:5},{value:"Database type declarations",paraId:31,tocIndex:5},{value:"Trait declarations",paraId:32,tocIndex:5},{value:"Method implementations",paraId:33,tocIndex:5},{value:"Function declarations and implementations",paraId:34,tocIndex:5},{value:"Query declarations",paraId:35,tocIndex:5},{value:"An example containing all the above components:",paraId:36,tocIndex:5},{value:`// script
// Package import/symbol import
use coref::java::* // Import all symbols
use coref::java::{JavaDB, Class} // Selective symbol import

// Function declaration
fn default_db() -> JavaDB {
    return JavaDB::load("example.db")
}

// Schema declaration
schema File {
    @primary id: int
}

// Database declaration
database NewDB {
    file: *File
}

// Trait declaration
trait FileTrait {
    fn getId(self) -> int;
}

// Impl trait for
impl FileTrait for File {
    fn getId(self) -> int {
        return self.id
    }
}

// Impl
impl File {
    @data_constraint
    fn all() -> *File {
        yield File {id: 1}
        yield File {id: 2}
    }
}

// Query
query get_all_anno from
    Annotation anno in Annotation(default_db())
select
    anno.id as id
`,paraId:37,tocIndex:5},{value:"G\xF6delScript uses comment syntax similar to C-like languages.",paraId:38,tocIndex:6},{value:`// Single line comment

/*
* 1. Multi-line comment
* 2. Multi-line comment
*/
`,paraId:39,tocIndex:6},{value:"main",paraId:40},{value:"A G\xF6delScript query script can include a ",paraId:41,tocIndex:7},{value:"main",paraId:41,tocIndex:7},{value:" function, which has no return value. If the ",paraId:41,tocIndex:7},{value:"main",paraId:41,tocIndex:7},{value:" function is not implemented and no query declarations are written, the program will not produce any output.",paraId:41,tocIndex:7},{value:"For more details, please refer to ",paraId:42,tocIndex:7},{value:"main function",paraId:43,tocIndex:7},{value:".",paraId:42,tocIndex:7},{value:`fn main() {
    output(query_1())
    output(query_2())
}
`,paraId:44,tocIndex:7},{value:"G\xF6delScript includes basic types ",paraId:45,tocIndex:8},{value:"int",paraId:45,tocIndex:8},{value:", ",paraId:45,tocIndex:8},{value:"string",paraId:45,tocIndex:8},{value:", and ",paraId:45,tocIndex:8},{value:"bool",paraId:45,tocIndex:8},{value:". ",paraId:45,tocIndex:8},{value:"bool",paraId:45,tocIndex:8},{value:" is a basic type but cannot be stored as a value.",paraId:45,tocIndex:8},{value:"int",paraId:40},{value:"Function",paraId:46,tocIndex:9},{value:"Type",paraId:46,tocIndex:9},{value:"Explanation",paraId:46,tocIndex:9},{value:"pow",paraId:46,tocIndex:9},{value:"(int, int) -> int",paraId:46,tocIndex:9},{value:"Exponentiation. Arguments must be non-negative numbers.",paraId:46,tocIndex:9},{value:"rem",paraId:46,tocIndex:9},{value:"(int, int) -> int",paraId:46,tocIndex:9},{value:"Remainder operation.",paraId:46,tocIndex:9},{value:"bitand",paraId:46,tocIndex:9},{value:"(int, int) -> int",paraId:46,tocIndex:9},{value:"Bitwise conjunction.",paraId:46,tocIndex:9},{value:"bitor",paraId:46,tocIndex:9},{value:"(int, int) -> int",paraId:46,tocIndex:9},{value:"Bitwise disjunction.",paraId:46,tocIndex:9},{value:"bitxor",paraId:46,tocIndex:9},{value:"(int, int) -> int",paraId:46,tocIndex:9},{value:"Bitwise exclusive disjunction.",paraId:46,tocIndex:9},{value:"bitnot",paraId:46,tocIndex:9},{value:"(int) -> int",paraId:46,tocIndex:9},{value:"Bitwise negation.",paraId:46,tocIndex:9},{value:"neg",paraId:46,tocIndex:9},{value:"(int) -> int",paraId:46,tocIndex:9},{value:"Arithmetic negation.",paraId:46,tocIndex:9},{value:"to_string",paraId:46,tocIndex:9},{value:"(int) -> string",paraId:46,tocIndex:9},{value:"Conversion to a string.",paraId:46,tocIndex:9},{value:"add",paraId:46,tocIndex:9},{value:"(int, int) -> int",paraId:46,tocIndex:9},{value:"Addition (+).",paraId:46,tocIndex:9},{value:"sub",paraId:46,tocIndex:9},{value:"(int, int) -> int",paraId:46,tocIndex:9},{value:"Subtraction (-).",paraId:46,tocIndex:9},{value:"mul",paraId:46,tocIndex:9},{value:"(int, int) -> int",paraId:46,tocIndex:9},{value:"Multiplication (*).",paraId:46,tocIndex:9},{value:"div",paraId:46,tocIndex:9},{value:"(int, int) -> int",paraId:46,tocIndex:9},{value:"Division (/).",paraId:46,tocIndex:9},{value:"eq",paraId:46,tocIndex:9},{value:"(int, int) -> bool",paraId:46,tocIndex:9},{value:"Equality (=).",paraId:46,tocIndex:9},{value:"ne",paraId:46,tocIndex:9},{value:"(int, int) -> bool",paraId:46,tocIndex:9},{value:"Inequality (!=).",paraId:46,tocIndex:9},{value:"gt",paraId:46,tocIndex:9},{value:"(int, int) -> bool",paraId:46,tocIndex:9},{value:"Greater than (>).",paraId:46,tocIndex:9},{value:"ge",paraId:46,tocIndex:9},{value:"(int, int) -> bool",paraId:46,tocIndex:9},{value:"Greater than or equal to (>=).",paraId:46,tocIndex:9},{value:"lt",paraId:46,tocIndex:9},{value:"(int, int) -> bool",paraId:46,tocIndex:9},{value:"Less than (<).",paraId:46,tocIndex:9},{value:"le",paraId:46,tocIndex:9},{value:"(int, int) -> bool",paraId:46,tocIndex:9},{value:"Less than or equal to (<=).",paraId:46,tocIndex:9},{value:"to_set",paraId:46,tocIndex:9},{value:"(int) -> *int",paraId:46,tocIndex:9},{value:"Cast to a set type.",paraId:46,tocIndex:9},{value:"string",paraId:40},{value:"Function",paraId:47,tocIndex:10},{value:"Type",paraId:47,tocIndex:10},{value:"Explanation",paraId:47,tocIndex:10},{value:"len",paraId:47,tocIndex:10},{value:"(string) -> int",paraId:47,tocIndex:10},{value:"Gets the length of a string.",paraId:47,tocIndex:10},{value:"substr",paraId:47,tocIndex:10},{value:"(string, int, int) -> string",paraId:47,tocIndex:10},{value:"Substring extraction using initial index and length.",paraId:47,tocIndex:10},{value:"contains",paraId:47,tocIndex:10},{value:"(string, string) -> bool",paraId:47,tocIndex:10},{value:"Checks if one string is contained within the current string.",paraId:47,tocIndex:10},{value:"matches",paraId:47,tocIndex:10},{value:"(string, string) -> bool",paraId:47,tocIndex:10},{value:"Checks if a regular expression fully matches the current string.",paraId:47,tocIndex:10},{value:"get_regex_match_result",paraId:47,tocIndex:10},{value:"(string, string, int) -> string",paraId:47,tocIndex:10},{value:'Gets a capture result from a full regex match on the current string, determined by the second parameter (int). For example, "abcdef".get_regex_match_result("a(.*)f", 1) yields "bcde".',paraId:47,tocIndex:10},{value:"to_int",paraId:47,tocIndex:10},{value:"(string) -> int",paraId:47,tocIndex:10},{value:"Converts to an integer.",paraId:47,tocIndex:10},{value:"add",paraId:47,tocIndex:10},{value:"(string, string) -> string",paraId:47,tocIndex:10},{value:"String concatenation.",paraId:47,tocIndex:10},{value:"eq",paraId:47,tocIndex:10},{value:"(string, string) -> bool",paraId:47,tocIndex:10},{value:"Checks string equality.",paraId:47,tocIndex:10},{value:"ne",paraId:47,tocIndex:10},{value:"(string, string) -> bool",paraId:47,tocIndex:10},{value:"Checks string inequality.",paraId:47,tocIndex:10},{value:"to_set",paraId:47,tocIndex:10},{value:"(string) -> *string",paraId:47,tocIndex:10},{value:"Cast to a set type.",paraId:47,tocIndex:10},{value:"bool",paraId:40},{value:"While ",paraId:48,tocIndex:11},{value:"bool",paraId:48,tocIndex:11},{value:" exists as a basic type, it cannot be used as data in intermediate calculations, only as a conditional result.",paraId:48,tocIndex:11},{value:"Function",paraId:49,tocIndex:11},{value:"Type",paraId:49,tocIndex:11},{value:"Explanation",paraId:49,tocIndex:11},{value:"not",paraId:49,tocIndex:11},{value:"(bool) -> bool",paraId:49,tocIndex:11},{value:"Logical negation.",paraId:49,tocIndex:11},{value:"and",paraId:49,tocIndex:11},{value:"(bool, bool) -> bool",paraId:49,tocIndex:11},{value:"Logical conjunction.",paraId:49,tocIndex:11},{value:"or",paraId:49,tocIndex:11},{value:"(bool, bool) -> bool",paraId:49,tocIndex:11},{value:"Logical disjunction.",paraId:49,tocIndex:11},{value:"eq",paraId:49,tocIndex:11},{value:"(bool, bool) -> bool",paraId:49,tocIndex:11},{value:"Equality.",paraId:49,tocIndex:11},{value:"ne",paraId:49,tocIndex:11},{value:"(bool, bool) -> bool",paraId:49,tocIndex:11},{value:"Inequality.",paraId:49,tocIndex:11},{value:"Function",paraId:50,tocIndex:12},{value:"Type",paraId:50,tocIndex:12},{value:"Explanation",paraId:50,tocIndex:12},{value:"len",paraId:50,tocIndex:12},{value:"(*T) -> int",paraId:50,tocIndex:12},{value:"Gets the count of a data set.",paraId:50,tocIndex:12},{value:"max",paraId:50,tocIndex:12},{value:"(*int) -> int",paraId:50,tocIndex:12},{value:"Finds the maximum value.",paraId:50,tocIndex:12},{value:"min",paraId:50,tocIndex:12},{value:"(*int) -> int",paraId:50,tocIndex:12},{value:"Finds the minimum value.",paraId:50,tocIndex:12},{value:"sum",paraId:50,tocIndex:12},{value:"(*int) -> int",paraId:50,tocIndex:12},{value:"Summation of the values.",paraId:50,tocIndex:12},{value:"find",paraId:50,tocIndex:12},{value:"(*T0) -> T1",paraId:50,tocIndex:12},{value:"Finds a data entry from a set using a primary key.",paraId:50,tocIndex:12},{value:"Function",paraId:51,tocIndex:13},{value:"Type",paraId:51,tocIndex:13},{value:"Explanation",paraId:51,tocIndex:13},{value:"output",paraId:51,tocIndex:13},{value:"((...) -> bool) -> ",paraId:51,tocIndex:13},{value:"Outputs query content.",paraId:51,tocIndex:13},{value:"Function",paraId:52,tocIndex:14},{value:"Type",paraId:52,tocIndex:14},{value:"Explanation",paraId:52,tocIndex:14},{value:"load",paraId:52,tocIndex:14},{value:"(string) -> T",paraId:52,tocIndex:14},{value:"Loads the database.",paraId:52,tocIndex:14},{value:"Function",paraId:53,tocIndex:15},{value:"Type",paraId:53,tocIndex:15},{value:"Explanation",paraId:53,tocIndex:15},{value:"to<T>",paraId:53,tocIndex:15},{value:"(self) -> T",paraId:53,tocIndex:15},{value:"Converts to another schema type, using duck typing.",paraId:53,tocIndex:15},{value:"is<T>",paraId:53,tocIndex:15},{value:"(self) -> bool",paraId:53,tocIndex:15},{value:"Determines if it can be another schema type, using duck typing. If the schema has a primary key, the underlying check will only use the primary key to determine compatibility.",paraId:53,tocIndex:15},{value:"key_eq",paraId:53,tocIndex:15},{value:"(self, T) -> bool",paraId:53,tocIndex:15},{value:"Checks if the primary keys of two schema instances are equal.",paraId:53,tocIndex:15},{value:"key_neq",paraId:53,tocIndex:15},{value:"(self, T) -> bool",paraId:53,tocIndex:15},{value:"Checks if the primary keys of two schema instances are ",paraId:53,tocIndex:15},{value:"not",paraId:53,tocIndex:15},{value:" equal.",paraId:53,tocIndex:15},{value:"Schema native function example:",paraId:54,tocIndex:15},{value:`use coref::java::*

fn default_java_db() -> JavaDB {
    return JavaDB::load("coref_java_src.db")
}

fn example() -> bool {
    for(stmt in StatementParent(default_java_db())) {
        if (stmt.is<ElementParent>()) {
            return true
        }
    }
}

fn convert() -> *ElementParent {
    for(stmt in StatementParent(default_java_db())) {
        yield stmt.to<ElementParent>()
    }
}
`,paraId:55,tocIndex:15},{value:"main",paraId:40},{value:"The main function is the only function in G\xF6delScript that does not declare a return type. The main function only allows the use of output, and other statements will result in a compilation error. Using output(...) multiple times can output multiple query results, which will be displayed in separate tables, with the table names corresponding to the names of the query functions called within output.",paraId:56,tocIndex:17},{value:"Query functions are recommended to have a ",paraId:57,tocIndex:18},{value:"bool",paraId:57,tocIndex:18},{value:" return type and need to use ",paraId:57,tocIndex:18},{value:"output()",paraId:57,tocIndex:18},{value:" to output query results.",paraId:57,tocIndex:18},{value:"The query functions called within ",paraId:58,tocIndex:18},{value:"output()",paraId:58,tocIndex:18},{value:" are no longer invoked in the conventional manner of passing arguments to functions. At this point, the parameter list changes to represent the table schema of the output table. Here are two examples of how query functions are applied:",paraId:58,tocIndex:18},{value:"Single-table ",paraId:59,tocIndex:18},{value:"output",paraId:59,tocIndex:18},{value:"A single-table ",paraId:60,tocIndex:18},{value:"output",paraId:60,tocIndex:18},{value:" specifically refers to using ",paraId:60,tocIndex:18},{value:"output",paraId:60,tocIndex:18},{value:" only once within the ",paraId:60,tocIndex:18},{value:"main",paraId:60,tocIndex:18},{value:" function to produce output.",paraId:60,tocIndex:18},{value:`fn example(a: int, b: string) -> bool {...}

fn main() {
    output(example()) // At this point, the parameter list becomes the output table schema and requires no arguments
}
`,paraId:61,tocIndex:18},{value:"The corresponding output table schema would be:",paraId:62,tocIndex:18},{value:`[
  { "a": 0, "b": "xxx" },
  { "a": 1, "b": "xxx" }
]
`,paraId:63,tocIndex:18},{value:"Multi-table ",paraId:64,tocIndex:18},{value:"output",paraId:64,tocIndex:18},{value:"A multi-table ",paraId:65,tocIndex:18},{value:"output",paraId:65,tocIndex:18},{value:" refers to using ",paraId:65,tocIndex:18},{value:"output",paraId:65,tocIndex:18},{value:" multiple times within the ",paraId:65,tocIndex:18},{value:"main",paraId:65,tocIndex:18},{value:" function to produce output. In this case, the output data will include corresponding table names.",paraId:65,tocIndex:18},{value:`fn example0(a: int, b: string) -> bool {...}
fn example1(a: string, b: int) -> bool {...}

fn main() {
    output(example0())
    output(example1())
}
`,paraId:66,tocIndex:18},{value:"The corresponding output table schema would be:",paraId:67,tocIndex:18},{value:`{
  "example0": [
    { "a": 0, "b": "xxx" },
    { "a": 1, "b": "xxx" }
  ],
  "example1": [
    { "a": "xxx", "b": 0 },
    { "a": "xxx", "b": 1 }
  ]
}
`,paraId:68,tocIndex:18},{value:"Below is a more detailed example where we directly construct two sets of data for output. In the following code, note that:",paraId:69,tocIndex:18},{value:"In G\xF6delScript, boolean values can be represented with the keywords ",paraId:70,tocIndex:18},{value:"true",paraId:70,tocIndex:18},{value:" and ",paraId:70,tocIndex:18},{value:"false",paraId:70,tocIndex:18},{value:".",paraId:70,tocIndex:18},{value:"The ",paraId:71,tocIndex:18},{value:"=",paraId:71,tocIndex:18},{value:" symbol in G\xF6delScript is quite special and should not be interpreted in the same way as in conventional programming languages. G\xF6delScript is a Datalog language. Here, the ",paraId:71,tocIndex:18},{value:"=",paraId:71,tocIndex:18},{value:" symbol carries dual semantics: both ",paraId:71,tocIndex:18},{value:"assignment",paraId:71,tocIndex:18},{value:" and ",paraId:71,tocIndex:18},{value:"equality comparison",paraId:71,tocIndex:18},{value:". Details can be found in ",paraId:71,tocIndex:18},{value:"=",paraId:72,tocIndex:18},{value:" operator",paraId:72,tocIndex:18},{value:".",paraId:71,tocIndex:18},{value:"In the conditional statements of this example, both ",paraId:73,tocIndex:18},{value:"a",paraId:73,tocIndex:18},{value:" and ",paraId:73,tocIndex:18},{value:"b",paraId:73,tocIndex:18},{value:" use the assignment semantics of ",paraId:73,tocIndex:18},{value:"=",paraId:73,tocIndex:18},{value:", because the ",paraId:73,tocIndex:18},{value:"int",paraId:73,tocIndex:18},{value:" and ",paraId:73,tocIndex:18},{value:"string",paraId:73,tocIndex:18},{value:" type parameters are considered ",paraId:73,tocIndex:18},{value:"ungrounded (unassigned/unbound)",paraId:73,tocIndex:18},{value:" within the function body and must be assigned before they can be used.",paraId:73,tocIndex:18},{value:"The return value of the ",paraId:74,tocIndex:18},{value:"=",paraId:74,tocIndex:18},{value:" assignment statement is ",paraId:74,tocIndex:18},{value:"true",paraId:74,tocIndex:18},{value:".",paraId:74,tocIndex:18},{value:`fn example(a: int, b: string) -> bool {
    // The = symbol serves both assignment and comparison purposes, depending on whether the left-hand value has been "assigned"
    // Here, the = symbols for a and b are used with assignment semantics
    if (a = 1 && b = "1") {
        // G\xF6delScript uses the keywords true and false to represent boolean values
        return true
    }
    if (a = 2 && b = "2") {
        return true
    }
}

fn main() {
    output(example())
}
`,paraId:75,tocIndex:18},{value:"The expected output should be:",paraId:76,tocIndex:18},{value:`[
  { "a": 1, "b": "1" },
  { "a": 2, "b": "2" }
]
`,paraId:77,tocIndex:18},{value:`Regular functions are used to encapsulate complex processes, and these functions must have a clear return type.
There are two possible return types:`,paraId:78,tocIndex:19},{value:"A single return value, followed by a declaration of the return type after the arrow.",paraId:79,tocIndex:19},{value:`fn getFile(c: Class) -> File {
    return c.getRelativePath()
}
`,paraId:80,tocIndex:19},{value:"A set of return values, the return type after the arrow needs to be prefixed with ",paraId:81,tocIndex:19},{value:"*",paraId:81,tocIndex:19},{value:" to indicate it's a set.",paraId:81,tocIndex:19},{value:`fn getAllFiles(db: JavaDB) -> *File {
    for (f: File in File(db)) {
        yield f
    }
}
`,paraId:82,tocIndex:19},{value:"Generally, ",paraId:83,tocIndex:19},{value:"return",paraId:83,tocIndex:19},{value:" is used for functions with a single return value, while ",paraId:83,tocIndex:19},{value:"yield",paraId:83,tocIndex:19},{value:` is used for functions returning a set.
In practice, since G\xF6delScript uses the Datalog engine underneath, all operations are based on sets; a single return value actually only means that the returned set may contain only one data item, but it could also contain multiple items.`,paraId:83,tocIndex:19},{value:"for",paraId:40},{value:"G\xF6delScript uses the ",paraId:84,tocIndex:21},{value:"for",paraId:84,tocIndex:21},{value:" keyword and syntax similar to loop statements to declare variables from a set:",paraId:84,tocIndex:21},{value:`for(f: File in getAllFiles()) {
    ...
}
`,paraId:85,tocIndex:21},{value:"The type after the colon for ",paraId:86,tocIndex:21},{value:"f: File",paraId:86,tocIndex:21},{value:` can be omitted.
The `,paraId:86,tocIndex:21},{value:"for",paraId:86,tocIndex:21},{value:" statement allows the direct definition of multiple variables, where subsequent variables can use all previously defined variables in the same statement during initialization:",paraId:86,tocIndex:21},{value:`for(a in XmlAttribute(db), b in XmlAttribute(db), c in XmlElement(db)) {
    ...
}

for(a in getAllFiles(), b in a.getAllPaths()) {
    ...
}
`,paraId:87,tocIndex:21},{value:"let",paraId:40},{value:"G\xF6delScript uses the ",paraId:88,tocIndex:22},{value:"let",paraId:88,tocIndex:22},{value:" keyword to declare a single/intermediate variable:",paraId:88,tocIndex:22},{value:`let(f: File = c.getRelativePath()) {
    ...
}
`,paraId:89,tocIndex:22},{value:"The type after the colon for ",paraId:90,tocIndex:22},{value:"f: File",paraId:90,tocIndex:22},{value:` can be omitted.
The `,paraId:90,tocIndex:22},{value:"let",paraId:90,tocIndex:22},{value:" statement allows the direct definition of multiple variables, where subsequent variables can use all previously defined variables in the same statement during initialization:",paraId:90,tocIndex:22},{value:`let(a = 1, b = a + 1, c = b + 1) {
    ...
}
`,paraId:91,tocIndex:22},{value:"if",paraId:40},{value:"Conditional statements in G\xF6delScript are similar to many procedural programming languages:",paraId:92,tocIndex:23},{value:`if (f.getName().contains("util") || f.getName().contains("com")) {
    ...
}
`,paraId:93,tocIndex:23},{value:"Conditions can be connected using logical operators: ",paraId:94,tocIndex:23},{value:"!",paraId:94,tocIndex:23},{value:" for NOT, ",paraId:94,tocIndex:23},{value:"||",paraId:94,tocIndex:23},{value:" for OR, and ",paraId:94,tocIndex:23},{value:"&&",paraId:94,tocIndex:23},{value:" for AND.",paraId:94,tocIndex:23},{value:"Comparative operators in conditions: ",paraId:95,tocIndex:23},{value:">",paraId:95,tocIndex:23},{value:" for greater than, ",paraId:95,tocIndex:23},{value:"<",paraId:95,tocIndex:23},{value:" for less than, ",paraId:95,tocIndex:23},{value:">=",paraId:95,tocIndex:23},{value:" for greater than or equal to, ",paraId:95,tocIndex:23},{value:"<=",paraId:95,tocIndex:23},{value:" for less than or equal to, ",paraId:95,tocIndex:23},{value:"=",paraId:95,tocIndex:23},{value:" for equal to or assignment, ",paraId:95,tocIndex:23},{value:"!=",paraId:95,tocIndex:23},{value:" for not equal to.",paraId:95,tocIndex:23},{value:"Regular arithmetic operations can use the following operators: ",paraId:96,tocIndex:23},{value:"+",paraId:96,tocIndex:23},{value:" for addition, ",paraId:96,tocIndex:23},{value:"-",paraId:96,tocIndex:23},{value:" for subtraction/negation, ",paraId:96,tocIndex:23},{value:"*",paraId:96,tocIndex:23},{value:" for multiplication, ",paraId:96,tocIndex:23},{value:"/",paraId:96,tocIndex:23},{value:" for division.",paraId:96,tocIndex:23},{value:"=",paraId:40},{value:"The ",paraId:97,tocIndex:24},{value:"=",paraId:97,tocIndex:24},{value:" symbol in G\xF6delScript carries two different semantics: assignment and equality comparison. The specific semantics need to be discussed based on the context:",paraId:97,tocIndex:24},{value:"Assignment",paraId:98,tocIndex:24},{value:"Assignment generally occurs with fundamental type variables such as ",paraId:99,tocIndex:24},{value:"int",paraId:99,tocIndex:24},{value:" and ",paraId:99,tocIndex:24},{value:"string",paraId:99,tocIndex:24},{value:". These variables, when used as function parameters, are typically considered unassigned. When a function with such variables is called, the parameters passed in actually serve as filtering conditions.",paraId:99,tocIndex:24},{value:`fn example(a: int) -> bool {
    // This is somewhat counterintuitive; in procedural languages, this is usually taken to mean a == 1
    // However, in Datalog dialects, each function in Datalog is essentially calculating an intermediate table (view)
    // So this function is essentially generating a view with data [{"a": 1}]
    return a = 1 // assign a = 1
}

fn test() -> bool {
    // Although it seems like we are passing a parameter to make a = 2, it's not really the case
    // example() itself returns the view: [{"a": 1}]
    // Then it is constrained by a = 2, and as you can see, we don't get any result here
    // So it returns false
    return example(2) // false
}
`,paraId:100,tocIndex:24},{value:"Equality Comparison",paraId:101,tocIndex:24},{value:"For schema types, since each schema type has a universe behind it, schema type parameters in the parameter list are generally considered to have been assigned. For variables that have already been assigned, ",paraId:102,tocIndex:24},{value:"=",paraId:102,tocIndex:24},{value:" operates as an equality comparison.",paraId:102,tocIndex:24},{value:`// Declare schema
schema A {...}

// Implement schema member functions
impl A {
    // Here we define the universe for schema A
    @data_constraint
    pub fn __all__() -> *A {...}
}

fn example(a: A) -> bool {
    for(temp in A::__all__()) {
        if (a = temp) {
            return true
        }
    }
}
`,paraId:103,tocIndex:24},{value:"Similarly, for internally declared ",paraId:104,tocIndex:24},{value:"int",paraId:104,tocIndex:24},{value:" or ",paraId:104,tocIndex:24},{value:"string",paraId:104,tocIndex:24},{value:" with initial values, ",paraId:104,tocIndex:24},{value:"=",paraId:104,tocIndex:24},{value:" also operates as an equality comparison.",paraId:104,tocIndex:24},{value:`fn example() -> bool {
    let (a = 1) { // assign a = 1
        if (a = 1) { // compare a = 1
            return true
        }
    }
}
`,paraId:105,tocIndex:24},{value:"match",paraId:40},{value:"G\xF6delScript allows writing ",paraId:106,tocIndex:25},{value:"match",paraId:106,tocIndex:25},{value:" statements for ",paraId:106,tocIndex:25},{value:"int",paraId:106,tocIndex:25},{value:" and ",paraId:106,tocIndex:25},{value:"string",paraId:106,tocIndex:25},{value:" types. A ",paraId:106,tocIndex:25},{value:"match",paraId:106,tocIndex:25},{value:" statement is similar to a ",paraId:106,tocIndex:25},{value:"switch",paraId:106,tocIndex:25},{value:" statement with multiple conditional branches, and the conditions in the ",paraId:106,tocIndex:25},{value:"match",paraId:106,tocIndex:25},{value:" must be literals:",paraId:106,tocIndex:25},{value:`match(a) {
    1 => return 0,
    2 => return 1,
    3 => if (a + 1 < 10) {
        return 10
    }
}
`,paraId:107,tocIndex:25},{value:"G\xF6delScript uses ",paraId:108,tocIndex:26},{value:"return",paraId:108,tocIndex:26},{value:" and ",paraId:108,tocIndex:26},{value:"yield",paraId:108,tocIndex:26},{value:". ",paraId:108,tocIndex:26},{value:"return",paraId:108,tocIndex:26},{value:" is for functions with a single return value, and ",paraId:108,tocIndex:26},{value:"yield",paraId:108,tocIndex:26},{value:" is for returning sets.",paraId:108,tocIndex:26},{value:`fn a() -> int {
    return 0
}

fn b() -> *int {
	yield 1
	yield 2
	yield 3
}
`,paraId:109,tocIndex:26},{value:"Schema is a structure for complex data tables in G\xF6delScript.",paraId:110,tocIndex:27},{value:"G\xF6delScript uses the ",paraId:111,tocIndex:28},{value:"schema",paraId:111,tocIndex:28},{value:" keyword to declare a table structure:",paraId:111,tocIndex:28},{value:`schema File {
    id: int,
    name: string
}
`,paraId:112,tocIndex:28},{value:"If a field exists as a primary key in the database, you can use the ",paraId:113,tocIndex:28},{value:"@primary",paraId:113,tocIndex:28},{value:" annotation to indicate that it's a primary key:",paraId:113,tocIndex:28},{value:`schema File {
    @primary id: int,
    name: string
}
`,paraId:114,tocIndex:28},{value:"Table structures with a primary key significantly improve query speed, so try to bind a primary key, preferably of type ",paraId:115,tocIndex:28},{value:"int",paraId:115,tocIndex:28},{value:".",paraId:115,tocIndex:28},{value:"G\xF6delScript declares and implements methods related to ",paraId:116,tocIndex:29},{value:"schema",paraId:116,tocIndex:29},{value:" as follows:",paraId:116,tocIndex:29},{value:`impl File {
    // Static method
    fn f1() -> ... {...}
	// Member method, the first argument must be self
	fn f2(self) -> ... {...}
	...
}
`,paraId:117,tocIndex:29},{value:"Static methods do not require ",paraId:118,tocIndex:30},{value:"self",paraId:118,tocIndex:30},{value:" as the first argument and are straightforward to use: ",paraId:118,tocIndex:30},{value:"ClassName::MethodName(...)",paraId:118,tocIndex:30},{value:".",paraId:118,tocIndex:30},{value:`impl File {
    fn getSchemaName() -> string {
        return "File"
    }
}

fn out(t: string) -> bool {
    if (t = File::getSchemaName()) {
        return true
    }
}
`,paraId:119,tocIndex:30},{value:"The first argument for member methods must be ",paraId:120,tocIndex:31},{value:"self",paraId:120,tocIndex:31},{value:", without specifying its type. These functions are called using ",paraId:120,tocIndex:31},{value:"InstanceName.FunctionName(...)",paraId:120,tocIndex:31},{value:".",paraId:120,tocIndex:31},{value:`impl File {
    fn getName(self) -> string {
        return self.name
    }
}

fn out(path: string) -> bool {
    let (db = JavaDB::load("coref_java_src.db")) {
        for (f in File::__all__(db)) {
            if (path = f.getName()) {
                return true
            }
        }
    }
}
`,paraId:121,tocIndex:31},{value:"fn __all__(db)",paraId:40},{value:"A ",paraId:122,tocIndex:32},{value:"schema",paraId:122,tocIndex:32},{value:" can contain a special ",paraId:122,tocIndex:32},{value:"static method",paraId:122,tocIndex:32},{value:" for loading its dataset from the database.",paraId:122,tocIndex:32},{value:`impl File {
    @data_constraint
    fn __all__(db: JavaDB) -> *File {
        ...
    }
}
`,paraId:123,tocIndex:32},{value:"This method must contain the special annotation ",paraId:124,tocIndex:32},{value:"@data_constraint",paraId:124,tocIndex:32},{value:", indicating that it is specialized for loading. Without this annotation, the method will return an ",paraId:124,tocIndex:32},{value:"empty set",paraId:124,tocIndex:32},{value:". The return type must be a set of itself.",paraId:124,tocIndex:32},{value:"A ",paraId:125,tocIndex:32},{value:"schema",paraId:125,tocIndex:32},{value:" that includes this method can use syntactic sugar to get its full set:",paraId:125,tocIndex:32},{value:`fn out() -> bool {
    for(f in File(JavaDB::load("..."))) {
        ...
    }
    ...
}
// Equivalent to
fn out() -> bool {
    for(f in File::__all__(JavaDB::load("..."))) {
        ...
    }
    ...
}
`,paraId:126,tocIndex:32},{value:"A ",paraId:127,tocIndex:33},{value:"schema",paraId:127,tocIndex:33},{value:" allows using static methods with different names than ",paraId:127,tocIndex:33},{value:"__all__",paraId:127,tocIndex:33},{value:" to indicate that some sets also exist within its full set. This method must also contain the special annotation ",paraId:127,tocIndex:33},{value:"@data_constraint",paraId:127,tocIndex:33},{value:". This method is generally used to manually add some data to the full set of that type.",paraId:127,tocIndex:33},{value:`impl File {
    @data_constraint
    fn extend_example() -> *File {
        yield File {id: 1234567}
    }
}
`,paraId:128,tocIndex:33},{value:"G\xF6delScript allows for the creation of anonymous instances with a specific syntax. The creation of anonymous instances is contingent on the instance existing within the full set of the ",paraId:129,tocIndex:34},{value:"schema",paraId:129,tocIndex:34},{value:", unless this usage appears within a ",paraId:129,tocIndex:34},{value:"@data_constraint",paraId:129,tocIndex:34},{value:" method, in which case the result will be empty.",paraId:129,tocIndex:34},{value:`schema A {
    @primary id: int,
    name: string
}
`,paraId:130,tocIndex:34},{value:"The corresponding syntax to create an anonymous instance is as follows:",paraId:131,tocIndex:34},{value:`A {id: 1, name: "first"}
`,paraId:132,tocIndex:34},{value:"Schema inheritance in G\xF6delScript is very straightforward, exemplified as follows:",paraId:133,tocIndex:35},{value:`schema MyFile extends File {}
`,paraId:134,tocIndex:35},{value:"The subclass will inherit all fields from the parent class by default, so there is no need to manually rewrite them.",paraId:135,tocIndex:36},{value:`schema File {
    @primary id: int,
    name: string
}

schema MyFile extends File {}
`,paraId:136,tocIndex:36},{value:"The subclass will inherit all methods from the parent class by default, except for those marked with ",paraId:137,tocIndex:37},{value:"@data_constraint",paraId:137,tocIndex:37},{value:". There is no need to manually rewrite them. However, the ",paraId:137,tocIndex:37},{value:"__all__",paraId:137,tocIndex:37},{value:" method is special and will not be inherited, so you need to rewrite the ",paraId:137,tocIndex:37},{value:"__all__",paraId:137,tocIndex:37},{value:" method to determine the full set of the inherited schema.",paraId:137,tocIndex:37},{value:`schema File {
    @primary id: int,
    name: string
}

impl File {
    @data_constraint
    fn __all__() -> *File {...}
	fn getId(self) -> int {...}
    fn staticMethod() -> string {return "File"}
}

schema MyFile extends File {}
`,paraId:138,tocIndex:37},{value:"If the subclass implementation contains a method with the same name as the parent class, the parent method will be ",paraId:139,tocIndex:38},{value:"overridden",paraId:139,tocIndex:38},{value:" by the subclass method.",paraId:139,tocIndex:38},{value:`schema File {
    @primary id: int,
    name: string
}

impl File {
    fn staticMethod() -> string {return "File"}
}

schema MyFile extends File {}

impl MyFile {
    fn staticMethod() -> string {return "MyFile"}
}
`,paraId:140,tocIndex:38},{value:"In this case, ",paraId:141,tocIndex:38},{value:"File::staticMethod",paraId:141,tocIndex:38},{value:" is overridden by ",paraId:141,tocIndex:38},{value:"MyFile::staticMethod",paraId:141,tocIndex:38},{value:", so when calling the subclass method, the result obtained will be ",paraId:141,tocIndex:38},{value:'"MyFile"',paraId:141,tocIndex:38},{value:".",paraId:141,tocIndex:38},{value:"The declaration format for databases is as follows:",paraId:142,tocIndex:40},{value:`database DatabaseName {
    // table_name corresponds to the real table name in the db
    // GodelSchemaType corresponds to the schema in which the table data is stored after reading into godel
    table_name : *GodelSchemaType
}
`,paraId:143,tocIndex:40},{value:"Before the colon is the ",paraId:144,tocIndex:40},{value:"real table name",paraId:144,tocIndex:40},{value:" in the loaded database; after the colon is the ",paraId:144,tocIndex:40},{value:"data table format",paraId:144,tocIndex:40},{value:", which must be a ",paraId:144,tocIndex:40},{value:"schema",paraId:144,tocIndex:40},{value:` type.
For example, if a table called `,paraId:144,tocIndex:40},{value:"annotation",paraId:144,tocIndex:40},{value:" exists in the db and corresponds to the ",paraId:144,tocIndex:40},{value:"Annotation",paraId:144,tocIndex:40},{value:" schema, the declaration would be:",paraId:144,tocIndex:40},{value:`database JavaDB {
    // Reads data from the db's annotation table and stores it in Annotation
    annotation : *Annotation
}
`,paraId:145,tocIndex:40},{value:"Additionally, it is necessary to ensure that the ",paraId:146,tocIndex:40},{value:"Annotation",paraId:146,tocIndex:40},{value:" structure matches the table structure. For example:",paraId:146,tocIndex:40},{value:`schema Annotation {
    @primary id: int, // The primary annotation indicates that this field is the primary key; a table can also have no primary key
    content: string
}
`,paraId:147,tocIndex:40},{value:"The ",paraId:148,tocIndex:40},{value:"annotation",paraId:148,tocIndex:40},{value:" table must contain ",paraId:148,tocIndex:40},{value:"id",paraId:148,tocIndex:40},{value:" and ",paraId:148,tocIndex:40},{value:"content",paraId:148,tocIndex:40},{value:" fields with corresponding storage types.",paraId:148,tocIndex:40},{value:"Database types have a static method ",paraId:149,tocIndex:41},{value:"(database)::load(filename: string)",paraId:149,tocIndex:41},{value:`fn loadDatabaseExample() -> bool {
    // The string passed to load is the db's filename, not the path
    // The db's path will be passed as a command-line argument when executing godel
    let (db: JavaDB = JavaDB::load("...")) {
        ...
    }
}
`,paraId:150,tocIndex:41},{value:"In the example above, to access the ",paraId:151,tocIndex:42},{value:"annotation",paraId:151,tocIndex:42},{value:" table:",paraId:151,tocIndex:42},{value:`fn getAnnotation() -> Annotation {
    // The string passed to load is the db's filename, not the path
    // The db's path will be passed as a command-line argument when executing godel
    let (db: JavaDB = JavaDB::load("...")) {
        // Directly use db.field to access the table data
        for (anno: Annotation in db.annotation) {
            ...
        }
    }
}
`,paraId:152,tocIndex:42},{value:"The syntax for declaring a ",paraId:153,tocIndex:44},{value:"trait",paraId:153,tocIndex:44},{value:" is as follows:",paraId:153,tocIndex:44},{value:`trait Example {
    fn getId(self) -> int;
    fn getName(self) -> string;
    fn getValueByName(self, name: string) -> string;
}
`,paraId:154,tocIndex:44},{value:"The syntax is similar to ",paraId:155,tocIndex:45},{value:"impl",paraId:155,tocIndex:45},{value:", but you must implement all the functions declared in the ",paraId:155,tocIndex:45},{value:"trait",paraId:155,tocIndex:45},{value:" to pass compilation.",paraId:155,tocIndex:45},{value:`impl Example for XmlElement {
    fn getId(self) -> int {return self.id}
    fn getName(self) -> int {return self.name}
    fn getValueByName(self, name: string) -> int {
        for(attr in XmlAttribute(XmlDB::load("...")) {
            if (attr.getName() = name && attr.id = self.getAttribute().id) {
                return attr.getValue()
            }
        }
    }
}
`,paraId:156,tocIndex:45},{value:"G\xF6delScript uses the ",paraId:157,tocIndex:46},{value:"use",paraId:157,tocIndex:46},{value:" keyword to import symbols from other files:",paraId:157,tocIndex:46},{value:`use coref::java::* // Import all symbols
use coref::xml::Location // Import a single symbol
use coref::xml::{XmlDB, XmlElement} // Import multiple symbols
`,paraId:158,tocIndex:46},{value:"The G\xF6delScript package manager is enabled when the input parameters include ",paraId:159,tocIndex:47},{value:"-p {package dir path}",paraId:159,tocIndex:47},{value:".",paraId:159,tocIndex:47},{value:"The package manager will parse the folder structure, traversing all ",paraId:160,tocIndex:47},{value:".gdl",paraId:160,tocIndex:47},{value:" files. After obtaining the relative path of the files, it will map the path to the corresponding package path. If the relative path contains ",paraId:160,tocIndex:47},{value:"-",paraId:160,tocIndex:47},{value:", or if a folder name or filename starts with a digit, the path will not be accepted by the package manager, but it will not issue an error and will simply ignore it.",paraId:160,tocIndex:47},{value:"If you want to know which paths were ignored, you can use the ",paraId:161,tocIndex:47},{value:"-v",paraId:161,tocIndex:47},{value:" parameter. With this parameter, the package manager will report the ignored paths as ",paraId:161,tocIndex:47},{value:"warnings",paraId:161,tocIndex:47},{value:". If there are path conflicts in the mapped paths, the package manager will report them as ",paraId:161,tocIndex:47},{value:"errors",paraId:161,tocIndex:47},{value:" and exit the compilation process.",paraId:161,tocIndex:47},{value:`packages:
  coref::cfamily    -> /.../Library/coref.cfamily.gdl
  coref::go         -> /.../Library/coref.go.gdl
  coref::java       -> /.../Library/coref.java.gdl
  coref::javascript -> /.../Library/coref.javascript.gdl
  coref::properties -> /.../Library/coref.properties.gdl
  coref::python     -> /.../Library/coref.python.gdl
  coref::sql        -> /.../Library/coref.sql.gdl
  coref::xml        -> /.../Library/coref.xml.gdl
modules
  +--coref -> coref
     |--xml -> coref::xml
     |--properties -> coref::properties
     |--cfamily -> coref::cfamily
     |--java -> coref::java
     |--javascript -> coref::javascript
     |--go -> coref::go
     |--sql -> coref::sql
     +--python -> coref::python
`,paraId:162,tocIndex:47},{value:`Library
|-- coref.java.gdl
|-- coref.xml.gdl
+-- coref
    |-- go.gdl
    +-- a
        +-- b.gdl
=>
coref::java
coref::xml
coref::go
coref::a::b
`,paraId:163,tocIndex:48},{value:"In this example, there is a path conflict:",paraId:164,tocIndex:48},{value:`Library
|-- coref
|   |-- java.gdl
|   +-- python.gdl
+-- coref.python.gdl
=>
coref::java
coref::python -- \\
                  > Conflict
coref::python -- /
`,paraId:165,tocIndex:48},{value:"In this example, there are invalid characters in the path:",paraId:166,tocIndex:48},{value:`Library
|-- 0123.gdl
|-- my-godel-lib
|   +-- js.gdl
+-- lib-file.123.gdl
=>
0123
^ The first character is a digit
my-godel-lib::js
  ^     ^ Uses the \`-\` character
lib-file::123
   ^      ^ First character after \`.\` is a digit, and the path contains \`-\`
`,paraId:167,tocIndex:48},{value:"In use, it's possible to encounter situations with symbol conflicts. In such cases, direct use of ",paraId:168,tocIndex:49},{value:"File",paraId:168,tocIndex:49},{value:" will result in a symbol conflict, and you need to specify one of the symbols.",paraId:168,tocIndex:49},{value:`use coref::java::Location
use coref::xml::Location
schema MyLoc extends Location {}
                     ^^^^^^^^
Error: "Location" is ambiguous, with multiple symbols
       "coref::java::Location, coref::xml::Location".
`,paraId:169,tocIndex:49},{value:"Like other languages, G\xF6delScript allows specifying a symbol directly through its full path, provided the symbol has been imported.",paraId:170,tocIndex:49},{value:`use coref::java::Location
use coref::xml::Location
schema MyLoc extends coref::xml::Location {}
`,paraId:171,tocIndex:49},{value:"Full path symbols can be used in the following situations:",paraId:172,tocIndex:49},{value:"Schema inheritance",paraId:173,tocIndex:49},{value:`schema JavaLocation extends coref::java::Location {}
`,paraId:174,tocIndex:49},{value:"Function parameters and return values",paraId:175,tocIndex:49},{value:`fn return_java_file(f: coref::java::File) -> coref::java::File {
    ...
}
`,paraId:176,tocIndex:49},{value:"Database declarations",paraId:177,tocIndex:49},{value:`database MyDB {
    java_file: coref::java::File,
    xml_file: coref::xml::File,
    java_loc: coref::java::Location,
    xml_loc: coref::xml::Location
}
`,paraId:178,tocIndex:49},{value:"Query list type declarations",paraId:179,tocIndex:49},{value:`query example from
	coref::java::Location loc in coref::java::Location(coref::java::JavaDB::load("..."))
where
	...
select
	...
`,paraId:180,tocIndex:49},{value:"Schema static method calls",paraId:181,tocIndex:49},{value:`for(loc in coref::java::Location(coref::java::JavaDB::load("..."))) {
    ...
}

stmt.to<coref::java::ElementParent>()
stmt.is<coref::java::ElementParent>()
`,paraId:182,tocIndex:49},{value:"Query is used for simple queries and is guaranteed to be output even without declaring a ",paraId:183,tocIndex:50},{value:"main",paraId:183,tocIndex:50},{value:" function. The syntax format for query is as follows:",paraId:183,tocIndex:50},{value:`query name from
	variable in initial value,
    variable in initial value,
    variable in initial value
where condition
select value as output column name
	value as output column name,
    value as output column name,
    value as output column name
`,paraId:184,tocIndex:50},{value:"Variable declarations in the ",paraId:185,tocIndex:50},{value:"from",paraId:185,tocIndex:50},{value:" list do not need type annotations, as the compiler will automatically infer them. Additionally, the ",paraId:185,tocIndex:50},{value:"select",paraId:185,tocIndex:50},{value:" list does not use ",paraId:185,tocIndex:50},{value:"=",paraId:185,tocIndex:50},{value:" but the ",paraId:185,tocIndex:50},{value:"in",paraId:185,tocIndex:50},{value:" keyword. Also, in the ",paraId:185,tocIndex:50},{value:"select",paraId:185,tocIndex:50},{value:" list, the output column name cannot conflict with the calculation variables, but the column name can be omitted. Omitted column names will take random names in the output results, so it's best not to omit them.",paraId:185,tocIndex:50},{value:"Here is a ",paraId:186,tocIndex:50},{value:"hello world",paraId:186,tocIndex:50},{value:" written in query syntax:",paraId:186,tocIndex:50},{value:`query hello_world from
	info in "hello world"
select info as greeting
`,paraId:187,tocIndex:50},{value:"The code above is equivalent to the following code:",paraId:188,tocIndex:50},{value:`fn hello_world(greeting: string) -> bool {
    let (info = "hello world") {
        if (greeting = info) {
            return true
        }
    }
}
fn main() {
    output(hello_world())
}
`,paraId:189,tocIndex:50},{value:"Query includes a query name, a ",paraId:190,tocIndex:51},{value:"from",paraId:190,tocIndex:51},{value:" list, a ",paraId:190,tocIndex:51},{value:"where",paraId:190,tocIndex:51},{value:" filter condition, and a ",paraId:190,tocIndex:51},{value:"select",paraId:190,tocIndex:51},{value:" list.",paraId:190,tocIndex:51},{value:`// script
use coref::java::{Callable, Class, Interface, JavaDB}

fn db() -> JavaDB {
    return JavaDB::load("coref_java_src.db")
}

query class_method from
    Callable m in Callable(db()),
    Class c in Class(db())
where
    c.id = m.getBelongedClass().id
select
    c.getQualifiedName() as className,
    m.getName() as methodName,
    m.getSignature() as methodSignature
`,paraId:191,tocIndex:51},{value:"The example above is equivalent to the following code:",paraId:192,tocIndex:52},{value:`// script
use coref::java::{Callable, Class, Interface, JavaDB}

fn db() -> JavaDB {
  return JavaDB::load("coref_java_src.db")
}

fn main() {
  output(class_method())
}

fn class_method(className: string, methodName: string, methodSignature: string) -> bool {
  for (m in Callable(db()), c in Class(db())) {
    if (c.id = m.getBelongedClass().id) {
      if (className = c.getQualifiedName() &&
          methodName = m.getName() &&
          methodSignature = m.getSignature()) {
        return true
      }
    }
  }
}
`,paraId:193,tocIndex:52},{value:"G\xF6delScript will determine symbols that are not bound to a set as ",paraId:194,tocIndex:53},{value:"ungrounded",paraId:194,tocIndex:53},{value:". The basic rule of judgment is:",paraId:194,tocIndex:53},{value:`Uninitialized/unusued/unbound symbols
`,paraId:195,tocIndex:53},{value:"Unbound ",paraId:196,tocIndex:53},{value:"int",paraId:196,tocIndex:53},{value:", ",paraId:196,tocIndex:53},{value:"string",paraId:196,tocIndex:53},{value:" arguments",paraId:196,tocIndex:53},{value:"Unused database type arguments",paraId:196,tocIndex:53},{value:"Function body has statements, but no return statements",paraId:196,tocIndex:53},{value:`Symbols bound within negation blocks
`,paraId:195,tocIndex:53},{value:"For example, ",paraId:197,tocIndex:53},{value:"!(__tmp = 1)",paraId:197,tocIndex:53},{value:", ",paraId:197,tocIndex:53},{value:"__tmp",paraId:197,tocIndex:53},{value:" is considered unbound",paraId:197,tocIndex:53},{value:"Calling inline functions or data constructors in negation blocks",paraId:197,tocIndex:53},{value:"In the function block, if there is a branch that does not use database or basic type parameters, it will inevitably lead to ",paraId:198,tocIndex:54},{value:"ungrounded",paraId:198,tocIndex:54},{value:":",paraId:198,tocIndex:54},{value:`fn test(db: JavaDB, a: int, b: string) -> bool {}
        ^^          ^       ^                  ^^
Error: ungrounded parameter "db, a, b" in this branch.
`,paraId:199,tocIndex:54},{value:"The compiler will indicate in which branch there is an unused parameter. Check the corresponding execution path and complete the parameter constraints based on the prompt.",paraId:200,tocIndex:54},{value:"If some functions have basic type parameters but always use literals when called, and if ",paraId:201,tocIndex:54},{value:"ungrounded",paraId:201,tocIndex:54},{value:" is incorrectly reported, you can add an ",paraId:201,tocIndex:54},{value:"@inline",paraId:201,tocIndex:54},{value:" annotation to the function to avoid incorrect constraint checks.",paraId:201,tocIndex:54},{value:`impl XXX {
    @inline
    fn getValueByAttributeNameByDefaultValue(self, attributeName: string) -> string {
        if (self.hasAttribute(attributeName)) {
            return self.getValueByAttributeName(attributeName)
        }
        if (!self.hasAttribute(attributeName) {
            return "null"
        }
    }
}

fn xxx() -> xx {
    ..
    attr.getValueByAttributeNameByDefaultValue("pattern")
                                               ^^^^^^^^^ Use literals, add @inline to pass the check
}
`,paraId:202,tocIndex:54},{value:"G\xF6delScript allows an empty function body without any statements. However, if there are other statements in the function body, G\xF6delScript requires at least one return statement, otherwise an ",paraId:203,tocIndex:55},{value:"ungrounded",paraId:203,tocIndex:55},{value:" error will occur.",paraId:203,tocIndex:55},{value:`fn test() -> int {}
                  ^^ No statements, passes compilation

fn test() -> int {
    let (a = 1) {}
    ^^^^^^^^^^^^^^ Statements present, no return statement, ungrounded
}
`,paraId:204,tocIndex:55},{value:"As mentioned above, ",paraId:205,tocIndex:56},{value:"@inline",paraId:205,tocIndex:56},{value:" annotation can be used to circumvent ",paraId:205,tocIndex:56},{value:"ungrounded",paraId:205,tocIndex:56},{value:" errors. However, if inline functions are used in negation blocks, it will inevitably result in ",paraId:205,tocIndex:56},{value:"ungrounded",paraId:205,tocIndex:56},{value:" errors.",paraId:205,tocIndex:56},{value:"Similarly, data constructors are used to bind temporary intermediate variables, but this will directly result in ",paraId:206,tocIndex:56},{value:"ungrounded",paraId:206,tocIndex:56},{value:` errors.
Therefore, using inline functions or data constructors in negation blocks will inevitably lead to `,paraId:206,tocIndex:56},{value:"ungrounded",paraId:206,tocIndex:56},{value:" errors, and the compiler will report errors for all such cases.",paraId:206,tocIndex:56},{value:`if (!check(method.to<ElementParent>())) {
           ^^^^^^^^^^^^^^^^^^^^^^^^^^ ungrounded
}
if (!check(ElementParent {id: 0})) {
           ^^^^^^^^^^^^^^ ungrounded
}

@inline
fn for_test() -> ElementParent {
    ...
}
if (!check(for_test())) {
           ^^^^^^^^^^ Negation block contains inline function, ungrounded
}
`,paraId:207,tocIndex:56},{value:"G\xF6delScript does not perform ",paraId:208,tocIndex:57},{value:"ungrounded",paraId:208,tocIndex:57},{value:" checks for negation of chained calls, but this writing will cause an ",paraId:208,tocIndex:57},{value:"ungrounded",paraId:208,tocIndex:57},{value:" error in Souffl\xE9:",paraId:208,tocIndex:57},{value:`use coref::java::*

fn default_java_db() -> JavaDB {
    return JavaDB::load("coref_java_src.db")
}

fn get_field() -> *Field {
    for (field in Field(default_java_db())) {
        if (!field.getLocation().getFile().getRelativePath().contains("/test/")) {
            yield field
        }
    }
}
`,paraId:209,tocIndex:57},{value:"Where:",paraId:210,tocIndex:57},{value:`!field.getLocation().getFile().getRelativePath().contains("/test/")
`,paraId:211,tocIndex:57},{value:"It will be translated to a Souffl\xE9 code fragment like this:",paraId:212,tocIndex:57},{value:`!(__tmp = field, Field_getLocation(__tmp, __tmp_1), ..., contains("/test/", __tmp_4))
  ^^^^^                                   ^^^^^^^
`,paraId:213,tocIndex:57},{value:"The variables used for intermediate storage being bound in ",paraId:214,tocIndex:57},{value:"!(...)",paraId:214,tocIndex:57},{value:" but due to the negation operator, this binding is considered hypothetical. However, ",paraId:214,tocIndex:57},{value:"__tmp",paraId:214,tocIndex:57},{value:", ",paraId:214,tocIndex:57},{value:"__tmp_1",paraId:214,tocIndex:57},{value:" are then considered to be variables declared for the entire statement scope, leading to ",paraId:214,tocIndex:57},{value:"ungrounded",paraId:214,tocIndex:57},{value:".",paraId:214,tocIndex:57},{value:"This can be avoided by declaring intermediate variables to catch intermediate results in a negation operation:",paraId:215,tocIndex:57},{value:`fn get_field() -> *Field {
    for (field in Field(default_java_db())) {
        let (path = field.getLocation().getFile().getRelativePath()) {
            if (!path.contains("/test/")) {
                yield field
            }
        }
    }
}
`,paraId:216,tocIndex:57},{value:`// script
use coref::java::*

fn default_java_db() -> JavaDB {
    return JavaDB::load("coref_java_src.db")
}

// find unused methods
fn unused_method(unused: string) -> bool {
    for(c in Callable(default_java_db()), method in Callable(default_java_db()), caller in method.getCaller()) {
        if (c != caller && unused = method.getSignature()) {
            return true
        }
    }
}

fn main() {
    output(unused_method())
}
`,paraId:217,tocIndex:60},{value:`// script
use coref::java::*

fn default_java_db() -> JavaDB {
	return JavaDB::load("coref_java_src.db")
}

/**
 * Find all class and the inheritances
 * including parent class inheritance and ancestor class inheritance
 */
fn class_hierarchy(className : string, superClassName : string) -> bool {
    for (c in Class(default_java_db()), ancestor in c.getAnAncestorClass()) {
        if (className = c.getQualifiedName() &&
            superClassName = ancestor.getQualifiedName()) {
            return true
        }
    }
}

fn main() {
	output(class_hierarchy())
}
`,paraId:218,tocIndex:61},{value:`// script
use coref::java::*

fn default_java_db() -> JavaDB {
	return JavaDB::load("coref_java_src.db")
}

// Find all methods of the class
fn methods(className : string, methodName : string) -> bool {
    for (c in Class(default_java_db()), m in c.getAllMethods()) {
        if (className = c.getQualifiedName() &&
            methodName = m.getName()){
            return true
        }
    }
}

fn main() {
	output(methods())
}
`,paraId:219,tocIndex:62},{value:`// script
use coref::python::*

fn default_db() -> PythonDB {
    return PythonDB::load("coref_python_src.db")
}

/**
 * Get cyclomatic complexity of functions
 *
 * @param name   function name
 * @param value  cyclomatic complexity of function
 * @param path   path of file including this function
 * @param sline  function start line
 * @param eline  function end line
 */
fn getCyclomaticComplexity(
    name: string,
    value: int,
    path: string,
    sline: int,
    eline: int) -> bool {
    // get metric function
    for (c in MetricFunction(default_db())) {
        if (path = c.getLocation().getFile().getRelativePath() &&
            name = c.getQualifiedName() &&
            value = c.getCyclomaticComplexity() &&
            sline = c.getLocation().getStartLineNumber() &&
            eline = c.getLocation().getEndLineNumber()) {
            return true
        }
    }
}

fn main() {
    output(getCyclomaticComplexity())
}
`,paraId:220,tocIndex:64},{value:`// script
use coref::python::*

schema PublicVisitedElement extends CombineElement {}

impl PublicVisitedElement {
    @data_constraint
    pub fn __all__(db: PythonDB) -> *PublicVisitedElement {
        for (tmp in Class(db)) {
            yield PublicVisitedElement {id: tmp.element_oid}
        }
        for (tmp in Function(db)) {
            yield PublicVisitedElement {id: tmp.element_oid}
        }
    }
}

fn default_db() -> PythonDB {
    return PythonDB::load("coref_python_src.db")
}


// count number of total public element
fn countTotalPublicElement() -> int {
    return PublicVisitedElement(default_db()).len()
}

// get public elements with Docstring comment
fn withDocstringCommentElement() -> *PublicVisitedElement {
    let (db = default_db()) {
        for (e in PublicVisitedElement(db), j in DocstringComment(db)) {
            if (e.key_eq(j.getDocumentableElement())) {
                yield e
            }
        }
    }
}

// count number of public elements with Docstring comment
fn countTotalPublicDocumentedElement() -> int {
    return withDocstringCommentElement().len()
}

fn withPublicDocumentedBelowElement() -> *PublicVisitedElement {
    let (db = default_db()) {
        for (e in PublicVisitedElement(db), j in Comment(db)) {
            if (e.key_eq(j.getDocumentedClassOrFunctionElement())) {
                yield e
            }
        }
    }
}

// count number of public element with single line comment
fn countTotalPublicDocumentedBelowElement() -> int {
    return withPublicDocumentedBelowElement().len()
}


// calculate documented percentage
fn getDocumentedPercentage(documentedPercentage: int) -> bool {
    let (i = countTotalPublicElement(),
        j = countTotalPublicDocumentedElement(),
        k = countTotalPublicDocumentedBelowElement()) {
        if (i = 0) {
            if (documentedPercentage = -1) {
                return true
            }
        }
        if (i != 0) {
            if (documentedPercentage = (j + k) * 1000 / i) {
                return true
            }
        }
    }
}

fn main() {
    output(getDocumentedPercentage())
}
`,paraId:221,tocIndex:65},{value:`// script
use coref::python::*

schema PublicVisitedElement extends CombineElement {}

impl PublicVisitedElement {
    @data_constraint
    pub fn __all__(db: PythonDB) -> *PublicVisitedElement {
        for (tmp in Class(db)) {
            yield PublicVisitedElement {id: tmp.element_oid}
        }
        for (tmp in Function(db)) {
            yield PublicVisitedElement {id: tmp.element_oid}
        }
    }

    pub fn getName(self) -> string {
        let (tmp = Class(__all_data__).find(self)) {
            return tmp.getQualifiedName()
        }
        let (tmp = Function(__all_data__).find(self)) {
            return tmp.getQualifiedName()
        }
    }
}

fn default_db() -> PythonDB {
    return PythonDB::load("coref_python_src.db")
}

fn hasComment(e: PublicVisitedElement) -> bool {
    let (db = default_db()) {
        for (j in DocstringComment(db)) {
            if (e.key_eq(j.getDocumentableElement())) {
                return true
            }
        }
        for (j in Comment(db)) {
            if (e.key_eq(j.getDocumentedClassOrFunctionElement())) {
                return true
            }
        }
    }
}

/**
 * Get comment of each public element
 *
 * @param type          public visited element type
 * @param name          public visited element name
 * @param filePath      file path
 * @param sline         element start line
 * @param eline         element end line
 * @param isCommented   if is commented
 */
fn output_result(
    type: string,
    name: string,
    filePath: string,
    sline: int,
    eline: int,
    isCommented: int) -> bool {
    for (e in PublicVisitedElement(default_db())) {
        if (type = e.getType() &&
            name = e.getName() &&
            filePath = e.getLocation().getFile().getRelativePath() &&
            sline = e.getLocation().getStartLineNumber() &&
            eline = e.getLocation().getEndLineNumber()) {
            if (hasComment(e)) {
                if (isCommented = 1) {
                    return true
                }
            }
            if (!hasComment(e)) {
                if (isCommented = 0) {
                    return true
                }
            }
        }
    }
}

fn main() {
    output(output_result())
}
`,paraId:222,tocIndex:66},{value:`// script
use coref::javascript::*

/**
 * print AST
 *
 * @param filePath          file path
 * @param parentId          parent node ID
 * @param parentKind        parent node kind
 * @param parentStartLine   parent node start line
 * @param parentEndLine     parent node end line
 * @param childId           child node ID
 * @param childKind         child node kind
 * @param childStartLine    child node start line
 * @param childEndLine      child node end line
 * @param index             child node index
 */
fn out(
    filePath: string,
    parentId: int,
    parentKind: string,
    parentStartLine: int,
    parentEndLine: int,
    childId: int,
    childKind: string,
    childStartLine: int,
    childEndLine: int,
    index: int
) -> bool {
    let (db = JavascriptDB::load("coref_javascript_src.db")) {
        for (parent in Node(db),
            child in Node(db),
            parentSyntaxKind in SyntaxKind(),
            childSyntaxKind in SyntaxKind(),
            parentLocation in Location(db),
            childLocation in Location(db),
            file in File(db)) {
            if (parent.key_eq(child.getParent()) &&
                parentId = parent.id &&
                childId = child.id &&
                parentSyntaxKind.id = parent.getKind() &&
                childSyntaxKind.id = child.getKind() &&
                parentKind = parentSyntaxKind.getName() &&
                childKind = childSyntaxKind.getName() &&
                index = child.getIndex() &&
                parentLocation = parent.getLocation() &&
                childLocation = parent.getLocation() &&
                file = parentLocation.getFile() &&
                filePath = file.getRelativePath() &&
                parentStartLine = parentLocation.getStartLineNumber() &&
                parentEndLine = parentLocation.getEndLineNumber() &&
                childStartLine = childLocation.getStartLineNumber() &&
                childEndLine = childLocation.getEndLineNumber()) {
                return true
            }
        }
    }
}

fn main() {
    output(out())
}
`,paraId:223,tocIndex:68},{value:`// script
use coref::javascript::*

fn default_db() -> JavascriptDB {
    return JavascriptDB::load("coref_javascript_src.db")
}

/**
 * Output the cyclomatic complexity of each function
 *
 * @param filePath      file path
 * @param functionName  function name
 * @param complexity    cyclomatic complexity
 * @param startLine     function start line
 * @param endLine       function end line
 */
fn out(filePath: string, functionName: string, complexity: int, startLine: int, endLine: int) -> bool {
    let (db = default_db()) {
        for (func in FunctionLikeDeclaration(db), file in File(db)) {
            if (complexity = func.getCyclomaticComplexity() &&
                functionName = func.getName() &&
                file = func.getLocation().getFile() &&
                filePath = file.getRelativePath() &&
                startLine = func.getLocation().getStartLineNumber() &&
                endLine = func.getLocation().getEndLineNumber()) {
                return true
            }
        }
    }
}

fn main() {
    output(out())
}
`,paraId:224,tocIndex:69},{value:`// script
use coref::javascript::*

fn default_db() -> JavascriptDB {
    return JavascriptDB::load("coref_javascript_src.db")
}

fn getACallerFunction(function: FunctionLikeDeclaration, callerFunction: FunctionLikeDeclaration) -> bool {
    for (mayInvokeExpression in MayInvokeExpression(default_db())) {
        if (mayInvokeExpression = function.getACallSite() &&
            callerFunction = mayInvokeExpression.getEnclosingFunction()) {
            return true
        }
    }
}

fn getAnEffectedFunction(function: FunctionLikeDeclaration, effectedFunction: FunctionLikeDeclaration) -> bool {
    if (getACallerFunction(function, effectedFunction)) {
        return true
    }
    for (callerFunction in FunctionLikeDeclaration(default_db())) {
        if (getACallerFunction(function, callerFunction) &&
            getAnEffectedFunction(callerFunction, effectedFunction)) {
            return true
        }
    }
}

/**
 * Query the effected functions according to the changed lines.
 *
 * @param function              the changed function id
 * @param signature             the changed function signature
 * @param functionPath          the changed function file path
 * @param startLine             the changed function start line
 * @param endLine               the changed function end line
 * @param effectedFunction      the effected function id
 * @param effectedSignature     the effected function signature
 * @param effectedFunctionPath  the effected function file path
 * @param effectedStartLine     the effected function start line
 * @param effectedEndLine       the effected function end line
 */
fn out(
    function: FunctionLikeDeclaration,
    signature: string,
    functionPath: string,
    startLine: int,
    endLine: int,
    effectedFunction: FunctionLikeDeclaration,
    effectedSignature: string,
    effectedFunctionPath: string,
    effectedStartLine: int,
    effectedEndLine: int
) -> bool {
    if (getAnEffectedFunction(function, effectedFunction)) {
        let (symbol = function.getSymbol(),
            effectedSymbol = effectedFunction.getSymbol(),
            location = function.getLocation(),
            effectedLocation = effectedFunction.getLocation()) {
            if (signature = symbol.getDescription() &&
                effectedSignature = effectedSymbol.getDescription() &&
                functionPath = location.getRelativePath() &&
                startLine = location.getStartLineNumber() &&
                endLine = location.getEndLineNumber() &&
                effectedFunctionPath = effectedLocation.getRelativePath() &&
                effectedStartLine = effectedLocation.getStartLineNumber() &&
                effectedEndLine = effectedLocation.getEndLineNumber()) {
                return true
            }
        }
    }
}

fn main() {
    output(out())
}
`,paraId:225,tocIndex:70},{value:`// script
use coref::xml::*

schema BeanXmlElement extends XmlElement {}

impl BeanXmlElement {
    @data_constraint
    pub fn __all__(db: XmlDB) -> *BeanXmlElement {
        for (e in XmlElement(db)) {
            let (path = e.getLocation().getFile().getRelativePath()) {
                if (!path.contains("target") && e.getName() = "bean") {
                    yield BeanXmlElement {
                        id: e.id,
                        location_id: e.location_id,
                        parent_id: e.parent_id,
                        index_order: e.index_order
                    }
                }
            }
        }
    }
}

schema EntryXmlElement extends XmlElement {}

impl EntryXmlElement {
    @data_constraint
    pub fn __all__(db: XmlDB) -> *EntryXmlElement {
        for (e in XmlElement(db)) {
            if (e.getName() = "entry") {
                yield EntryXmlElement {
                    id: e.id,
                    location_id: e.location_id,
                    parent_id: e.parent_id,
                    index_order: e.index_order
                }
            }
        }
    }
}

schema PropertyXmlElement extends XmlElement {}

impl PropertyXmlElement {
    @data_constraint
    pub fn __all__(db: XmlDB) -> *PropertyXmlElement {
        for (e in XmlElement(db)) {
            if (e.getName() = "property") {
                yield PropertyXmlElement {
                    id: e.id,
                    location_id: e.location_id,
                    parent_id: e.parent_id,
                    index_order: e.index_order
                }
            }
        }
    }
}

fn default_db() -> XmlDB {
    return XmlDB::load("coref_xml_src.db")
}

// get class name
fn getClassName(bean: BeanXmlElement) -> string {
    for (attr in bean.getAttribute()) {
        if (attr.getName() = "class") {
            return attr.getValue()
        }
    }
}

// get key
fn getKey(e: EntryXmlElement) -> string {
    for (attr in e.getAttribute()) {
        if (attr.getName() = "key") {
            return attr.getValue()
        }
    }
}

// output value and class info of the bean
fn output1(className: string, pName: string, kName: string) -> bool {
    let (db = default_db()) {
        for (bean in BeanXmlElement(db), p in PropertyXmlElement(db), e in EntryXmlElement(db)) {
            if (className = getClassName(bean) &&
                bean.key_eq(p.getParent()) &&
                p.key_eq(e.getParent().getParent()) &&
                pName = p.getName() &&
                kName = getKey(e)) {
                return true
            }
        }
    }
}

fn main() {
    output(output1())
}
`,paraId:226,tocIndex:72},{value:`// script
use coref::xml::*

schema DependencyElement extends XmlElement {}

impl DependencyElement {
    @data_constraint
    pub fn __all__(db: XmlDB) -> *DependencyElement {
        for(e in XmlElement(db)) {
            if (e.getElementName() = "dependency") {
                yield DependencyElement {
                    id: e.id,
                    location_id: e.location_id,
                    parent_id: e.parent_id,
                    index_order: e.index_order
                }
            }
        }
    }
}

schema GroupElement extends XmlElement {}

impl GroupElement {
    @data_constraint
    pub fn __all__(db: XmlDB) -> *GroupElement {
        for(e in XmlElement(db)) {
            if (e.getElementName() = "groupId") {
                yield GroupElement {
                    id: e.id,
                    location_id: e.location_id,
                    parent_id: e.parent_id,
                    index_order: e.index_order
                }
            }
        }
    }
}

schema VersionElement extends XmlElement {}

impl VersionElement {
    @data_constraint
    pub fn __all__(db: XmlDB) -> *VersionElement {
        for(e in XmlElement(db)) {
            if (e.getElementName() = "version") {
                yield VersionElement {
                    id: e.id,
                    location_id: e.location_id,
                    parent_id: e.parent_id,
                    index_order: e.index_order
                }
            }
        }
    }
}

schema ArtifactElement extends XmlElement {}

impl ArtifactElement {
    @data_constraint
    pub fn __all__(db: XmlDB) -> *ArtifactElement {
        for(e in XmlElement(db)) {
            if (e.getElementName() = "artifactId") {
                yield ArtifactElement {
                    id: e.id,
                    location_id: e.location_id,
                    parent_id: e.parent_id,
                    index_order: e.index_order
                }
            }
        }
    }
}

schema PomFile extends XmlFile {}

impl PomFile {
    @data_constraint
    pub fn __all__(db: XmlDB) -> *PomFile {
        for(f in XmlFile(db)) {
            if (f.getFileName() = "pom.xml") {
                yield PomFile {
                    id: f.id,
                    file_name: f.file_name,
                    relative_path: f.relative_path
                }
            }
        }
    }
}

// output relative path of the file, referenced jar name and version
fn out(fileName: string, m1: string, m2: string, m3: string) -> bool {
    let (db = XmlDB::load("coref_xml_src.db")) {
        for (f in PomFile(db),
            e1 in GroupElement(db),
            e2 in VersionElement(db),
            e3 in ArtifactElement(db),
            c1 in XmlCharacter(db),
            c2 in XmlCharacter(db),
            c3 in XmlCharacter(db),
            p in DependencyElement(db)) {
            if (f.key_eq(p.getLocation().getFile()) &&
                fileName = f.getRelativePath() &&
                p.key_eq(e1.getParent()) &&
                e1.key_eq(c1.getBelongedElement()) &&
                m1 = c1.getText() &&
                p.key_eq(e2.getParent()) &&
                e2.key_eq(c2.getBelongedElement()) &&
                m2 = c2.getText() &&
                p.key_eq(e3.getParent()) &&
                e3.key_eq(c3.getBelongedElement()) &&
                m3 = c3.getText()) {
                return true
            }
        }
    }
}

fn main() {
    output(out())
}
`,paraId:227,tocIndex:73},{value:`// script
use coref::xml::*

// select XmlElement containing "mobileService"
schema MobileServiceXmlElement extends XmlElement{}

impl MobileServiceXmlElement {
    @data_constraint
    pub fn __all__(db: XmlDB) -> *MobileServiceXmlElement {
        for (e in XmlElement(db)) {
            if (e.getElementName() = "mobileService") {
                yield MobileServiceXmlElement {
                    id: e.id,
                    location_id: e.location_id,
                    parent_id: e.parent_id,
                    index_order: e.index_order
                }
            }
        }
    }

    pub fn getServiceBeanValue(self) -> string {
        for (a in self.getAttribute()) {
            if (a.getName() = "serviceBean") {
                return a.getValue()
            }
        }
    }
}

// select XmlElement containing "sofa:extension"
schema SofaExtensionXmlElement extends XmlElement{}
impl SofaExtensionXmlElement {
    @data_constraint
    pub fn __all__(db: XmlDB) -> *SofaExtensionXmlElement {
        for (e in XmlElement(db)) {
            if (e.getName() = "sofa:extension") {
                yield SofaExtensionXmlElement {
                    id: e.id,
                    location_id: e.location_id,
                    parent_id: e.parent_id,
                    index_order: e.index_order
                }
            }
        }
    }
}

fn out(value: string) -> bool {
    let (db = XmlDB::load("coref_xml_src.db")) {
        for (m in MobileServiceXmlElement(db), s in SofaExtensionXmlElement(db), ancestor in m.getAnAncestor()) {
            if (s.key_eq(ancestor) && value = m.getServiceBeanValue()) {
                return true
            }
        }
    }
}

fn main() {
    output(out())
}
`,paraId:228,tocIndex:74},{value:`// script
use coref::go::*

fn default_db() -> GoDB {
    return GoDB::load("coref_go_src.db")
}
/**
 * @param name          file name
 * @param funcCount     function/method quantity
 * @param totallines    total lines of file
 * @param codelines     code line of file
 * @param commentlines  comment line of fine
 * @param md5           md5 of this file
 * @param sha256        sha256 of this file
 */
fn out(
    name: string,
    funcCount: int,
    totallines: int,
    codelines: int,
    commentlines: int,
    md5: string,
    sha256: string) -> bool {
    for(f in File(default_db())) {
        if (name = f.getName() &&
            funcCount = f.getFunctionCount() &&
            md5 = f.getMd5Sum() &&
            sha256 = f.getSha256Sum() &&
            totallines = f.getLineInfo().getNumberOfTotalLines() &&
            codelines = f.getLineInfo().getNumberOfCodeLines() &&
            commentlines = f.getLineInfo().getNumberOfCommentLines()) {
            return true
        }
    }
}

fn main() {
    output(out())
}
`,paraId:229,tocIndex:76},{value:`// script
use coref::go::*

fn default_db() -> GoDB {
    return GoDB::load("coref_go_src.db")
}

// Define a predicate called 'out' with parameters fileName, funcName, funcComment, and signature
fn out(fileName: string, funcName: string, funcComment: string, signature: string) -> bool {
    // Check if there exists a Function object 'func'
    for(func in Function(default_db())) {
        if (
            // Get the name of the file the function belongs to and assign it to the variable 'fileName'
            fileName = func.getBelongsFile().getName() &&
            // Get the name of the function and assign it to the variable 'funcName'
            funcName = func.getName() &&
            // Get the associated comment string for the function and assign it to the variable 'funcComment'
            funcComment = func.getAssociatedCommentString() &&
            // Get the function type signature and assign it to the variable 'signature'
            signature = func.getFunctionTypeSignature()) {
            return true
        }
    }
}

fn main() {
    output(out())
}
`,paraId:230,tocIndex:77},{value:`// script
use coref::go::*

fn default_db() -> GoDB {
    return GoDB::load("coref_go_src.db")
}

/**
 * @param name: file name
 * @param func: function name
 * @param cmplx: function cyclomatic complexity
 * @param sl,el,sc,ec: function location info
 */
fn out(name: string, func: string, cmplx: int, sl: int, el: int) -> bool {
    for(f in GoFile(default_db()), function in Function(default_db())) {
        if ((!f.isAutoGenereatedFile()) &&
            f.key_eq(function.getBelongsFile()) &&
            name = f.getName() &&
            func = function.getName() &&
            cmplx = function.getCyclomaticComplexity() &&
            sl = function.getLocation().getStartLineNumber() &&
            el = function.getLocation().getEndLineNumber()) {
            return true
        }
    }
}

fn main() {
    output(out())
}
`,paraId:231,tocIndex:78},{value:"When running G\xF6delScript scripts, it is common to encounter issues with excessively long run times. Here, we provide some basic methods for diagnosis and solutions.",paraId:232,tocIndex:79},{value:"By default, function parameters without the ",paraId:233,tocIndex:80},{value:"@inline",paraId:233,tocIndex:80},{value:' annotation are considered "qualification" conditions, not true input values.',paraId:233,tocIndex:80},{value:"For example, in the following case, ",paraId:234,tocIndex:80},{value:"get",paraId:234,tocIndex:80},{value:" receives a ",paraId:234,tocIndex:80},{value:"Class",paraId:234,tocIndex:80},{value:" type parameter, but the actual final compilation result will resemble the code below:",paraId:234,tocIndex:80},{value:`fn check(class: Class) -> bool {
    if (class.getName().contains("io")) {
        return true
    }
}

// Actual compilation result
fn check(class: Class) -> bool {
    // Actually, it needs to fetch the entire Class set first
    for(__temp_class in Class::__all__(__all_data__)) {
        if (class = __temp_class) {
            if (class.getName().contains("io")) {
                return true
            }
        }
    }
}
`,paraId:235,tocIndex:80},{value:`Therefore, when passing multiple schema types as parameters, there will be Cartesian products of multiple full schema sets, leading to a significant increase in space and time costs.
The solution is simple: just add an `,paraId:236,tocIndex:80},{value:"@inline",paraId:236,tocIndex:80},{value:" annotation:",paraId:236,tocIndex:80},{value:`@inline
fn check(class: Class) -> bool {
    if (class.getName().contains("io")) {
        return true
    }
}

fn example() -> bool {
    for(class in Class(default_java_db())) {
        if (check(class)) {
            return true
        }
    }
}

// The inline annotation will forcibly inline the function into the statement during the code generation stage, avoiding multiple table loads
// The actual compilation result is similar to
fn example() -> bool {
    for(class in Class(default_java_db())) {
        if (class.getName().contains("io")) {
            return true
        }
    }
}
`,paraId:237,tocIndex:80},{value:"for",paraId:40},{value:"In some cases, it is unavoidable to use multiple layers of ",paraId:238,tocIndex:81},{value:"for",paraId:238,tocIndex:81},{value:" loops to load multiple tables for joint queries, causing severe inflation of Cartesian products. The number of Cartesian product results can be reduced by decreasing (filtering) the size of the sets in advance, as shown in the example:",paraId:238,tocIndex:81},{value:`fn getByIndex(self) -> Expression {
    let (db = default_java_db()) {
        for(e in Expression(db), p in Parameter(db)) {
            let (i = p.getIndex()) {
                if (e.key_eq(self.getValueByIndex(i))) {
                    return e
                }
            }
        }
    }
}
`,paraId:239,tocIndex:81},{value:`In this example, e and p form a Cartesian product, causing the intermediate process to take too long.
The set i is actually obtained from a method of p, and in actual use, this set is very small, much smaller than the full set of Parameter. Therefore, the retrieval of the i set can be extracted as a separate function to produce a small set, avoiding Cartesian product computations between large sets while ensuring result equivalence:`,paraId:240,tocIndex:81},{value:`fn getAllParameterIndex() -> *int {
    let (db = default_java_db()) {
        for (p in Parameter(db)) {
            yield p.getIndex()
        }
    }
}

fn getByIndex(self) -> Expression {
    let (db = default_java_db()) {
        for(e in Expression(db), i in getAllParameterIndex()) {
            if (e.key_eq(self.getValueByIndex(i))) {
                return e
            }
        }
	}
}
`,paraId:241,tocIndex:81},{value:"The Cartesian product of e and p becomes e and i. Operationally, the cost of the Cartesian product is reduced, and the ",paraId:242,tocIndex:81},{value:"getIndex",paraId:242,tocIndex:81},{value:" operation is advanced, rather than taking place after the Cartesian product, significantly improving performance.",paraId:242,tocIndex:81},{value:"@inline",paraId:40},{value:"@inline",paraId:40},{value:"The underlying mechanism of inline functions is to ",paraId:243,tocIndex:82},{value:"expand at the call site",paraId:243,tocIndex:82},{value:". If the function does not have a large number of schema parameters and is called in many places, inline may lead to ",paraId:243,tocIndex:82},{value:"code bloat and an exponential increase in the number of redundant calculations",paraId:243,tocIndex:82},{value:`, which may sometimes be counterproductive in reducing runtime.
If you must use inline, such as to avoid `,paraId:243,tocIndex:82},{value:"ungrounded",paraId:243,tocIndex:82},{value:", but find that using inline slows down the execution speed, you can split the embedded statements into predicates to prevent code bloat caused by expansion.",paraId:243,tocIndex:82},{value:"In the following example, ",paraId:244,tocIndex:82},{value:"getValueByAttributeNameByDefaultValue",paraId:244,tocIndex:82},{value:" is marked with inline to prevent ",paraId:244,tocIndex:82},{value:"attributeName",paraId:244,tocIndex:82},{value:" from being identified as ",paraId:244,tocIndex:82},{value:"ungrounded",paraId:244,tocIndex:82},{value:". Subsequently, a conditional statement was added in the if branch, causing the execution time to increase from 3 seconds to 35 seconds:",paraId:244,tocIndex:82},{value:`impl XmlElementBase {
  @inline
  fn getValueByAttributeNameByDefaultValue(self, attributeName: string) -> string {
    if (self.hasAttribute(attributeName)) {
      // return self.getValueByAttributeName(attributeName)
      // Changed to the following statement:
      let(value = self.getValueByAttributeName(attributeName)) {
        If (value = "n/a") {
          return ""
        }
        if (value != "n/a") {
          return value
        }
      }
    }
    if (!self.hasAttribute(attributeName)) {
      return "null"
    }
  }
}
`,paraId:245,tocIndex:82},{value:"As you can see, adding a level of assignment and a conditional statement, where this function is called nearly 20 times in the subsequent context, resulted in the code being expanded nearly 20 times. This also caused a magnitude difference in performance. At this point, you can extract the changed statement into a separate function. Since the extracted function does not use complex types as parameters, performance is not lost without inline, and after extraction, the result is as follows:",paraId:246,tocIndex:82},{value:`impl XmlElementBase {
  fn getTransValueByAttributeName(self, attributeName: string) -> string {
    let (value = self.getValueByAttributeName(attributeName)) {
      if (value = "n/a") {
        return ""
      }
      if (value != "n/a") {
        return value
      }
    }
  }
  @inline
  fn getValueByAttributeNameByDefaultValue(self, attributeName: string) -> string {
    if (self.hasAttribute(attributeName)) {
      return self.getTransValueByAttributeName(attributeName)
    }
    if (!self.hasAttribute(attributeName)) {
      return "null"
    }
  }
}
`,paraId:247,tocIndex:82},{value:"This way, the execution time is reduced from 35 seconds back to 3 seconds, meeting expectations.",paraId:248,tocIndex:82},{value:"For instructions on using query scripts on your machine, see ",paraId:249,tocIndex:83},{value:"Installation, Configuration, and Running",paraId:250,tocIndex:83},{value:".",paraId:249,tocIndex:83}]},10248:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"Hardware: 4C8G",paraId:0,tocIndex:1},{value:"Environment Requirements: Java 1.8 and Python 3.8 or above runtime environments. Please ensure Java and Python executables are available.",paraId:0,tocIndex:1},{value:"The CodeFuse-Query download package is a zip archive that contains tools, scripts, and various files specific to CodeFuse-Query. If you do not have a CodeFuse-Query license, downloading this archive indicates your agreement with the ",paraId:1,tocIndex:2},{value:"CodeFuse-Query Terms and Conditions",paraId:2,tocIndex:2},{value:".",paraId:1,tocIndex:2},{value:`CodeFuse-Query is currently only supported on Mac and Linux systems. The download links are: (currently, only a sample is given, the official download link will be provided after open-source release)
`,paraId:1,tocIndex:2},{value:"Mac: ",paraId:3,tocIndex:2},{value:"CodeFuse-Query 2.0.0",paraId:3,tocIndex:2},{value:"Linux: ",paraId:3,tocIndex:2},{value:"CodeFuse-Query 2.0.0",paraId:3,tocIndex:2},{value:"You should always use the CodeFuse-Query bundle to ensure version compatibility.",paraId:1,tocIndex:2},{value:"On Mac systems, directly downloading the package may prompt a verification for the developer.",paraId:4,tocIndex:3},{value:"You can modify the verification in the security settings.",paraId:5,tocIndex:3},{value:'Click "Allow Anyway."',paraId:6,tocIndex:3},{value:"For detailed steps, please refer to the ",paraId:7,tocIndex:3},{value:"Mac Official Documentation: How to safely open an app on your Mac",paraId:7,tocIndex:3},{value:"Or use the ",paraId:8,tocIndex:3},{value:"xattr -d com.apple.quarantine",paraId:8,tocIndex:3},{value:" command to remove the external attribute assigned to CodeFuse-Query by macOS.",paraId:8,tocIndex:3},{value:"xattr -d com.apple.quarantine",paraId:9,tocIndex:3},{value:" is a command-line instruction used to delete a file's ",paraId:9,tocIndex:3},{value:"com.apple.quarantine",paraId:9,tocIndex:3},{value:" extended attribute. This attribute is used by the macOS system to mark files or applications downloaded from external sources to ensure security.",paraId:9,tocIndex:3},{value:`xattr -d com.apple.quarantine path/to/file
`,paraId:10,tocIndex:3},{value:"Unzip using the command line or by simply clicking to unzip.",paraId:11,tocIndex:4},{value:"You need to have Java 8 and Python 3.8 or higher runtime environments.",paraId:12,tocIndex:4},{value:"After unzipping CodeFuse-Query, you can run the Sparrow process by running the executable in the following ways:",paraId:13,tocIndex:4},{value:"By executing ",paraId:14,tocIndex:4},{value:"<extraction-root>/sparrow-cli/sparrow",paraId:14,tocIndex:4},{value:", where ",paraId:14,tocIndex:4},{value:"<extraction-root>",paraId:14,tocIndex:4},{value:" is the folder where you extracted the CodeFuse-Query package.",paraId:14,tocIndex:4},{value:"By adding ",paraId:15,tocIndex:4},{value:"<extraction-root>/sparrow-cli",paraId:15,tocIndex:4},{value:" to your PATH, so you can directly run the executable ",paraId:15,tocIndex:4},{value:"sparrow",paraId:15,tocIndex:4},{value:".",paraId:15,tocIndex:4},{value:"At this point, you can execute the ",paraId:16,tocIndex:4},{value:"sparrow",paraId:16,tocIndex:4},{value:" command.",paraId:16,tocIndex:4},{value:"Confirm the source code directory you need to query.",paraId:17,tocIndex:6},{value:"Extract code data from the source code.",paraId:18,tocIndex:6},{value:"Write a G\xF6del script based on the code data to obtain the desired code data.",paraId:19,tocIndex:6},{value:"For how to write G\xF6del scripts, refer to ",paraId:20,tocIndex:6},{value:"G\xF6delScript Query Language",paraId:21,tocIndex:6},{value:`<extraction-root>/sparrow-cli/sparrow database create -s <src> -lang <language> -o <output>
`,paraId:22,tocIndex:8},{value:"<output>",paraId:23,tocIndex:8},{value:": The output directory for the code data extracted from the codebase, referred to as ",paraId:23,tocIndex:8},{value:"<database>",paraId:23,tocIndex:8},{value:" later.",paraId:23,tocIndex:8},{value:"<language>",paraId:24,tocIndex:8},{value:': The language of the code to be extracted, fill in "java" for analyzing Java.',paraId:24,tocIndex:8},{value:"<src>",paraId:25,tocIndex:8},{value:": The source code directory to be scanned.",paraId:25,tocIndex:8},{value:"In the data extraction step, you obtain the database ",paraId:26,tocIndex:8},{value:"<database>",paraId:26,tocIndex:8},{value:" required for executing the script.",paraId:26,tocIndex:8},{value:"Assuming you have the following G\xF6del script to get all Java method names from a specified repository:",paraId:27,tocIndex:9},{value:"For specific G\xF6del script writing, refer to ",paraId:28,tocIndex:9},{value:"G\xF6delScript Query Language",paraId:29,tocIndex:9},{value:`// script
use coref::java::*

// Define the global Java database
fn default_db() -> JavaDB {
    return JavaDB::load("coref_java_src.db")
}

// Iterate over all methods, get the method name, output limit
fn getFunctionName(name: string) -> bool {
    let (db = default_db()) {
        for (method in Method(db)) {
            if (name = method.getName()) {
                return true
            }
        }
    }
}


fn main() {
    output(getFunctionName())
}
`,paraId:30,tocIndex:9},{value:`<extraction-root>/sparrow-cli/sparrow query run -d <database> -gdl <gdl_path> -o <output>
`,paraId:31,tocIndex:10},{value:"<database>",paraId:32,tocIndex:10},{value:": The code data extracted from the codebase to be scanned, consistent with ",paraId:32,tocIndex:10},{value:"<output>",paraId:32,tocIndex:10},{value:" above.",paraId:32,tocIndex:10},{value:"<gdl_path>",paraId:33,tocIndex:10},{value:": The path where the G\xF6del script is located, fill in the directory path, and it will execute all files ending with ",paraId:33,tocIndex:10},{value:".gdl",paraId:33,tocIndex:10},{value:" in that directory in sequence.",paraId:33,tocIndex:10},{value:"<output>",paraId:34,tocIndex:10},{value:": The output directory path, the result of executing xxx.gdl will be stored in ",paraId:34,tocIndex:10},{value:"<output>/xxx.json",paraId:34,tocIndex:10},{value:" in JSON format.",paraId:34,tocIndex:10},{value:"You can verify if the script executed correctly by checking the data file.",paraId:35,tocIndex:10},{value:"Suppose there is the following Java code:",paraId:36,tocIndex:11},{value:`public class HelloWorld {
    public static void main(String[] args) {
        HelloWorld tmp = new HelloWorld();
        String hello = tmp.getHello();
        String world = tmp.getWorld();
        System.out.println(hello + " " + world);
    }

    public String getHello() {
        return "Hello";
    }

    public String getWorld() {
        return "World";
    }
}

`,paraId:37,tocIndex:11},{value:`sparrow database create -s <example> -lang java -o ./db/
sparrow query run -d ./db/ -gdl example.gdl -o ./
`,paraId:38,tocIndex:11},{value:"<example>",paraId:39,tocIndex:11},{value:" is the directory where the given Java file is stored.",paraId:39,tocIndex:11},{value:"example.gdl is the given G\xF6del script sample, saved in the current directory.",paraId:39,tocIndex:11},{value:"After execution, you can find the example.json file in the current directory.",paraId:39,tocIndex:11},{value:"The corresponding script output JSON file content is as follows:",paraId:40,tocIndex:11},{value:`[{"name": "getHello"},
{"name": "getWorld"},
{"name": "main"}]

`,paraId:41,tocIndex:11}]},21078:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:`CodeFuse-Query is a code data platform that supports structured analysis of various programming languages. The core idea is to transform all code into data using various language parsers and to store this data in a structured format within a code database. Data analysis is then performed according to business needs using a custom query language, as shown in the diagram below:
`,paraId:0,tocIndex:0},{value:`Overall, the CodeFuse-Query code data platform is divided into three main parts: the code data model, the code query DSL (Domain-Specific Language), and platform productization services. The main workflow is illustrated in the following diagram:
`,paraId:1,tocIndex:1},{value:`We have defined a model for code datafication and standardization called COREF, which requires all code to be converted to this model through various language extractors.
COREF mainly includes the following information:
`,paraId:2,tocIndex:2},{value:"COREF",paraId:2,tocIndex:2},{value:` = AST (Abstract Syntax Tree) + ASG (Abstract Semantic Graph) + CFG (Control Flow Graph) + PDG (Program Dependency Graph) + Call Graph + Class Hierarchy + Documentation (Documentation/Commentary Information)
Note: As the computational complexity of each type of information varies, not all languages' COREF information includes all of the above. The basic information mainly includes AST, ASG, Call Graph, Class Hierarchy, and Documentation, while other information (CFG and PDG) is still under development and will be gradually supported.`,paraId:2,tocIndex:2},{value:"Based on the generated COREF code data, CodeFuse-Query uses a custom DSL language called ",paraId:3,tocIndex:3},{value:"G\xF6del",paraId:3,tocIndex:3},{value:` for querying, thereby fulfilling code analysis requirements.
G\xF6del is a logic-based reasoning language, whose underlying implementation is based on the logical reasoning language Datalog. By describing "facts" and "rules," the program can continuously derive new facts. G\xF6del is also a declarative language, focusing more on describing "what is needed" and leaving the implementation to the computational engine.
Since code has already been converted to relational data (COREF data stored in the form of relational tables), one might wonder why not use SQL directly, or use an SDK instead of learning a new DSL language. Because Datalog's computation is monotonic and terminating. Simply put, Datalog sacrifices expressiveness to achieve higher performance, and G\xF6del inherits this feature.`,paraId:3,tocIndex:3},{value:"Compared to SDKs, G\xF6del's main advantage is its ease of learning and use. As a declarative language, users do not need to focus on intermediate computations and can simply describe their needs as they would with SQL.",paraId:4,tocIndex:3},{value:"Compared to SQL, G\xF6del's advantages are stronger descriptive capabilities and faster computation speed, for example, describing recursive algorithms and multi-table joint queries, which are difficult for SQL.",paraId:4,tocIndex:3},{value:"CodeFuse-Query includes the ",paraId:5,tocIndex:4},{value:"Sparrow CLI",paraId:5,tocIndex:4},{value:" and the online service ",paraId:5,tocIndex:4},{value:"Query Centre",paraId:5,tocIndex:4},{value:". Sparrow CLI contains all components and dependencies, such as extractors, data models, compilers, etc., and users can completely generate and query code data locally using Sparrow CLI (for how to use Sparrow CLI, please see Section 3: Installation, Configuration, Running). If users have online query needs, they can use the Query Centre to experiment.",paraId:5,tocIndex:4},{value:"As of October 31, 2023, CodeFuse-Query supports data analysis for 11 programming languages. Among these, support for 5 languages (Java, JavaScript, TypeScript, XML, Go) is very mature, while support for the remaining 6 languages (Objective-C, C++, Python3, Swift, SQL, Properties) is in beta and has room for further improvement. The specific support status is shown in the table below:",paraId:6,tocIndex:5},{value:"Language",paraId:7,tocIndex:5},{value:"Status",paraId:7,tocIndex:5},{value:"Number of Nodes in the COREF Model",paraId:7,tocIndex:5},{value:"Java",paraId:7,tocIndex:5},{value:"Mature",paraId:7,tocIndex:5},{value:"162",paraId:7,tocIndex:5},{value:"XML",paraId:7,tocIndex:5},{value:"Mature",paraId:7,tocIndex:5},{value:"12",paraId:7,tocIndex:5},{value:"TS/JS",paraId:7,tocIndex:5},{value:"Mature",paraId:7,tocIndex:5},{value:"392",paraId:7,tocIndex:5},{value:"Go",paraId:7,tocIndex:5},{value:"Mature",paraId:7,tocIndex:5},{value:"40",paraId:7,tocIndex:5},{value:"OC/C++",paraId:7,tocIndex:5},{value:"Beta",paraId:7,tocIndex:5},{value:"53/397",paraId:7,tocIndex:5},{value:"Python3",paraId:7,tocIndex:5},{value:"Beta",paraId:7,tocIndex:5},{value:"93",paraId:7,tocIndex:5},{value:"Swift",paraId:7,tocIndex:5},{value:"Beta",paraId:7,tocIndex:5},{value:"248",paraId:7,tocIndex:5},{value:"SQL",paraId:7,tocIndex:5},{value:"Beta",paraId:7,tocIndex:5},{value:"750",paraId:7,tocIndex:5},{value:"Properties",paraId:7,tocIndex:5},{value:"Beta",paraId:7,tocIndex:5},{value:"9",paraId:7,tocIndex:5},{value:"Note: The maturity level of the language status above is determined based on the types of information included in COREF and the actual implementation. Except for OC/C++, all languages support complete AST information and Documentation. For example, COREF for Java also supports ASG, Call Graph, Class Hierarchy, and some CFG information.",paraId:8,tocIndex:5},{value:"A developer wants to know which String type variables are used in Repo A, so they write a G\xF6del script as follows and submit it to the CodeFuse-Query system for results.",paraId:9,tocIndex:7},{value:`// script
use coref::java::*

fn out(var: string) -> bool {
  for(v in Variable(JavaDB::load("coref_java_src.db"))) {
    if (v.getType().getName() = "String" && var = v.getName()) {
      return true
    }
  }
}

fn main() {
  output(out())
}
`,paraId:10,tocIndex:7},{value:"Similar needs: Queries for classes, functions, variables, return values, call graphs, class inheritance, etc.",paraId:11,tocIndex:7},{value:"A security team member sets up ",paraId:12,tocIndex:8},{value:"a system",paraId:12,tocIndex:8},{value:` to cross-verify that log data and code data are consistent. To complete a certain analysis task, they plan to derive static data D1 through G\xF6del queries, merge with dynamic data D2, and combine analysis to reach conclusion C. After verifying the technical feasibility on CodeFuse-Query, they integrate the system using the standard API provided by CodeFuse-Query.
Similar needs: Using static analysis as a system checkpoint, improving testing efficiency, merging the analyzed data into a documentation.`,paraId:12,tocIndex:8},{value:"A team lead finds that the team often introduces similar bugs, Bug A, ",paraId:13,tocIndex:9},{value:"and decides to establish a code rule and its checker",paraId:13,tocIndex:9},{value:` to be applied during CodeReview. After writing an analysis query on the CodeFuse-Query platform and testing that it meets requirements, they codify the query as a code rule and roll it out to the CodeReview/CI phase. Since then, this bug has never occurred again.
Similar needs: Writing static defect scanning rules to intercept code risks.`,paraId:13,tocIndex:9},{value:"A developer from the R&D department wants to know the current proportion of Spring and Spring Boot projects in the code repository to quantify the promotion of the new framework. By writing a G\xF6del Query to describe different project analysis features, they ",paraId:14,tocIndex:10},{value:"queried 110,000 code repositories at once",paraId:14,tocIndex:10},{value:` and obtained all the code data after a few dozen minutes, happily moving on to their KPIs.
Similar needs: Application profiling, code profiling, architectural analysis.`,paraId:14,tocIndex:10},{value:"A researcher finds that traditional code complexity metrics struggle to accurately measure the complexity of the code. Inspired by international advanced experiences and a moment of insight, they design a set of complexity metrics and algorithms. After implementing it with G\xF6del and finding it already highly performant with little optimization, they quickly apply it to over 10 languages and more than 110,000 repositories. They now have an in-depth understanding of the overall complexity of the code repositories, unlike before when they had to parse the code and analyze the syntax tree themselves, ",paraId:15,tocIndex:11},{value:"which is so much more convenient",paraId:15,tocIndex:11},{value:`.
Similar needs: Code statistics, code metrics, algorithm design, academic research.`,paraId:15,tocIndex:11},{value:`An architect recently promoted a new message middleware based on txt files, and existing analysis platforms couldn't support analyzing dependencies in such systems. By quickly modeling the message format with G\xF6del, they soon obtain the dependency relationships between different components in the system.
Similar needs: System overview, architecture governance, lineage analysis.`,paraId:16,tocIndex:12},{value:"A developer designs a system that requires users to play games before claiming coupons. They describe ",paraId:17,tocIndex:13},{value:"the model's validation logic",paraId:17,tocIndex:13},{value:" with G\xF6del, then use the CodeFuse-Query system to ",paraId:17,tocIndex:13},{value:"ensure that both current and future system implementations",paraId:17,tocIndex:13},{value:` fully comply with the model. No longer worried about potential financial losses from the game!
Similar needs: System verification, network validation, permission verification.`,paraId:17,tocIndex:13},{value:"Currently, CodeFuse-Query at Ant Group already supports ",paraId:18,tocIndex:14},{value:"CodeFuse large language model data cleaning",paraId:18,tocIndex:14},{value:", ",paraId:18,tocIndex:14},{value:"code metrics evaluation",paraId:18,tocIndex:14},{value:", ",paraId:18,tocIndex:14},{value:"R&D risk control",paraId:18,tocIndex:14},{value:", ",paraId:18,tocIndex:14},{value:"privacy security analysis",paraId:18,tocIndex:14},{value:", ",paraId:18,tocIndex:14},{value:"code intelligence",paraId:18,tocIndex:14},{value:", ",paraId:18,tocIndex:14},{value:"terminal package size management",paraId:18,tocIndex:14},{value:`, and other scenarios with implemented applications, serving over a million monthly calls.
`,paraId:18,tocIndex:14},{value:`The CodeFuse Large Code Model is a model by Ant Group for handling code-related issues and has been open-sourced. For the CodeFuse large language model, the quality of the training data directly affects the model's inference results. Low-quality code data can directly contaminate the language model's output, for example: the model might learn incorrect code patterns, generating erroneous code; if the data only contains code in a single programming language, the model might not adapt well to code in other languages.
To control the quality of code data entering the model and thereby improve the model's inferencing capabilities, we have drawn upon the Ant Group program analysis team's years of practical experience coupled with industry consensus to clarify the definition of high-quality code. We have also implemented automated, large-scale code data cleaning using existing program analysis technologies.
CodeFuse-Query provides the following data cleaning capabilities for the CodeFuse Large Code Model:`,paraId:19,tocIndex:15},{value:"High-quality code data cleaning: Clean code data, including vulnerability scanning for 7 languages (Python, Java, JavaScript, TypeScript, Go, C, C++), filtering by language type/star number, filtering out data with 0 valid lines of code, etc. We have currently accumulated about ",paraId:20,tocIndex:15},{value:"2TB",paraId:20,tocIndex:15},{value:" of cleaned code data from GitHub and internally at Ant Group.",paraId:20,tocIndex:15},{value:"Code Profiling: Implements high-performance, multi-dimensional automatic tagging for large-scale code, supporting ",paraId:20,tocIndex:15},{value:"10",paraId:20,tocIndex:15},{value:" languages (Java, Scala, Kotlin, JavaScript, JSX, TypeScript, TSX, Vue, Python, Go), ",paraId:20,tocIndex:15},{value:"77",paraId:20,tocIndex:15},{value:" common tags, ",paraId:20,tocIndex:15},{value:"40",paraId:20,tocIndex:15},{value:" Ant-specific tags, totaling ",paraId:20,tocIndex:15},{value:"117",paraId:20,tocIndex:15},{value:" tags. The current auto-tagging performance can reach ",paraId:20,tocIndex:15},{value:"40MB/s",paraId:20,tocIndex:15},{value:".",paraId:20,tocIndex:15},{value:`Other Atomic Abilities
`,paraId:20,tocIndex:15},{value:"Advanced code feature extraction, including extraction of AST (Abstract Syntax Tree), DFG (Data Flow Graph), etc. The AST information has been used for SFT training with about 97% accuracy.",paraId:21,tocIndex:15},{value:`Code snippet identification, used for extracting code from text data, convenient for formatting or adding Markdown:
`,paraId:21,tocIndex:15},{value:"Text extraction of code: Extracting code block information from text, parsing main languages, function and class definitions, only verifying a binary problem, that is, verifying whether the text contains code blocks with about 83% accuracy.",paraId:22,tocIndex:15},{value:"Identifying the programming language of a code snippet: Identifying the programming language of any code snippet, supporting 30+ languages, with about 80% accuracy.",paraId:22,tocIndex:15},{value:"Code comment pair extraction: Supports extracting method-level comment-code pair information, covering ",paraId:21,tocIndex:15},{value:"15",paraId:21,tocIndex:15},{value:" most popular languages on GitHub, used for Text To Code/Code To Text SFT training.",paraId:21,tocIndex:15},{value:`Guangmu is an internal product at Ant Group aimed at different R&D personnel and team managers, providing objective data and analysis results to assess code capabilities.
Guangmu offers individual code capability assessment reports, daily code capability metric data analysis, team code capability management, and code excellence award displays, all aimed at helping Ant Group's R&D engineers continuously improve code quality, reduce code debt, and enhance R&D efficiency in the long run.
CodeFuse-Query provides Guangmu with two types of capabilities:`,paraId:23,tocIndex:16},{value:"Code Evaluation Metrics: Code complexity, code annotation rate, standard development volume, etc.",paraId:24,tocIndex:16},{value:"Code Excellence Metrics: Code reuse degree.",paraId:24,tocIndex:16},{value:"The Youku Quality Assurance team started exploring server-side precision testing in 2023. After six months of technical sedimentation and system building, they established a precision testing system capable of ",paraId:25,tocIndex:17},{value:"change content identification, change impact analysis, testing capability recommendation, and test coverage assessment",paraId:25,tocIndex:17},{value:`.
In this process, CodeFuse-Query can provide capabilities including:`,paraId:25,tocIndex:17},{value:"Analyzing the impacted objects based on code change content (file + line number): methods, entry points (HTTP entry, HSF entry), call routes (all call routes from the entry to the changed method), database operations (tables, types of operations).",paraId:26,tocIndex:17},{value:"Enhancing the effectiveness and readiness of change analysis impact by combining the precise analysis capabilities of online dynamic call routes (method routes) and CodeFuse-Query static analysis call routes.",paraId:26,tocIndex:17},{value:"To date, Youku has integrated all core applications through CodeFuse-Query and based on static analysis data collection, has built a complete server-side code and traffic knowledge base.",paraId:27,tocIndex:17}]},27107:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"VSCode Extension",paraId:0,tocIndex:2},{value:"Download the plugin.",paraId:1,tocIndex:3},{value:`Manually install from vsix:
`,paraId:1,tocIndex:3},{value:"Or use the command directly from the terminal to install:",paraId:1,tocIndex:3},{value:`code --install-extension [extension vsix file path]
`,paraId:2,tocIndex:3},{value:"Sparrow CLI, refer to Section 3 Installation, Configuration, and Running.",paraId:3,tocIndex:4},{value:"This extension provides the following feature modules:",paraId:4,tocIndex:5},{value:"COREF AST Viewer",paraId:5,tocIndex:5},{value:"G\xF6del Language Server",paraId:5,tocIndex:5},{value:"G\xF6del Language Runner",paraId:5,tocIndex:5},{value:"The following features need to be enabled in the extension settings. Currently, it only supports the Java language.",paraId:6,tocIndex:6},{value:"The following features need to be enabled after setting up the extension. Syntax highlighting is still available without setting related items.",paraId:7,tocIndex:11},{value:`Error information automatically updates with code changes.
`,paraId:8,tocIndex:12},{value:"Completion suggestions that include local variables and global symbols. Keywords provide corresponding usage examples; global symbol information offers more detailed internal information, such as member variables, member methods, and static methods.",paraId:9,tocIndex:13},{value:"Keyword completion and usage example tips",paraId:10,tocIndex:13},{value:"Local variable type information and symbol completion",paraId:10,tocIndex:13},{value:".",paraId:10,tocIndex:13},{value:" followed by symbol information and completion",paraId:10,tocIndex:13},{value:"::",paraId:10,tocIndex:13},{value:" followed by symbol information and completion",paraId:10,tocIndex:13},{value:"Annotation usage example tips",paraId:10,tocIndex:13},{value:"Global symbol type information (internal structure, member methods, static methods)",paraId:10,tocIndex:13},{value:"You can jump to definitions with a right-click or ",paraId:11,tocIndex:14},{value:"ctrl",paraId:11,tocIndex:14},{value:"/",paraId:11,tocIndex:14},{value:"command",paraId:11,tocIndex:14},{value:"+",paraId:11,tocIndex:14},{value:"left click",paraId:11,tocIndex:14},{value:" to go directly to the exact symbol definition location.",paraId:11,tocIndex:14},{value:"The extension provides some code snippets to quickly write G\xF6del 1.0/script code.",paraId:12,tocIndex:15},{value:"Use after setting the Sparrow CLI path in the extension. The database needs to be loaded before running the script. For how to generate a database, refer to Section 3.4, Running, in the data extraction part.",paraId:13,tocIndex:16},{value:`
There are four different script running buttons provided:`,paraId:14,tocIndex:17},{value:"Right-click to execute at the script you want to run.",paraId:15,tocIndex:17},{value:"Choose ",paraId:15,tocIndex:17},{value:"Run G\xF6delScript",paraId:15,tocIndex:17},{value:" on the extension ",paraId:15,tocIndex:17},{value:"GodelScript Runner",paraId:15,tocIndex:17},{value:" panel.",paraId:15,tocIndex:17},{value:"Choose ",paraId:15,tocIndex:17},{value:"Run",paraId:15,tocIndex:17},{value:" on the extension ",paraId:15,tocIndex:17},{value:"GodelScript Runner Setting",paraId:15,tocIndex:17},{value:" panel.",paraId:15,tocIndex:17},{value:"Click the run button at the top right of the extension ",paraId:15,tocIndex:17},{value:"GodelScript Runner Setting",paraId:15,tocIndex:17},{value:" panel.",paraId:15,tocIndex:17},{value:"Right-click at the script you want to run and choose the folder containing the database to load.",paraId:16,tocIndex:18},{value:"Choose ",paraId:16,tocIndex:18},{value:"Load Database Directory",paraId:16,tocIndex:18},{value:" on the extension ",paraId:16,tocIndex:18},{value:"GodelScript Runner",paraId:16,tocIndex:18},{value:" panel.",paraId:16,tocIndex:18},{value:"Choose ",paraId:16,tocIndex:18},{value:"Database",paraId:16,tocIndex:18},{value:" on the extension ",paraId:16,tocIndex:18},{value:"GodelScript Runner Setting",paraId:16,tocIndex:18},{value:" panel.",paraId:16,tocIndex:18},{value:"Click the database load button at the top right of the extension ",paraId:16,tocIndex:18},{value:"GodelScript Runner Setting",paraId:16,tocIndex:18},{value:" panel.",paraId:16,tocIndex:18},{value:"corefASTViewer.sparrowCliRoot",paraId:17,tocIndex:20},{value:"Specify the root directory of Sparrow CLI, referring to Section 3 of the installation part.",paraId:18,tocIndex:20},{value:"When the extension starts, a prompt will pop up if any one of the following two items is not set. Clicking the ",paraId:19,tocIndex:21},{value:"configure",paraId:19,tocIndex:21},{value:" button will redirect to the respective configuration page.",paraId:19,tocIndex:21},{value:"godelScript.executablePath",paraId:20,tocIndex:21},{value:"Used to specify the executable path of G\xF6delScript, default is empty. Please replace with the actual absolute path of the G\xF6delScript executable when needed.",paraId:21,tocIndex:21},{value:"If Sparrow CLI is already downloaded, then the G\xF6delScript executable file is ",paraId:21,tocIndex:21},{value:"[sparrow cli root]/godel-script/usr/bin/godel",paraId:21,tocIndex:21},{value:".",paraId:21,tocIndex:21},{value:"godelScript.libraryDirectoryPath",paraId:20,tocIndex:21},{value:"Used to specify the library folder path of G\xF6delScript, default is empty. Please replace with the absolute path of the G\xF6delScript library folder when needed.",paraId:22,tocIndex:21},{value:"If Sparrow CLI is already downloaded, then the library folder path is ",paraId:22,tocIndex:21},{value:"[sparrow cli root]/lib-1.0",paraId:22,tocIndex:21},{value:".",paraId:22,tocIndex:21},{value:"Stay tuned for the opening!",paraId:23,tocIndex:22}]},15422:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"A developer wants to know which String type variables are used in Repo A, so he writes a G\xF6del script as follows and submits it to the CodeFuse-Query system for results.",paraId:0,tocIndex:1},{value:`// script
use coref::java::*

fn out(var: string) -> bool {
  for(v in Variable(JavaDB::load("coref_java_src.db"))) {
    if (v.getType().getName() = "String" && var = v.getName()) {
      return true
    }
  }
}

fn main() {
  output(out())
}
`,paraId:1,tocIndex:1},{value:"Similar needs: querying for classes, functions, variables, return values, call graphs, class inheritance, etc.",paraId:2,tocIndex:1},{value:"A team leader found that the team always wrote many bugs similar to Bug A. ",paraId:3,tocIndex:2},{value:"He wanted to establish a code rule for Bug A and its checker",paraId:3,tocIndex:2},{value:` and do a check at the CodeReview stage. Through writing a query analysis on the CodeFuse-Query platform, and after testing it on the platform to meet the requirements, he solidified this analysis query as a code rule and launched it to the CodeReview/CI stage. Since then, this bug has never happened again.
Similar needs: writing static defect scanning rules for code risk interception.`,paraId:3,tocIndex:2},{value:"A researcher found that traditional code complexity metrics are difficult to accurately measure code complexity. By learning from international advanced experience and a stroke of genius, he designed a set of complexity metrics and algorithms. After implementing it with G\xF6del, ",paraId:4,tocIndex:3},{value:"he found that without much optimization, the performance was already very high",paraId:4,tocIndex:3},{value:", and it was quickly applied to more than 10 languages and over 110,000 repositories. He immediately had an in-depth understanding of the overall complexity of code repositories. Compared to the past, when he had to parse code and analyze syntax trees himself, and interface with systems, ",paraId:4,tocIndex:3},{value:"it's hard to know how much more convenient it has become",paraId:4,tocIndex:3},{value:`.
Similar needs: code statistics, code metrics, algorithm design, academic research.`,paraId:4,tocIndex:3},{value:"Currently, CodeFuse-Query at Ant Group has already supported the implementation of multiple scenarios such as ",paraId:5,tocIndex:4},{value:"CodeFuse large language model data cleaning",paraId:5,tocIndex:4},{value:", ",paraId:5,tocIndex:4},{value:"code metric assessment",paraId:5,tocIndex:4},{value:", ",paraId:5,tocIndex:4},{value:"R&D risk control",paraId:5,tocIndex:4},{value:", ",paraId:5,tocIndex:4},{value:"privacy security analysis",paraId:5,tocIndex:4},{value:", ",paraId:5,tocIndex:4},{value:"code intelligence",paraId:5,tocIndex:4},{value:", ",paraId:5,tocIndex:4},{value:"end-package size governance",paraId:5,tocIndex:4},{value:", etc., with a monthly service call volume exceeding one million.",paraId:5,tocIndex:4},{value:`The CodeFuse code large model is a model for dealing with code-related issues open-sourced by Ant Group. For the CodeFuse large language model, the quality of the training data directly affects the inference results of the model. Low-quality code data will directly pollute the output of the language model. For example, the model may learn incorrect code patterns, thereby generating incorrect code. If the data only contains code in a certain programming language, the model may not adapt well to the code of other programming languages.
To control the quality of code data entering the model and thereby improve the inferential capability of the model, we have sorted out the definition of high-quality code based on years of practical experience of the Ant code analysis team combined with industry consensus, and implemented automated, large-scale code data cleaning using existing program analysis technology.
CodeFuse-Query provides the following data cleaning capabilities for the CodeFuse code large model:`,paraId:6,tocIndex:5},{value:"High-quality code data cleaning: clean code data, including vulnerability scanning for Python, Java, JavaScript, TypeScript, Go, C, C++ 7 languages, filtering by language type/star count, filtering out data with 0 effective code lines, etc. Currently, about ",paraId:7,tocIndex:5},{value:"2TB",paraId:7,tocIndex:5},{value:" of cleaned GitHub and Ant internal code data has been accumulated.",paraId:7,tocIndex:5},{value:"Code Portrait: Implement high-performance, multi-dimensional automatic annotation of large-scale code, supporting Java, Scala, Kotlin, JavaScript, JSX, TypeScript, TSX, Vue, Python, Go, and other ",paraId:7,tocIndex:5},{value:"10",paraId:7,tocIndex:5},{value:" languages, ",paraId:7,tocIndex:5},{value:"77",paraId:7,tocIndex:5},{value:" common tags, ",paraId:7,tocIndex:5},{value:"40",paraId:7,tocIndex:5},{value:" Ant-specific tags, a total of ",paraId:7,tocIndex:5},{value:"117",paraId:7,tocIndex:5},{value:" tags. Current auto-annotation performance can reach ",paraId:7,tocIndex:5},{value:"40MB/s",paraId:7,tocIndex:5},{value:".",paraId:7,tocIndex:5},{value:`Other atomic capabilities
`,paraId:7,tocIndex:5},{value:"Advanced code feature extraction, including AST (Abstract Syntax Tree), DFG (Data Flow Graph) data extraction, etc. Currently, AST information has been used for SFT training, with an accuracy of about 97%.",paraId:8,tocIndex:5},{value:`Code snippet identification, used for extracting code from text data, convenient for code formatting or adding Markdown format:
`,paraId:8,tocIndex:5},{value:"Text extraction code: extract code block information from the text, support parsing of mainstream languages, function and class definitions, only validate binary classification, which is to verify whether the text contains code blocks, accuracy is about 83%.",paraId:9,tocIndex:5},{value:"Identify the programming language type of code snippets: identify the programming language type of any code snippet, support 30+ languages, accuracy is about 80%.",paraId:9,tocIndex:5},{value:"Code comment pair extraction: support extraction of method-level comment-code pair information, cover ",paraId:8,tocIndex:5},{value:"15 kinds",paraId:8,tocIndex:5},{value:" of GitHub's most popular languages, used for Text To Code/Code To Text SFT training.",paraId:8,tocIndex:5},{value:"From 2023, the Youku quality assurance team started exploring precise testing for the server-side. After half a year of technical accumulation and system building, a precise testing system with ",paraId:10,tocIndex:6},{value:"change content identification, change impact analysis, testing capability recommendation, testing coverage assessment",paraId:10,tocIndex:6},{value:` was formed.
In this process, the capabilities that CodeFuse-Query can provide mainly include:`,paraId:10,tocIndex:6},{value:"Analyze the affected objects based on the code changes (file + line number): methods, entry points (http entry, hsf entry), call routes (all call routes from entry to changed method), database operations (table, operation type)",paraId:11,tocIndex:6},{value:"Combined with the online dynamic call route (method route), CodeFuse-Query static analysis call route impact surface precise analysis capability, improve the effectiveness and preparation rate of change analysis impact surface",paraId:11,tocIndex:6},{value:"Up to now, Youku has integrated all core applications through CodeFuse-Query and has built a comprehensive server-side code knowledge base and network traffic knowledge base based on static analysis.",paraId:12,tocIndex:6}]},59279:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:`
  `,paraId:0},{value:`
    `,paraId:1,tocIndex:0},{value:"CodeFuseEval on ModelScope",paraId:1,tocIndex:0},{value:`\uFF5C
    `,paraId:1,tocIndex:0},{value:"CodeFuseEval on Hugging Face",paraId:1,tocIndex:0},{value:`
  `,paraId:1,tocIndex:0},{value:"CodeFuseEval is a Code Generation benchmark that combines the multi-tasking scenarios of CodeFuse Model with the benchmarks of HumanEval-x and MBPP. This benchmark is designed to evaluate the performance of models in various multi-tasking tasks, including code completion, code generation from natural language, test case generation, cross-language code translation, and code generation from Chinese commands, among others.Continuously open, stay tuned !",paraId:2,tocIndex:0},{value:`
    `,paraId:3,tocIndex:0}]},42123:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"CodeFuse-13B: Python 3.8 or above,PyTorch 1.12 or above, with a recommendation for 2.0 or above, Transformers 4.24.0 or above ,CUDA 11.4 or above (for GPU users and flash-attention users, this option should be considered).",paraId:0,tocIndex:0},{value:"CodeFuse-CodeLlama-34B:python>=3.8,pytorch>=2.0.0,transformers==4.32.0,Sentencepiece,CUDA 11.",paraId:1,tocIndex:0},{value:"The evaluation of the generated codes involves compiling and running in multiple programming languages. The versions of the programming language environments and packages we use are as follows:",paraId:2,tocIndex:1},{value:"Dependency",paraId:3,tocIndex:1},{value:"Version",paraId:3,tocIndex:1},{value:"Python",paraId:3,tocIndex:1},{value:"3.10.9",paraId:3,tocIndex:1},{value:"JDK",paraId:3,tocIndex:1},{value:"18.0.2.1",paraId:3,tocIndex:1},{value:"Node.js",paraId:3,tocIndex:1},{value:"16.14.0",paraId:3,tocIndex:1},{value:"js-md5",paraId:3,tocIndex:1},{value:"0.7.3",paraId:3,tocIndex:1},{value:"C++",paraId:3,tocIndex:1},{value:"11",paraId:3,tocIndex:1},{value:"g++",paraId:3,tocIndex:1},{value:"7.5.0",paraId:3,tocIndex:1},{value:"Boost",paraId:3,tocIndex:1},{value:"1.75.0",paraId:3,tocIndex:1},{value:"OpenSSL",paraId:3,tocIndex:1},{value:"3.0.0",paraId:3,tocIndex:1},{value:"go",paraId:3,tocIndex:1},{value:"1.18.4",paraId:3,tocIndex:1},{value:"cargo",paraId:3,tocIndex:1},{value:"1.71.1",paraId:3,tocIndex:1},{value:"In order to save everyone the trouble of setting up the environments for these languages, we create a Docker image with the required environments and codefuseEval.",paraId:4,tocIndex:1},{value:`docker pull registry.cn-hangzhou.aliyuncs.com/codefuse/codefuseeval:latest
`,paraId:5,tocIndex:1},{value:"If you are familiar with docker, you can build the image from ",paraId:6,tocIndex:1},{value:"codefuseEval/docker/Dockerfile",paraId:6,tocIndex:1},{value:" or configure the Dockerfile as you like it:",paraId:6,tocIndex:1},{value:`cd codefuseEval/docker
docker build [OPTIONS] .
`,paraId:7,tocIndex:1},{value:"After obtaining the image, you can build a container using the following command:",paraId:8,tocIndex:1},{value:`docker run -it --gpus all --mount type=bind,source=<LOCAL PATH>,target=<PATH IN CONTAINER> [OPTIONS] <IMAGE NAME:TAG>
`,paraId:9,tocIndex:1},{value:"We provide the script to check the result for provided code LLMs. Please use following scripts to check corresponding results and the environment .",paraId:10,tocIndex:2},{value:`bash codefuseEval/script/check_reference.sh codefuseEval/result/CodeFuse-CodeLlama-34B/humaneval_result_python.jsonl humaneval_python
bash codefuseEval/script/check_reference.sh codefuseEval/result/CodeFuse-13B/humaneval_result_python.jsonl humaneval_python
`,paraId:11,tocIndex:2},{value:"Download the model and update current model infomation in ckpt_config.json. Mainly update \u300Cpath\u300Dparameter in corresponding model and version.",paraId:12,tocIndex:3},{value:"Run following generation comand to generate result.",paraId:12,tocIndex:3},{value:`bash codefuseEval/script/generation.sh MODELNAME MODELVERSION EVALDATASET OUTFILE

eg:
bash codefuseEval/script/generation.sh CodeFuse-13B v1 humaneval_python result/test.jsonl
`,paraId:13,tocIndex:3},{value:"Run following evaluation command to evaluate the generated result for corresponding model and version.",paraId:14,tocIndex:3},{value:`bash codefuseEval/script/evaluation.sh <RESULT_FILE> <METRIC> <PROBLEM_FILE>
eg:
bash codefuseEval/script/evaluation.sh codefuseEval/result/test.jsonl pass@k humaneval_python
`,paraId:15,tocIndex:3},{value:"We recommend evaluating in ",paraId:16,tocIndex:4},{value:"the provided image",paraId:17,tocIndex:4},{value:". To evaluate the generated samples, save generated codes in the following JSON list format:",paraId:16,tocIndex:4},{value:`{"task_id": "../..", "generation: "..."}
{"task_id": "../..", "generation: "..."}
...
`,paraId:18,tocIndex:4},{value:"and evaluate them using the following script under the root directory of the repository (",paraId:19,tocIndex:4},{value:"please execute with caution, the generated codes might have unexpected behaviours though with very low possibility. See the warnings in ",paraId:19,tocIndex:4},{value:"execution.py",paraId:20,tocIndex:4},{value:" and uncomment the execution lines at your own risk",paraId:19,tocIndex:4},{value:"):",paraId:19,tocIndex:4},{value:"Data are stored in ",paraId:21,tocIndex:5},{value:"codefuseEval/data",paraId:21,tocIndex:5},{value:", using JSON list format. We first integrated humaneval-X dataset.",paraId:21,tocIndex:5},{value:"task_id",paraId:22,tocIndex:5},{value:': indicates the target language and ID of the problem. Language is one of ["Python", "Java", "JavaScript", "CPP", "Go"].',paraId:22,tocIndex:5},{value:"prompt",paraId:22,tocIndex:5},{value:": the function declaration and docstring, used for code generation.",paraId:22,tocIndex:5},{value:"declaration",paraId:22,tocIndex:5},{value:": only the function declaration, used for code translation.",paraId:22,tocIndex:5},{value:"canonical_solution",paraId:22,tocIndex:5},{value:": human-crafted example solutions.",paraId:22,tocIndex:5},{value:"test",paraId:22,tocIndex:5},{value:": hidden test samples, used for evaluation",paraId:22,tocIndex:5},{value:"example_test",paraId:22,tocIndex:5},{value:": public test samples (appeared in prompt), used for evaluation.",paraId:22,tocIndex:5},{value:"prompt_text",paraId:22,tocIndex:5},{value:": prompt text",paraId:22,tocIndex:5},{value:"prompt_explain",paraId:22,tocIndex:5},{value:": prompt explanation",paraId:22,tocIndex:5},{value:"func_title",paraId:22,tocIndex:5},{value:": code function title",paraId:22,tocIndex:5},{value:"prompt_text_chinese",paraId:22,tocIndex:5},{value:": Chinese prompt",paraId:22,tocIndex:5},{value:"In addition to the unbiased pass@k indicators currently provided in ",paraId:23,tocIndex:6},{value:"Codex",paraId:23,tocIndex:6},{value:", we will also integrate the relevant indicators of huggingface open source with ",paraId:23,tocIndex:6},{value:"CodeBLEU",paraId:23,tocIndex:6},{value:` for integration.
The main indicators currently recommended for users are as follows:`,paraId:23,tocIndex:6},{value:"codebleu",paraId:24,tocIndex:6},{value:"pass@k",paraId:24,tocIndex:6},{value:"bleu",paraId:24,tocIndex:6},{value:"bleurt",paraId:24,tocIndex:6},{value:"For other related metrics, you can check the code of the metric or the evaluation code to meet your requirements.",paraId:25,tocIndex:6},{value:"At the same time, we supplemented the indicators of the total and average generation time of the model for the dataset ",paraId:26,tocIndex:6},{value:"total_time_cost",paraId:26,tocIndex:6},{value:" and ",paraId:26,tocIndex:6},{value:"Average time cost",paraId:26,tocIndex:6},{value:"Output during each generation, making it convenient for users to measure the generation performance of the model in the same environment. This indicator is passive output, and it will be output every time it is generated.",paraId:27,tocIndex:6},{value:`bash codefuseEval/script/evaluation.sh <RESULT_FILE> <METRIC> <PROBLEM_FILE> <TEST_GROUDTRUTH>
eg:
bash codefuseEval/script/evaluation.sh codefuseEval/result/test.jsonl pass@k humaneval_python
`,paraId:28,tocIndex:7},{value:"At the same time, we currently provide the following flags, which can directly bring the sample answers in the test data set as generated answers for testing.",paraId:29,tocIndex:7},{value:"TEST_GROUDTRUTH",paraId:30,tocIndex:7},{value:" default False",paraId:30,tocIndex:7},{value:"When TEST_GROUDTRUTH is True, the self-test mode is turned on, PROBLEM_FILE will be read, and the sample answer will be substituted as the generated answer for testing.",paraId:31,tocIndex:7},{value:"When TEST_GROUDTRUTH is False, open the evaluation mode, read RESULT_FILE and PROBLEM_FILE, and substitute the generated answer for testing.",paraId:32,tocIndex:7},{value:"Registry your evaluate dataset.",paraId:33,tocIndex:9},{value:"Download evaluation dataset to store in ",paraId:34,tocIndex:9},{value:"codefuseEval/data",paraId:34,tocIndex:9},{value:" or other directory. Dataset must be jsonl.",paraId:34,tocIndex:9},{value:"Setup information dataset ",paraId:34,tocIndex:9},{value:"EVAL_DATASET",paraId:34,tocIndex:9},{value:",",paraId:34,tocIndex:9},{value:"DATASET_SUPPORT",paraId:34,tocIndex:9},{value:" and ",paraId:34,tocIndex:9},{value:"DATASET_LANGUAGE",paraId:34,tocIndex:9},{value:" in ",paraId:34,tocIndex:9},{value:"codefuseEval/util.py",paraId:34,tocIndex:9},{value:" for dataset path, dataset task_mode and generation code language",paraId:34,tocIndex:9},{value:"Registry your evaluate model.",paraId:35,tocIndex:9},{value:"Download evaluation model to store in ",paraId:36,tocIndex:9},{value:"codefuseEval/model",paraId:36,tocIndex:9},{value:" or other directory.",paraId:36,tocIndex:9},{value:"Write your evaluation model processor code in ",paraId:36,tocIndex:9},{value:"codefuseEval/processor",paraId:36,tocIndex:9},{value:" package.",paraId:36,tocIndex:9},{value:"We designed an infrastructure called Processor. Its main purpose is to handle the differences between different models. It mainly needs to complete three abstract functions:",paraId:37,tocIndex:9},{value:"load_model_tokenizer",paraId:38,tocIndex:9},{value:":Due to differences in model loading parameters and tokenizer terminators, models need to use different parameters for adaptation and loading. The current function is mainly to help users load and adapt different models.",paraId:38,tocIndex:9},{value:"process_before",paraId:38,tocIndex:9},{value:": Since prompt adapts to different prompt styles according to different types of evaluation tasks or different models selected by users, the \u300Cprocess_before\u300Dfunction is extracted mainly to help users process prompts.",paraId:38,tocIndex:9},{value:"process_after",paraId:38,tocIndex:9},{value:":Due to the diversity of model generation results, in order to adapt to the evaluation framework, the generated result data can be spliced into appropriate use cases for automated operation. The current function mainly processes the generated results to adapt to the evaluation data set and results based on the task type and data set conditions.",paraId:38,tocIndex:9},{value:"You can extend the ",paraId:39,tocIndex:9},{value:"BaseProcessor",paraId:39,tocIndex:9},{value:" in ",paraId:39,tocIndex:9},{value:"codefuseEval/processor/base.py",paraId:39,tocIndex:9},{value:" and implement above functions",paraId:39,tocIndex:9},{value:"Setup information model in ",paraId:40,tocIndex:9},{value:"ckpt_config.json",paraId:40,tocIndex:9},{value:". For Example as follow",paraId:40,tocIndex:9},{value:`{
  "CodeFuse-13B": {     //model name
    "v1": {             //model version
      "path": "/mnt/model/CodeFuse13B-evol-instruction-4K/",       // model path
      "processor_class": "codefuseEval.process.codefuse13b.Codefuse13BProcessor",  // model processor
      "tokenizer": {                 // tokenizer params to token input string.
        "truncation": true,
        "padding": true,
        "max_length": 600
      },
      "generation_config": {        //generation config params.
        "greedy": {                 //If JsonObject, it is a decode mode, you can set \u300Cdecode_mode\u300Dparam to load params defined in the decode_mode.
          "do_sample": false,
          "num_beams": 1,
          "max_new_tokens": 512
        },
        "beams": {
          "do_sample": false,
          "num_beams": 5,
          "max_new_tokens": 600,
          "num_return_sequences": 1
        },
        "dosample": {
          "da_sample": true
        },
        "temperature": 0.2,          //If not JsonObject, it is a default param, we will set in generation_config default. You can cover param in decode_mode same name param.
        "max_new_tokens": 600,
        "num_return_sequences": 1,
        "top_p": 0.9,
        "num_beams": 1,
        "do_sample": true
      },
      "batch_size": 1,            // batch size for generate
      "sample_num": 1,            // The number of samples generated by a single piece of data
      "decode_mode": "beams"      // choose decode mode defined in generation_config
    }
  }
`,paraId:41,tocIndex:9},{value:`To check whether the reference values provided by the evaluation data set are correct,
we provide the following command to check the dataset.`,paraId:42,tocIndex:10},{value:"CodeCompletion",paraId:43,tocIndex:10},{value:`bash codefuseEval/script/check_dataset.sh humaneval_python

bash codefuseEval/script/check_dataset.sh humaneval_java

bash codefuseEval/script/check_dataset.sh humaneval_js

bash codefuseEval/script/check_dataset.sh humaneval_rust

bash codefuseEval/script/check_dataset.sh humaneval_go

bash codefuseEval/script/check_dataset.sh humaneval_cpp
`,paraId:44,tocIndex:10},{value:"NL2Code",paraId:45,tocIndex:10},{value:`bash codefuseEval/script/check_dataset.sh mbpp
`,paraId:46,tocIndex:10},{value:"CodeTrans",paraId:47,tocIndex:10},{value:`bash codefuseEval/script/check_dataset.sh codeTrans_python_to_java

bash codefuseEval/script/check_dataset.sh codeTrans_python_to_cpp

bash codefuseEval/script/check_dataset.sh codeTrans_cpp_to_java

bash codefuseEval/script/check_dataset.sh codeTrans_cpp_to_python

bash codefuseEval/script/check_dataset.sh codeTrans_java_to_python

bash codefuseEval/script/check_dataset.sh codeTrans_java_to_cpp
`,paraId:48,tocIndex:10},{value:"CodeScience",paraId:49,tocIndex:10},{value:`bash codefuseEval/script/check_dataset.sh codeCompletion_matplotlib

bash codefuseEval/script/check_dataset.sh codeCompletion_numpy

bash codefuseEval/script/check_dataset.sh codeCompletion_pandas

bash codefuseEval/script/check_dataset.sh codeCompletion_pytorch

bash codefuseEval/script/check_dataset.sh codeCompletion_scipy

bash codefuseEval/script/check_dataset.sh codeCompletion_sklearn

bash codefuseEval/script/check_dataset.sh codeCompletion_tensorflow

bash codefuseEval/script/check_dataset.sh codeInsertion_matplotlib

bash codefuseEval/script/check_dataset.sh codeInsertion_numpy

bash codefuseEval/script/check_dataset.sh codeInsertion_pandas

bash codefuseEval/script/check_dataset.sh codeInsertion_pytorch

bash codefuseEval/script/check_dataset.sh codeInsertion_scipy

bash codefuseEval/script/check_dataset.sh codeInsertion_sklearn

bash codefuseEval/script/check_dataset.sh codeInsertion_tensorflow
`,paraId:50,tocIndex:10}]},71464:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:`
    \u{1F917} `,paraId:0},{value:`HuggingFace
    `,paraId:0},{value:"\u2022 \u{1F916}",paraId:0},{value:` ModelScope
    `,paraId:0},{value:"News",paraId:1,tocIndex:0},{value:"Articles",paraId:2,tocIndex:0},{value:"Introduction",paraId:3,tocIndex:0},{value:"Requirements",paraId:4,tocIndex:0},{value:"Training",paraId:5,tocIndex:0},{value:"Models",paraId:6,tocIndex:0},{value:"Datasets",paraId:7,tocIndex:0},{value:"Star History",paraId:8,tocIndex:0},{value:"\u{1F525}\u{1F525}\u{1F525} [2024/01/17] We released MFTCoder v0.3.0, mainly for MFTCoder-accelerate. It now supports new models like Mixtral(MoE), DeepSeek-coder, chatglm3. It supports FSDP as an option. It also supports Self-paced Loss as a solution for convergence balance in Multitask Fine-tuning.",paraId:9,tocIndex:1},{value:"\u{1F525}\u{1F525}\u{1F525} [2024/01/17] ",paraId:10,tocIndex:1},{value:"CodeFuse-DeepSeek-33B",paraId:10,tocIndex:1},{value:" has been released, achieving a pass@1 (greedy decoding) score of 78.7% on HumanEval. It lists as top-1 LLM on Bigcode Leardboard in terms of win-rate, the official result is going to be published later.",paraId:10,tocIndex:1},{value:"\u{1F525}\u{1F525}\u{1F525} [2024/01/17] ",paraId:11,tocIndex:1},{value:"CodeFuse-Mixtral-8x7B",paraId:11,tocIndex:1},{value:" has been released, achieving a pass@1 (greedy decoding) score of 56.1% on HumanEval.",paraId:11,tocIndex:1},{value:"\u{1F525}\u{1F525} [2023/11/07] ",paraId:12,tocIndex:1},{value:"MFTCoder Paper",paraId:12,tocIndex:1},{value:" has been released on Arxiv, which discloses technique details of multi-task-fine-tuning.",paraId:12,tocIndex:1},{value:"\u{1F525}\u{1F525} [2023/10/20] ",paraId:13,tocIndex:1},{value:"CodeFuse-QWen-14B",paraId:13,tocIndex:1},{value:" has been released, achieving a pass@1 (greedy decoding) score of 48.8% on HumanEval, which gains 16% absolute improvement over the base model ",paraId:13,tocIndex:1},{value:"Qwen-14b",paraId:13,tocIndex:1},{value:"\u{1F525}\u{1F525} [2023/09/27] ",paraId:14,tocIndex:1},{value:"CodeFuse-StarCoder-15B",paraId:14,tocIndex:1},{value:" has been released, achieving a pass@1 (greedy decoding) score of 54.9% on HumanEval.",paraId:14,tocIndex:1},{value:"\u{1F525}\u{1F525} [2023/09/26]We are pleased to announce the release of the ",paraId:15,tocIndex:1},{value:"4-bit quantized version of CodeFuse-CodeLlama-34B",paraId:15,tocIndex:1},{value:". Despite the quantization process, the model still achieves a remarkable 73.8% accuracy (greedy decoding) on the HumanEval pass@1 metric.",paraId:15,tocIndex:1},{value:"\u{1F525}\u{1F525} [2023/09/07]We released ",paraId:16,tocIndex:1},{value:"CodeFuse-CodeLlama-34B",paraId:16,tocIndex:1},{value:", which achieves the ",paraId:16,tocIndex:1},{value:"74.4% Python Pass@1",paraId:16,tocIndex:1},{value:" (greedy decoding) and surpasses GPT4 (2023/03/15) and ChatGPT-3.5 on the ",paraId:16,tocIndex:1},{value:"HumanEval Benchmarks",paraId:16,tocIndex:1},{value:".",paraId:16,tocIndex:1},{value:"\u{1F525}\u{1F525} [2023/08/26]We released MFTCoder-v0.1.0 which supports finetuning Code Llama, Llama, Llama2, StarCoder, ChatGLM2, CodeGeeX2, Qwen, and GPT-NeoX models with LoRA/QLoRA.",paraId:17,tocIndex:1},{value:"Model",paraId:18,tocIndex:2},{value:"HumanEval(Pass@1)",paraId:18,tocIndex:2},{value:"Date",paraId:18,tocIndex:2},{value:"CodeFuse-DeepSeek-33B",paraId:18,tocIndex:2},{value:"78.7%",paraId:18,tocIndex:2},{value:"2024/01",paraId:18,tocIndex:2},{value:"CodeFuse-CodeLlama-34B",paraId:18,tocIndex:2},{value:"74.4%",paraId:18,tocIndex:2},{value:"2023/09",paraId:18,tocIndex:2},{value:"CodeFuse-CodeLlama-34B-4bits",paraId:18,tocIndex:2},{value:"73.8%",paraId:18,tocIndex:2},{value:"2023/09",paraId:18,tocIndex:2},{value:"WizardCoder-Python-34B-V1.0",paraId:18,tocIndex:2},{value:"73.2%",paraId:18,tocIndex:2},{value:"2023/08",paraId:18,tocIndex:2},{value:"GPT-4(zero-shot)",paraId:18,tocIndex:2},{value:"67.0%",paraId:18,tocIndex:2},{value:"2023/03",paraId:18,tocIndex:2},{value:"PanGu-Coder2 15B",paraId:18,tocIndex:2},{value:"61.6%",paraId:18,tocIndex:2},{value:"2023/08",paraId:18,tocIndex:2},{value:"CodeFuse-Mixtral-8x7B",paraId:18,tocIndex:2},{value:"56.1%",paraId:18,tocIndex:2},{value:"2024/01",paraId:18,tocIndex:2},{value:"CodeFuse-StarCoder-15B",paraId:18,tocIndex:2},{value:"54.9%",paraId:18,tocIndex:2},{value:"2023/08",paraId:18,tocIndex:2},{value:"CodeLlama-34b-Python",paraId:18,tocIndex:2},{value:"53.7%",paraId:18,tocIndex:2},{value:"2023/08",paraId:18,tocIndex:2},{value:"CodeFuse-QWen-14B",paraId:18,tocIndex:2},{value:"48.8%",paraId:18,tocIndex:2},{value:"2023/10",paraId:18,tocIndex:2},{value:"CodeLlama-34b",paraId:18,tocIndex:2},{value:"48.8%",paraId:18,tocIndex:2},{value:"2023/08",paraId:18,tocIndex:2},{value:"GPT-3.5(zero-shot)",paraId:18,tocIndex:2},{value:"48.1%",paraId:18,tocIndex:2},{value:"2022/11",paraId:18,tocIndex:2},{value:"OctoCoder",paraId:18,tocIndex:2},{value:"46.2%",paraId:18,tocIndex:2},{value:"2023/08",paraId:18,tocIndex:2},{value:"StarCoder-15B",paraId:18,tocIndex:2},{value:"33.6%",paraId:18,tocIndex:2},{value:"2023/05",paraId:18,tocIndex:2},{value:"QWen-14B",paraId:18,tocIndex:2},{value:"32.3%",paraId:18,tocIndex:2},{value:"2023/10",paraId:18,tocIndex:2},{value:"MFT Arxiv paper",paraId:19,tocIndex:3},{value:"High Accuracy and efficiency Multi-task Fine-tuning framework for Code LLMs.",paraId:20,tocIndex:4},{value:"MFTCoder",paraId:21,tocIndex:4},{value:` is an open-source project of CodeFuse for accurate and efficient Multi-task Fine-tuning(MFT) on Large Language Models(LLMs), especially on Code-LLMs(large language model for code tasks).
Moreover, we open source Code LLM models and code-related datasets along with the MFTCoder framework.`,paraId:21,tocIndex:4},{value:"In MFTCoder, we released two codebases for finetuning Large Language Models:",paraId:22,tocIndex:4},{value:"MFTCoder-accelerate",paraId:23,tocIndex:4},{value:" is a framework with accelerate and DeepSpeed/FSDP. All tech-stacks are open-source and vibrant. We highly recommend you try this framework and make your fintuning accurate and efficient.",paraId:23,tocIndex:4},{value:"MFTCoder-atorch",paraId:23,tocIndex:4},{value:" is based on the ",paraId:23,tocIndex:4},{value:"ATorch frameworks",paraId:23,tocIndex:4},{value:", which is a fast distributed training framework of LLM.",paraId:23,tocIndex:4},{value:"The aim of this project is to foster collaboration and share advancements in large language models, particularly within the domain of code development.",paraId:24,tocIndex:4},{value:":white_check_mark: ",paraId:25,tocIndex:6},{value:"Multi-task",paraId:25,tocIndex:6},{value:": Train models on multiple tasks while maintaining a balance between them. The models can even generalize to new, previously unseen tasks.",paraId:25,tocIndex:6},{value:":white_check_mark: ",paraId:26,tocIndex:6},{value:"Multi-model",paraId:26,tocIndex:6},{value:": It integrates state-of-the-art open-source models such as gpt-neox, llama, llama-2, baichuan, Qwen, chatglm2, and more. (These finetuned models will be released in the near future.)",paraId:26,tocIndex:6},{value:":white_check_mark: ",paraId:27,tocIndex:6},{value:"Multi-framework",paraId:27,tocIndex:6},{value:": It provides support for both Accelerate (with Deepspeed and FSDP) and ATorch",paraId:27,tocIndex:6},{value:":white_check_mark: ",paraId:28,tocIndex:6},{value:"Efficient fine-tuning",paraId:28,tocIndex:6},{value:": It supports LoRA, QLoRA as well as Full-parameters training, enabling fine-tuning of large models with minimal resources. The training speed meets the demands of almost all fine-tuning scenarios.",paraId:28,tocIndex:6},{value:"The main components of this project include:",paraId:29,tocIndex:6},{value:"Support for both SFT (Supervised FineTuning) and MFT (Multi-task FineTuning). The current MFTCoder achieves data balance among multiple tasks, and future releases will achieve a balance between task difficulty and convergence speed during training.",paraId:30,tocIndex:6},{value:"Support for QLoRA instruction fine-tuning, LoRA fine-tuning as well as Full-parameters fine-tuning.",paraId:30,tocIndex:6},{value:"Support for most mainstream open-source large models, particularly those relevant to Code-LLMs, such as DeepSeek-coder, Mistral, Mixtral, Chatglm3, Code-LLaMA, Starcoder, Codegeex2, Qwen, GPT-Neox, and more.",paraId:30,tocIndex:6},{value:"Support for weight merging between the LoRA adaptor and base models, simplifying the inference process.",paraId:30,tocIndex:6},{value:"Release of 2 high-quality code-related instruction fine-tuning datasets: ",paraId:30,tocIndex:6},{value:"Evol-instruction-66k",paraId:30,tocIndex:6},{value:" and ",paraId:30,tocIndex:6},{value:"CodeExercise-Python-27k",paraId:30,tocIndex:6},{value:".",paraId:30,tocIndex:6},{value:"Release of many Code LLMs, please refer to organizations: ",paraId:30,tocIndex:6},{value:"codefuse-ai on huggingface",paraId:30,tocIndex:6},{value:" or ",paraId:30,tocIndex:6},{value:"codefuse-ai on modelscope",paraId:30,tocIndex:6},{value:".",paraId:30,tocIndex:6},{value:"Contributions are welcome! If you have any suggestions, ideas, bug reports, or new model/feature supported, please open an issue or submit a pull request.",paraId:31,tocIndex:7},{value:"If you find our work useful or helpful for your R&D works, please feel free to cite our paper as below.",paraId:32,tocIndex:8},{value:`@article{mftcoder2023,
      title={MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning},
      author={Bingchang Liu and Chaoyu Chen and Cong Liao and Zi Gong and Huan Wang and Zhichao Lei and Ming Liang and Dajun Chen and Min Shen and Hailian Zhou and Hang Yu and Jianguo Li},
      year={2023},
      journal={arXiv preprint arXiv},
      archivePrefix={arXiv},
      eprint={2311.02303}
}
`,paraId:33,tocIndex:8}]},85125:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:`\xA0
`,paraId:0},{value:"\u{1F525} MFTCoder-accelerate supports Full-parameters/LoRA using accelerate + FSDP Framework;",paraId:1,tocIndex:0},{value:"\u{1F525} MFTCoder-accelerate supports MFT/SFT on more new mainstream open-source base models: mistral, mixtral-8x7b(Mixture of Experts), deepseek, chatglm3;",paraId:2,tocIndex:0},{value:"\u{1F525} MFTCoder-accelerate supports Self-Paced Loss for Convergence Balance;",paraId:3,tocIndex:0},{value:"\u{1F525} MFTCoder-accelerate supports Full-parameters/QLoRA/LoRA using accelerate + DeepSpeed Framework;",paraId:4,tocIndex:0},{value:"\u{1F525} MFTCoder-accelerate supports Multitask Fine-Tuning(MFT), which is able to balance diffenrent tasks in data level.",paraId:5,tocIndex:0},{value:"\u{1F525} MFTCoder-accelerate supports finetuning most of mainstream open-source base models: codellama, llama2, llama, starcoder, codegeex2, chatglm2, qwen.",paraId:6,tocIndex:0},{value:`The training data is required to be a uniformed JSONL format, in which each line of data has the following "chatML"-style JSON format. The "chat_rounds" field is required, and other fields can be added or removed based on specific needs.
The reason why we selected "chatML" style as our training and inference data format is that "chatML" style is compatible with both "conversation" and "instruction/response" scenarios.`,paraId:7,tocIndex:2},{value:'For the keys of roles in "chat_rounds", you could use "system/human/bot" tuple or "system/user/assistant" tuple.',paraId:8,tocIndex:2},{value:`{
  "id": 0,
  "data_name": "code-helper",
  "chat_rounds": [
    {
      "role": "system",
      "content": "You are a expert in coding and help answer code questions"
    },
    {
      "role": "human",
      "content": "Write a python function of quick sort"
    },
    {
      "role": "bot",
      "content": "Below is the function of quick sort: ..."
    },
    {
      "role": "human",
      "content": "Explain the code"
    },
    {
      "role": "bot",
      "content": "OK, this code ..."
    }
  ]
}
`,paraId:9,tocIndex:2},{value:`Inference data format is the real string format consumed by tokenizers and then LLMs. It is also the string format to which the training data is converted before tokenization.
The default inference data format contains strings concatenated by conversation data(system, human and bot contents) in the training data format.
It is used as the data "seen"(before tokenization) by the model in training process.
It is used as input during the inference process as well.
Here is an example format of the inference string:`,paraId:10,tocIndex:3},{value:`"""
<s>system
System instruction
<s>human
User 1st round input
<s>bot
Assistant 1st round output{EOS_TOKEN}
<s>human
User 2nd round input
<s>bot
Assistant 2nd round output{EOS_TOKEN}
...
...
...
<s>human
User nth round input
<s>bot
{Assistant output to be genreated}{EOS_TOKEN}
"""
`,paraId:11,tocIndex:3},{value:"When applying inference, you always make your input string end with ",paraId:12,tocIndex:3},{value:"<s>bot\\n",paraId:12,tocIndex:3},{value:" to request the model generating answers.",paraId:12,tocIndex:3},{value:`Currently, the "MFTCoder-accelerate" codebase supports Full-parameters/LoRA/QLoR along with Multi-Task FineTuning(MFT).
In theory, this project can be used to train any publicly available model in the HuggingFace Format.`,paraId:13,tocIndex:4},{value:"Here are some excellent pre-trained models weights available on Huggingface that can be finetuned with this codebase:",paraId:14,tocIndex:4},{value:"\u{1F917} ",paraId:15,tocIndex:4},{value:"Latest code pre-trained SOTA, CodeLlama-34b-Python",paraId:15,tocIndex:4},{value:" : code-llama-34b, code-llama-34b-python, a new SOTA base model.",paraId:15,tocIndex:4},{value:"\u{1F917} ",paraId:16,tocIndex:4},{value:"Best 10B level pre-trained Code LLM, Starcoder:",paraId:16,tocIndex:4},{value:" wizardCoder-15B, PanGu-coder2, and other previous SOTA were trained on it.",paraId:16,tocIndex:4},{value:"\u{1F917} ",paraId:17,tocIndex:4},{value:"Multilingual powerhouse, Qwen-7b",paraId:17,tocIndex:4},{value:": Suitable for multilingual tasks, including Chinese tasks, for instruction fine-tuning.",paraId:17,tocIndex:4},{value:"mftcoder_accelerate directory structure",paraId:18,tocIndex:4},{value:`mftcoder_accelerate
       |
       src
          configs
          |
          data
          |
          model
          |
          *pefts*
          |
          tokenizer
          |
          utils
       |
       evals
`,paraId:19,tocIndex:4},{value:"\u6211\u4EEC\u5C06\u8BAD\u7EC3\u4E2D\u4F7F\u7528\u7684\u5404\u79CD\u7EC4\u4EF6\u62BD\u53D6\u51FA\u6765\uFF0C\u4EE5\u4FBF\u540E\u7EED\u7684\u6269\u5C55\u548C\u4F18\u5316\uFF0C \u8BE6\u89C1",paraId:20,tocIndex:4},{value:"src",paraId:20,tocIndex:4},{value:"\u76EE\u5F55\u4E0B\u7684\u5B9E\u73B0\u3002",paraId:20,tocIndex:4},{value:"\u8BAD\u7EC3\u5165\u53E3\u6587\u4EF6\u662F",paraId:21,tocIndex:4},{value:"mftcoder_accelerate/src/pefts/mft_accelerate.py",paraId:21,tocIndex:4},{value:"\u53C2\u6570\u914D\u7F6E\u5B58\u50A8\u5728",paraId:22,tocIndex:4},{value:"mftcoder_accelerate/src/configs",paraId:22,tocIndex:4},{value:"\u76EE\u5F55\u4E0B\uFF0C\u65B9\u4FBF\u7EDF\u4E00\u7BA1\u7406\u548C\u66F4\u6539\u3002",paraId:22,tocIndex:4},{value:"\u6240\u4EE5\uFF0C\u5728\u4F60\u5F00\u542F\u8BAD\u7EC3\u4E4B\u524D\uFF0C\u8BF7\u8FDB\u5165 src \u76EE\u5F55",paraId:23,tocIndex:4},{value:`cd mftcoder_accelerate/src
`,paraId:24,tocIndex:4},{value:"You can find the implementations in the ",paraId:25,tocIndex:4},{value:"mftcoder_accelerate/src",paraId:25,tocIndex:4},{value:` directory.
The entry directory for fine-tuning training is `,paraId:25,tocIndex:4},{value:"mftcoder_accelerate/src",paraId:25,tocIndex:4},{value:", and the entry file for training is ",paraId:25,tocIndex:4},{value:"mftcoder_accelerate/src/pefts/mft_accelerate.py",paraId:25,tocIndex:4},{value:`.
Configurations are stored in the `,paraId:25,tocIndex:4},{value:"mftcoder_accelerate/src/configs",paraId:25,tocIndex:4},{value:" directory for easy management and modification.",paraId:25,tocIndex:4},{value:"As a result, before you start training, you should first change your dir by",paraId:26,tocIndex:4},{value:`cd mftcoder_accelerate/src
`,paraId:27,tocIndex:4},{value:"During training, we concatenate multi-turn dialogues into the following format (also known as the inference data format mentioned before) and then tokenize it.",paraId:28,tocIndex:5},{value:"In default format, ",paraId:29,tocIndex:5},{value:"<s>human\\n",paraId:29,tocIndex:5},{value:" starts the user's input (i.e., prompt),",paraId:29,tocIndex:5},{value:"<s>bot\\n",paraId:29,tocIndex:5},{value:" starts the assistant's output (i.e., response)",paraId:29,tocIndex:5},{value:"{EOS_TOKEN}",paraId:30,tocIndex:5},{value:` represents the proper eos_token.
We have different eos_tokens in `,paraId:30,tocIndex:5},{value:"src/pefts/model_mapping.py",paraId:30,tocIndex:5},{value:" which fits different base models.",paraId:30,tocIndex:5},{value:"Here is a visionable example of the training data after formatting:",paraId:31,tocIndex:5},{value:`f"<s>human\\n{input1}<s>bot\\n{target1}{EOS_TOKEN}\\n<s>human\\n{input2}<s>bot\\ntarget2{EOS_TOKEN}\\n"
`,paraId:32,tocIndex:5},{value:"During the calculation of loss, we use a ",paraId:33,tocIndex:5},{value:"loss mask",paraId:33,tocIndex:5},{value:" to ensure that the loss from the input part does not contribute to parameter updates. Only the loss from the ",paraId:33,tocIndex:5},{value:"target{EOS_TOKEN}",paraId:33,tocIndex:5},{value:` part is used for updating parameters.
This approach takes full advantage of the benefits of model parallelism, making training more efficient. It also leverages the characteristic of decoder-only models with left-to-right attention.
By including all target parts from multiple turns in a single training iteration, the training process becomes more efficient.`,paraId:33,tocIndex:5},{value:"You can refer to the Lora paper for details about LoRA\uFF1A",paraId:34,tocIndex:7},{value:"LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS",paraId:34,tocIndex:7},{value:"You can refer to the Qlora paper for details about QLoRA\uFF1A",paraId:35,tocIndex:7},{value:"QLORA: Efficient Finetuning of Quantized LLMs",paraId:35,tocIndex:7},{value:"QLoRA (Quantized LoRA) is a method that combines 4-bit nf4 quantization and additional adapters to achieve a balance between reducing GPU memory consumption and approaching the performance of full-parameter fine-tuning.",paraId:36,tocIndex:7},{value:"According to the QLoRA paper, this method enables fine-tuning of a 33B model on a single V100 GPU while achieving performance close to that of full-parameter fine-tuning.",paraId:37,tocIndex:7},{value:"To perform LoRA/QLoRA fine-tuning, you can execute the following command:",paraId:38,tocIndex:7},{value:"DeepSpeed config in accelerate_ds_config.yaml.",paraId:39,tocIndex:8},{value:`accelerate launch --config_file accelerate_ds_config.yaml pefts/mft_accelerate.py --train_config configs/xxx_train_config.json --distributed_type "DeepSpeed"
`,paraId:40,tocIndex:8},{value:`or
DeepSpeed config in command line arguments`,paraId:41,tocIndex:8},{value:`sh ds_single_launch.sh
`,paraId:42,tocIndex:8},{value:"FSDP config in accelerate_fsdp_config.yaml.",paraId:43,tocIndex:9},{value:`accelerate launch --config_file accelerate_fsdp_config.yaml pefts/mft_accelerate.py --train_config configs/xxx_train_config.json --distributed_type "FSDP"
`,paraId:44,tocIndex:9},{value:`or
FSDP config in command line arguments`,paraId:45,tocIndex:9},{value:`sh ds_single_launch.sh
`,paraId:46,tocIndex:9},{value:"All arguments allowed in ***_train_config.josn are defined in ",paraId:47,tocIndex:10},{value:"arguments.py",paraId:47,tocIndex:10},{value:".",paraId:47,tocIndex:10},{value:"Frequently used arguments are provided in ",paraId:48,tocIndex:10},{value:"configs/***_train_config",paraId:48,tocIndex:10},{value:" and explained as follows. You can modify these parameters according to your needs:",paraId:48,tocIndex:10},{value:"load_raw_dataset",paraId:49,tocIndex:10},{value:": Need to be true at present. Only JSONL format is supported.",paraId:49,tocIndex:10},{value:"data_paths",paraId:50,tocIndex:10},{value:': Input data paths in a String of list format, e.g., "[path1,path2,path3]". Each path represents a task directory and each task directory contains one or more JSONL data files.',paraId:50,tocIndex:10},{value:"output_dir",paraId:51,tocIndex:10},{value:": Training output directory to store checkpoints, Lora adapter, etc.",paraId:51,tocIndex:10},{value:"tb_dir",paraId:52,tocIndex:10},{value:": TensorBoard directory to store logs, metrics, etc.",paraId:52,tocIndex:10},{value:"model_type",paraId:53,tocIndex:10},{value:': Type of the model to train, e.g., "mixtral | llama | starcoder | chatglm2 | qwen | gpt_neox".',paraId:53,tocIndex:10},{value:"attn_implementation",paraId:54,tocIndex:10},{value:': "flash_attention_2" or "eager" or "sdpa", worked when model is supported by transformers officially',paraId:54,tocIndex:10},{value:"peft_type",paraId:55,tocIndex:10},{value:': null or "lora" or "qlora". null for full-params training',paraId:55,tocIndex:10},{value:"lora_rank",paraId:56,tocIndex:10},{value:": Rank value for Lora.",paraId:56,tocIndex:10},{value:"lora_alpha",paraId:57,tocIndex:10},{value:": Alpha value for Lora.",paraId:57,tocIndex:10},{value:"lora_dropout",paraId:58,tocIndex:10},{value:": Dropout rate for Lora.",paraId:58,tocIndex:10},{value:"target_modules",paraId:59,tocIndex:10},{value:": List of target modules in lora, we have default values if None",paraId:59,tocIndex:10},{value:"quantization",paraId:60,tocIndex:10},{value:': "4bit" for QLoRA/ null for LoRA and Full-params training.',paraId:60,tocIndex:10},{value:"pretrained_model_path",paraId:61,tocIndex:10},{value:": Local/Shared disk path or model name on HuggingFace for the pre-trained model.",paraId:61,tocIndex:10},{value:"weighted_loss_mode",paraId:62,tocIndex:10},{value:': Loss weighting method for multitask training. "case3" is recommended at present, "self-paced" is supported but need tuning of hyperparameters.',paraId:62,tocIndex:10},{value:"padding_mode",paraId:63,tocIndex:10},{value:': The way tokenized data is set. "padding" means padding for each sample to seq_length, "pack" means putting samples into seq_length as many as possible.',paraId:63,tocIndex:10},{value:"num_train_epochs",paraId:64,tocIndex:10},{value:": Number of training epochs.",paraId:64,tocIndex:10},{value:"per_device_train_batch_size",paraId:65,tocIndex:10},{value:": Batch size per GPU for training.",paraId:65,tocIndex:10},{value:"per_device_eval_batch_size",paraId:66,tocIndex:10},{value:": Batch size per GPU for evaluation.",paraId:66,tocIndex:10},{value:"gradient_accumulation_steps",paraId:67,tocIndex:10},{value:": Number of gradient accumulation steps. Global batch size is calculated as num",paraId:67,tocIndex:10},{value:"gpus * per",paraId:67,tocIndex:10},{value:"device_train_batch_size * gradient_accumulation_steps.",paraId:67,tocIndex:10},{value:"learning_rate",paraId:68,tocIndex:10},{value:": Initial Learning rate. For full-parameter fine-tuning, it is recommended to use a smaller value such as 1e-5 or 5e-6. For QLoRA, a larger learning rate is generally used, such as 1e-4 or 2e-4.",paraId:68,tocIndex:10},{value:"min_lr",paraId:69,tocIndex:10},{value:": Minimum learning rate. Usually set to one-tenth of the learning rate.",paraId:69,tocIndex:10},{value:"seq_length",paraId:70,tocIndex:10},{value:": Maximum input sequence length during training.",paraId:70,tocIndex:10},{value:"log_interval",paraId:71,tocIndex:10},{value:": Log training loss every ",paraId:71,tocIndex:10},{value:"log_interval",paraId:71,tocIndex:10},{value:" steps.",paraId:71,tocIndex:10},{value:"checkpointing_steps",paraId:72,tocIndex:10},{value:": Save a checkpoint every ",paraId:72,tocIndex:10},{value:"checkpointing_steps",paraId:72,tocIndex:10},{value:" steps.",paraId:72,tocIndex:10},{value:"evaluation_steps",paraId:73,tocIndex:10},{value:": Evaluate on the validation set every ",paraId:73,tocIndex:10},{value:"evaluation_steps",paraId:73,tocIndex:10},{value:" steps.",paraId:73,tocIndex:10},{value:"early_stopping",paraId:74,tocIndex:10},{value:": Enable early stopping or not.",paraId:74,tocIndex:10},{value:"early_stopping_stall_num",paraId:75,tocIndex:10},{value:": Number of evaluation points without improvement which triggers early stopping.",paraId:75,tocIndex:10},{value:"lr_scheduler_type",paraId:76,tocIndex:10},{value:': Type of learning rate scheduler. "cosine" is a good choice already.',paraId:76,tocIndex:10},{value:"num_warmup_steps",paraId:77,tocIndex:10},{value:": Number of warm-up steps to gradually increase the learning rate.",paraId:77,tocIndex:10},{value:"seed",paraId:78,tocIndex:10},{value:": Random seed for reproducibility.",paraId:78,tocIndex:10},{value:"saving_limit",paraId:79,tocIndex:10},{value:": ckpt saving limit num, must be set in Full-parameter training.",paraId:79,tocIndex:10},{value:"role_markers",paraId:80,tocIndex:10},{value:': {"system": "<s>system\\n", "user": "<s>human\\n", "assistant": "<s>bot\\n} as default(null). You could set your preferred role_markers as the templates startting "system", "user" and "assistant". e.g. {"system": "### System:\\n", "user": "### Instruction:\\n", "assistant": "### Response:\\n"}',paraId:80,tocIndex:10},{value:`Using LoRA or QLoRA for training, this project only saves the weights and configuration files of the adapters.
To merge the adapter weights with the base model:`,paraId:81,tocIndex:12},{value:`python pefts/merge_base_and_lora_to_hf.py \\
    --base_model_or_path model_path \\
    --adaptor_path lora_adapter_path \\
    --model_type model_type \\
    --merged_output_path output_path
`,paraId:82,tocIndex:12},{value:"Here is the script for inference on models trained by MFTCoder since v0.3.0, which is compatible with most HuggingFace models:",paraId:83,tocIndex:13},{value:`from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
)
model_name_or_path = "codefuse-ai/CodeFuse-Deepseek-33B"
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True, padding_side="left")
tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids("<\uFF5Cend\u2581of\u2581sentence\uFF5C>")
tokenizer.pad_token_id = tokenizer.eos_token_id
model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True)

HUMAN_ROLE_START_TAG = "<s>human\\n"
BOT_ROLE_START_TAG = "<s>bot\\n"
texts = ["write a python function of quick sort."]
texts = [f"{HUMAN_ROLE_START_TAG}{text}{BOT_ROLE_START_TAG}" for text in texts]

inputs = tokenizer(texts, return_tensors='pt', padding=True, add_special_tokens=False).to("cuda")
outputs = model.generate(
        inputs=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=512,
        top_p=0.95,
        temperature=0.1,
        do_sample=True,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id
    )
gen_text = tokenizer.batch_decode(outputs[:, inputs["input_ids"].shape[1]:], skip_special_tokens=True)
print(gen_text)
`,paraId:84,tocIndex:13},{value:`Indeed, the parameters top_p, temperature, repetition_penalty, do_sample, etc., have a significant impact on the model's generation output.
You can modify these parameters based on your specific use case.`,paraId:85,tocIndex:13},{value:"In code generation scenarios, if you are using the sampling mode (do_sample=True), the following parameter settings can yield good results for the Pass@1 metric:",paraId:86,tocIndex:13},{value:"top_p: Set a higher value, such as 0.95, to retain highly probable generated words. This helps ensure more accurate and fluent generation results.",paraId:87,tocIndex:13},{value:"temperature: Set a lower value, such as 0.1, to reduce randomness. Lower temperature values make the generation output more deterministic.",paraId:88,tocIndex:13},{value:"These parameter combinations can control the diversity of the generated outputs while maintaining naturalness. Additionally, you can adjust other related parameters, such as repetition_penalty, to reduce repetition in the generated results.",paraId:89,tocIndex:13},{value:"If you choose the non-sampling mode (do_sample=False), you can consider the following parameter settings:",paraId:90,tocIndex:13},{value:"beam_num: Set a smaller value such as 1 or 3. ",paraId:91,tocIndex:13},{value:"beam_num=1",paraId:91,tocIndex:13},{value:" represents greedy decoding, which selects the most probable single generated word. ",paraId:91,tocIndex:13},{value:"beam_num=3",paraId:91,tocIndex:13},{value:" represents beam search mode, which considers multiple potential generation paths and chooses the best path among them.",paraId:91,tocIndex:13},{value:`If OOM happened\uFF0Cyou can reduce parameters such as per_device_train_batch_size and seq_length. Since you are dealing with large models (6B, 13B, 34B, 70B, etc.), you are already using gradient checkpointing technology by default, which significantly reduces GPU memory consumption.
However, this may slightly slow down the training speed.`,paraId:92,tocIndex:15},{value:`Please refer to init_env.sh and requirements.txt
We highly recommend you install Flash Attention 2 (flash_attn>=2.1.0, 2.3.6 used by us) first to get memory-efficient and fast training.`,paraId:93,tocIndex:16},{value:"You can specify the visiable GPUs as below:",paraId:94,tocIndex:17},{value:`CUDA_VISIBLE_DEVICES=0,1 accelerate launch --config_file accelerate_ds_config.yaml pefts/mft_accelerate.py --train_config configs/xxx_train_config.json
`,paraId:95,tocIndex:17},{value:`For LoRA/QLoRA, we recommend DeepSpeed(ZeRO2) as the underlying framework, because it is easy and stable to use, moreover it is more compatable for different settings.
And FSDP does not support Quantization(integer type in training).`,paraId:96,tocIndex:18},{value:"For Full-parameter finetuning, FSDP is usually faster, and may help you with very large models by sharding parameters and gradients.",paraId:97,tocIndex:18}]},79143:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:`\xA0
`,paraId:0},{value:"\u{1F525} MFTCoder supports fine-tuning of the GPTNeoX model under the Atorch framework.",paraId:1,tocIndex:0},{value:"\u{1F525} MFTCoder supports both fully supervised fine-tuning.",paraId:2,tocIndex:0},{value:"\u{1F525} MFTCoder supports LoRA using the Atorch Framework.",paraId:3,tocIndex:0},{value:'The training data is in a uniformed JSONL format, in which each line of data has the following JSON format. The "chat_rounds" field is required, and other fields can be added or removed based on the specific need.',paraId:4,tocIndex:2},{value:`{
  "id": 0,
  "data_name": "code-helper",
  "chat_rounds": [
    {
      "role": "system",
      "content": "You are a expert in coding and help answer code questions",
      "chat_round_id": 0
    },
    {
      "role": "human",
      "content": "Write a python function of quick sort",
      "chat_round_id": 1
    },
    {
      "role": "bot",
      "content": "Below is the function of quick sort: ...",
      "chat_round_id": 1
    },
    {
      "role": "human",
      "content": "Explain the code",
      "chat_round_id": 2
    },
    {
      "role": "bot",
      "content": "OK, this code ...",
      "chat_round_id": 2
    }
  ]
}
`,paraId:5,tocIndex:2},{value:`The inference data contains strings concatenated by conversation data(system, human and bot contents) in the training data format.
It is used as the data "seen"(before tokenization) by the model in training process.
It is used as input during the inference process as well.
Here is an example format of the concatenated string:`,paraId:6,tocIndex:3},{value:`"""
<|role_start|>system<|role_end|>System instruction
<|role_start|>human<|role_end|>Human 1st round input
<|role_start|>bot<|role_end|>Bot 1st round output</s>
<|role_start|>human<|role_end|>Human 2nd round input
<|role_start|>bot<|role_end|>Bot 2nd round output</s>
...
...
...
<|role_start|>human<|role_end|>Human nth round input
<|role_start|>bot<|role_end|>{Bot output to be genreated}</s>
"""
`,paraId:7,tocIndex:3},{value:'When applying inference, you always make your input string end with "<|role_start|>bot<|role_end|>" to request the model generating answers.',paraId:8,tocIndex:3},{value:'Currently, the "MFTCoder/mft_atorch" code repository supports fully instruction fine-tuning, and LoRA instruction fine-tuning. Only the training of the GPTNeoX model is supported. In theory, the pretrained weights of the GPTNeoX model available on HuggingFace can be used for training within this project.',paraId:9,tocIndex:4},{value:"We have extracted various components used in training to facilitate future extension and optimization. Please refer to the implementation in the main directory for more details. The entry directory for fine-tuning training is ",paraId:10,tocIndex:4},{value:"train/",paraId:10,tocIndex:4},{value:", and the entry file for training is ",paraId:10,tocIndex:4},{value:"train/run_train.py",paraId:10,tocIndex:4},{value:". The parameter configurations are stored in the launch scripts such as ",paraId:10,tocIndex:4},{value:"train/run_gpt_*.sh",paraId:10,tocIndex:4},{value:", making it easier to manage and modify them uniformly.",paraId:10,tocIndex:4},{value:`During training, we concatenate multi-turn dialogues into the following format (also known as the inference data format mentioned earlier) and then tokenize it. In this format, <|role_start|>human<|role_end|> represents the human input (i.e., prompt), <|role_start|>bot<|role_end|> represents the bot output, and  represents the eos_token.
You can modify and replace the eos_token based on different models' requirements.`,paraId:11,tocIndex:5},{value:"Here is an example of the concatenated format with prompts:",paraId:12,tocIndex:5},{value:`"<|role_start|>human<|role_end|>input1</s>target1</s>input2</s>target2</s>...
`,paraId:13,tocIndex:5},{value:"During the calculation of loss, we use a ",paraId:14,tocIndex:5},{value:"loss mask",paraId:14,tocIndex:5},{value:" to ensure that the loss from the input part does not contribute to the parameter updates. Only the loss from the ",paraId:14,tocIndex:5},{value:"target</s>",paraId:14,tocIndex:5},{value:` part is used for updating parameters.
This approach takes full advantage of the benefits of model parallelism, making training more efficient. It also leverages the characteristic of decoder-only models with left-to-right attention.
By including all target parts from multiple turns in a single training iteration, the training process becomes more efficient.`,paraId:14,tocIndex:5},{value:"To perform fully SFT, you can execute the following command:",paraId:15,tocIndex:6},{value:`sh run_gpt_mft.sh 10 1 8 5
`,paraId:16,tocIndex:6},{value:"Please note that the four parameters after the launch script have the following meanings:",paraId:17,tocIndex:6},{value:"The first parameter is the per GPU batch size.",paraId:18,tocIndex:6},{value:"The second parameter is the number of tensor parallelism (currently only supports 1).",paraId:18,tocIndex:6},{value:"The third parameter is the number of data parallelism, which should match the number of GPUs used.",paraId:18,tocIndex:6},{value:"The fourth parameter is the number of training epochs.",paraId:18,tocIndex:6},{value:"For other training modes, the same four parameters need to be configured in the launch script.",paraId:19,tocIndex:6},{value:"To perform LoRA SFT, you can execute the following command:",paraId:20,tocIndex:7},{value:`sh run_gpt_mft_peft.sh 10 1 8 5
`,paraId:21,tocIndex:7},{value:"The main parameter explanations for the ",paraId:22,tocIndex:8},{value:"train/run_gpt_*.sh",paraId:22,tocIndex:8},{value:" are as follows. You can modify these parameters according to your needs:",paraId:22,tocIndex:8},{value:"tokenize_mode",paraId:23,tocIndex:8},{value:": Need to be 'sft' at present.",paraId:23,tocIndex:8},{value:"train_mode",paraId:24,tocIndex:8},{value:": Need to be 'sft' at present.",paraId:24,tocIndex:8},{value:"load_raw_dataset",paraId:25,tocIndex:8},{value:": Need to be 'True' at present. Only JSONL format is supported.",paraId:25,tocIndex:8},{value:"data_paths",paraId:26,tocIndex:8},{value:': "[path1,path2,path3]" Input data addresses, a string enclosed in [], with different paths separated by commas (,). Each path is a directory where the last level of the directory name is considered as the task name. Each task directory contains 1 to multiple jsonl data files.',paraId:26,tocIndex:8},{value:"output_dir",paraId:27,tocIndex:8},{value:": Training output directory to store checkpoints, lora_adaptor checkpoints, etc.",paraId:27,tocIndex:8},{value:"tensorboard_dir",paraId:28,tocIndex:8},{value:": Can be temporarily ignored, as the actual tensorboard is stored in the runs directory under output_dir.",paraId:28,tocIndex:8},{value:"model_type",paraId:29,tocIndex:8},{value:": Currently only supports gpt_neox.",paraId:29,tocIndex:8},{value:"peft_type",paraId:30,tocIndex:8},{value:": Currently only supports lora.",paraId:30,tocIndex:8},{value:"pretrained_model_path",paraId:31,tocIndex:8},{value:": Local directory of the pre-trained model.",paraId:31,tocIndex:8},{value:"total_train_batch_size",paraId:32,tocIndex:8},{value:": The total batch size for training across all GPUs, calculated automatically based on per gpu batch size entered in the script.",paraId:32,tocIndex:8},{value:"per_device_valid_batch_size",paraId:33,tocIndex:8},{value:": The batch size for evaluation on each GPU, calculated automatically based on per gpu batch size entered in the script.",paraId:33,tocIndex:8},{value:"gradient_accumulation_steps",paraId:34,tocIndex:8},{value:": Number of gradient accumulation steps. Global batch size = num",paraId:34,tocIndex:8},{value:"gpus * per",paraId:34,tocIndex:8},{value:"device_train_batch_size * gradient_accumulation_steps.",paraId:34,tocIndex:8},{value:"checkpoint_activations",paraId:35,tocIndex:8},{value:": Enable if running out of GPU memory. Trades time for space by not caching activation states, resulting in two forward passes to save memory.",paraId:35,tocIndex:8},{value:"learning_rate",paraId:36,tocIndex:8},{value:": Learning rate. When fine-tuning the entire model, it is recommended to use a smaller value, such as 1e-5 or 5e-6. For lora, a larger learning rate is generally used, such as 1e-4 or 2e-4.",paraId:36,tocIndex:8},{value:"min_lr",paraId:37,tocIndex:8},{value:": Minimum learning rate, usually one-tenth of the learning_rate.",paraId:37,tocIndex:8},{value:"seq_length",paraId:38,tocIndex:8},{value:": Maximum length during training. Set according to your device, longer lengths require more memory.",paraId:38,tocIndex:8},{value:"log_interval",paraId:39,tocIndex:8},{value:": Frequency of logging training loss.",paraId:39,tocIndex:8},{value:"checkpointing_steps",paraId:40,tocIndex:8},{value:": Frequency of saving a model checkpoint.",paraId:40,tocIndex:8},{value:"evalation_steps",paraId:41,tocIndex:8},{value:": Frequency of evaluating on the validation set.",paraId:41,tocIndex:8},{value:"early_stopping_patience",paraId:42,tocIndex:8},{value:": Number of consecutive eval points without further convergence to stop training.",paraId:42,tocIndex:8},{value:"lr_scheduler_type",paraId:43,tocIndex:8},{value:": Learning rate changing strategy.",paraId:43,tocIndex:8},{value:"num_warmup_steps",paraId:44,tocIndex:8},{value:": Number of warm-up steps for the learning rate to increase to the specified value.",paraId:44,tocIndex:8},{value:"seed",paraId:45,tocIndex:8},{value:": Random seed used for reproducibility of experimental results.",paraId:45,tocIndex:8},{value:"train_iters",paraId:46,tocIndex:8},{value:": Can be temporarily set to a small value, such as 10, which does not affect the actual number of training steps, kept for future expansion to support reading datasets in other formats.",paraId:46,tocIndex:8},{value:"valid_iters",paraId:47,tocIndex:8},{value:": Can be temporarily set to a small value, such as 10, which does not affect the actual number of training steps, kept for future expansion to support reading datasets in other formats.",paraId:47,tocIndex:8},{value:"evaluation_strategy",paraId:48,tocIndex:8},{value:': Evaluation strategy during training. "steps" means to evaluate every "valid_interval" steps, "epoch" means to evaluate every epoch. Both can be enabled simultaneously.',paraId:48,tocIndex:8},{value:"save_strategy",paraId:49,tocIndex:8},{value:': Strategy for saving model weights during training. "steps" means to save every "checkpointing_steps" steps.',paraId:49,tocIndex:8},{value:"extra_save_by_epoch",paraId:50,tocIndex:8},{value:": Whether to save an epoch-level checkpoint every epoch.",paraId:50,tocIndex:8},{value:"save_total_limit",paraId:51,tocIndex:8},{value:": Maximum number of model checkpoints to keep. Generally set to 2, retaining the checkpoint with the lowest valid loss and the latest checkpoint. Note that epoch-level checkpoints will always be retained and are not subject to this limit.",paraId:51,tocIndex:8},{value:"weighted_loss_mode",paraId:52,tocIndex:8},{value:": Loss weighting method for multi-task training.",paraId:52,tocIndex:8},{value:`Using LoRA or QLoRA for training, this project only saves the weights and configuration files of the adapters.
To merge the adapter weights with the base model, see `,paraId:53,tocIndex:10},{value:"src/pefts/merge_base_and_lora_to_hf.py",paraId:53,tocIndex:10},{value:"Here is the script for inference on our trained models, which is compatible with most Hugging Face models:",paraId:54,tocIndex:11},{value:`from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
)
tokenizer = AutoTokenizer.from_pretrained(mode_name_or_path, trust_remote_code=True, use_fast=False, legacy=False)
tokenizer.padding_side = "left"
tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids("<unk>")
tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids("</s>")
model = AutoModelForCausalLM.from_pretrained(mode_name_or_path, trust_remote_code=True)

HUMAN_ROLE_START_TAG = "<|role_start|>human<|role_end|>"
BOT_ROLE_START_TAG = "<|role_start|>bot<|role_end|>"
texts = ["write a python function of quick sort."]
texts = [f"{HUMAN_ROLE_START_TAG}{text}{BOT_ROLE_START_TAG}" for text in texts]

inputs = tokenizer(texts, return_tensors='pt', padding=True, add_special_tokens=False).to("cuda")
outputs = model.generate(
        inputs=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=512,
        top_p=0.95,
        temperature=0.1,
        do_sample=True,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id
    )
gen_text = tokenizer.batch_decode(outputs[:, inputs["input_ids"].shape[1]:], skip_special_tokens=True)
print(gen_text)
`,paraId:55,tocIndex:11},{value:`Indeed, the parameters top_p, temperature, repetition_penalty, do_sample, etc., have a significant impact on the model's generation output.
You can modify these parameters based on your specific use case.`,paraId:56,tocIndex:11},{value:"In code generation scenarios, if you are using the sampling mode (do_sample=True), the following parameter settings can yield good results for the Pass@1 metric:",paraId:57,tocIndex:11},{value:"top_p: Set a higher value, such as 0.95, to retain highly probable generated words. This helps ensure more accurate and fluent generation results.",paraId:58,tocIndex:11},{value:"temperature: Set a lower value, such as 0.1, to reduce randomness. Lower temperature values make the generation output more deterministic.",paraId:59,tocIndex:11},{value:"These parameter combinations can control the diversity of the generated outputs while maintaining naturalness. Additionally, you can adjust other related parameters, such as repetition_penalty, to reduce repetition in the generated results.",paraId:60,tocIndex:11},{value:"If you choose the non-sampling mode (do_sample=False), you can consider the following parameter settings:",paraId:61,tocIndex:11},{value:"beam_num: Set a smaller value such as 1 or 3. ",paraId:62,tocIndex:11},{value:"beam_num=1",paraId:62,tocIndex:11},{value:" represents greedy decoding, which selects the most probable single generated word. ",paraId:62,tocIndex:11},{value:"beam_num=3",paraId:62,tocIndex:11},{value:" represents beam search mode, which considers multiple potential generation paths and chooses the best path among them.",paraId:62,tocIndex:11},{value:"If OOM (Out of Memory) occurs, you can mitigate it by reducing parameters such as per GPU batch size (the first argument when starting the training script) and seq_length. You can also set gradient_checkpointing=true, which significantly reduces memory usage but may slow down the training speed.",paraId:63,tocIndex:13}]},15733:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"High Accuracy and efficiency Multi-task Fine-tuning framework for Code LLMs.",paraId:0,tocIndex:0},{value:"MFTCoder",paraId:1,tocIndex:0},{value:` is an open-source project of CodeFuse for accurate and efficient Multi-task Fine-tuning(MFT) on Large Language Models(LLMs), especially on Code-LLMs(large language model for code tasks).
Moreover, we open source Code LLM models and code-related datasets along with the MFTCoder framework.`,paraId:1,tocIndex:0},{value:"In MFTCoder, we released two codebases for finetuning Large Language Models:",paraId:2,tocIndex:0},{value:"MFTCoder-accelerate",paraId:3,tocIndex:0},{value:" is a framework with accelerate and DeepSpeed/FSDP. All tech-stacks are open-source and vibrant. We highly recommend you try this framework and make your fintuning accurate and efficient.",paraId:3,tocIndex:0},{value:"MFTCoder-atorch",paraId:3,tocIndex:0},{value:" is based on the ",paraId:3,tocIndex:0},{value:"ATorch frameworks",paraId:3,tocIndex:0},{value:", which is a fast distributed training framework of LLM.",paraId:3,tocIndex:0},{value:"The aim of this project is to foster collaboration and share advancements in large language models, particularly within the domain of code development.",paraId:4,tocIndex:0},{value:":white_check_mark: ",paraId:5,tocIndex:2},{value:"Multi-task",paraId:5,tocIndex:2},{value:": Train models on multiple tasks while maintaining a balance between them. The models can even generalize to new, previously unseen tasks.",paraId:5,tocIndex:2},{value:":white_check_mark: ",paraId:6,tocIndex:2},{value:"Multi-model",paraId:6,tocIndex:2},{value:": It integrates state-of-the-art open-source models such as gpt-neox, llama, llama-2, baichuan, Qwen, chatglm2, and more. (These finetuned models will be released in the near future.)",paraId:6,tocIndex:2},{value:":white_check_mark: ",paraId:7,tocIndex:2},{value:"Multi-framework",paraId:7,tocIndex:2},{value:": It provides support for both Accelerate (with Deepspeed and FSDP) and ATorch",paraId:7,tocIndex:2},{value:":white_check_mark: ",paraId:8,tocIndex:2},{value:"Efficient fine-tuning",paraId:8,tocIndex:2},{value:": It supports LoRA, QLoRA as well as Full-parameters training, enabling fine-tuning of large models with minimal resources. The training speed meets the demands of almost all fine-tuning scenarios.",paraId:8,tocIndex:2},{value:"The main components of this project include:",paraId:9,tocIndex:2},{value:"Support for both SFT (Supervised FineTuning) and MFT (Multi-task FineTuning). The current MFTCoder achieves data balance among multiple tasks, and future releases will achieve a balance between task difficulty and convergence speed during training.",paraId:10,tocIndex:2},{value:"Support for QLoRA instruction fine-tuning, LoRA fine-tuning as well as Full-parameters fine-tuning.",paraId:10,tocIndex:2},{value:"Support for most mainstream open-source large models, particularly those relevant to Code-LLMs, such as DeepSeek-coder, Mistral, Mixtral, Chatglm3, Code-LLaMA, Starcoder, Codegeex2, Qwen, GPT-Neox, and more.",paraId:10,tocIndex:2},{value:"Support for weight merging between the LoRA adaptor and base models, simplifying the inference process.",paraId:10,tocIndex:2},{value:"Release of 2 high-quality code-related instruction fine-tuning datasets: ",paraId:10,tocIndex:2},{value:"Evol-instruction-66k",paraId:10,tocIndex:2},{value:" and ",paraId:10,tocIndex:2},{value:"CodeExercise-Python-27k",paraId:10,tocIndex:2},{value:".",paraId:10,tocIndex:2},{value:"Release of many Code LLMs, please refer to organizations: ",paraId:10,tocIndex:2},{value:"codefuse-ai on huggingface",paraId:10,tocIndex:2},{value:" or ",paraId:10,tocIndex:2},{value:"codefuse-ai on modelscope",paraId:10,tocIndex:2},{value:".",paraId:10,tocIndex:2}]},97672:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"To begin, ensure that you have successfully installed CUDA (version >= 11.4, preferably 11.7) along with the necessary drivers. Additionally, make sure you have installed torch (version 2.0.1).",paraId:0,tocIndex:0},{value:"Next, we have provided an init_env.sh script to simplify the installation of required packages. Execute the following command to run the script:",paraId:1,tocIndex:0},{value:`sh init_env.sh
`,paraId:2,tocIndex:0},{value:"We highly recommend training with flash attention(version >= 2.1.0, preferably 2.3.6), please refer to the following link for installation instructions: ",paraId:3,tocIndex:0},{value:"https://github.com/Dao-AILab/flash-attention",paraId:3,tocIndex:0},{value:"As mentioned above, we open source two training frameworks. You could refer to their own READMEs for more details as followed.",paraId:4,tocIndex:1},{value:"If you are familiar with open source ",paraId:5,tocIndex:1},{value:"transformers",paraId:5,tocIndex:1},{value:", ",paraId:5,tocIndex:1},{value:"DeepSpeed",paraId:5,tocIndex:1},{value:" or ",paraId:5,tocIndex:1},{value:"FSDP",paraId:5,tocIndex:1},{value:", we highly recommend you try:",paraId:5,tocIndex:1},{value:"\u{1F680}\u{1F680} ",paraId:6,tocIndex:1},{value:"MFTCoder-accelerate: Accelerate + Deepspeed/FSDP Codebase for MFT(Multi-task Finetuning)",paraId:7,tocIndex:1},{value:"If you want to explore some new framework like atorch, you could check:",paraId:8,tocIndex:1},{value:"\u{1F680} ",paraId:9,tocIndex:1},{value:"MFTCoder-atorch: Atorch Codebase for MFT(Multi-task Finetuning)",paraId:10,tocIndex:1},{value:"We are excited to release the following two CodeLLMs trained by MFTCoder, now available on both HuggingFace and ModelScope:",paraId:11,tocIndex:2},{value:"Model",paraId:12,tocIndex:2},{value:"HuggingFace Links",paraId:12,tocIndex:2},{value:"ModelScope Links",paraId:12,tocIndex:2},{value:"Base Model",paraId:12,tocIndex:2},{value:"Num of examples trained",paraId:12,tocIndex:2},{value:"Batch Size",paraId:12,tocIndex:2},{value:"Seq Length",paraId:12,tocIndex:2},{value:"\u{1F525} CodeFuse-DeepSeek-33B",paraId:12,tocIndex:2},{value:"h-link",paraId:12,tocIndex:2},{value:"m-link",paraId:12,tocIndex:2},{value:"DeepSeek-coder-33B",paraId:12,tocIndex:2},{value:"60 \u4E07",paraId:12,tocIndex:2},{value:"80",paraId:12,tocIndex:2},{value:"4096",paraId:12,tocIndex:2},{value:"\u{1F525} CodeFuse-Mixtral-8x7B",paraId:12,tocIndex:2},{value:"h-link",paraId:12,tocIndex:2},{value:"m-link",paraId:12,tocIndex:2},{value:"Mixtral-8x7B",paraId:12,tocIndex:2},{value:"60 \u4E07",paraId:12,tocIndex:2},{value:"80",paraId:12,tocIndex:2},{value:"4096",paraId:12,tocIndex:2},{value:"\u{1F525} CodeFuse-CodeLlama-34B",paraId:12,tocIndex:2},{value:"h-link",paraId:12,tocIndex:2},{value:"m-link",paraId:12,tocIndex:2},{value:"CodeLlama-34b-Python",paraId:12,tocIndex:2},{value:"60 \u4E07",paraId:12,tocIndex:2},{value:"80",paraId:12,tocIndex:2},{value:"4096",paraId:12,tocIndex:2},{value:"\u{1F525} CodeFuse-CodeLlama-34B-4bits",paraId:12,tocIndex:2},{value:"h-link",paraId:12,tocIndex:2},{value:"m-link",paraId:12,tocIndex:2},{value:"CodeLlama-34b-Python",paraId:12,tocIndex:2},{value:"4096",paraId:12,tocIndex:2},{value:"\u{1F525} CodeFuse-StarCoder-15B",paraId:12,tocIndex:2},{value:"h-link",paraId:12,tocIndex:2},{value:"m-link",paraId:12,tocIndex:2},{value:"StarCoder-15B",paraId:12,tocIndex:2},{value:"60 \u4E07",paraId:12,tocIndex:2},{value:"80",paraId:12,tocIndex:2},{value:"4096",paraId:12,tocIndex:2},{value:"\u{1F525} CodeFuse-QWen-14B",paraId:12,tocIndex:2},{value:"h-link",paraId:12,tocIndex:2},{value:"m-link",paraId:12,tocIndex:2},{value:"Qwen-14b",paraId:12,tocIndex:2},{value:"110 \u4E07",paraId:12,tocIndex:2},{value:"256",paraId:12,tocIndex:2},{value:"4096",paraId:12,tocIndex:2},{value:"\u{1F525} CodeFuse-CodeGeex2-6B",paraId:12,tocIndex:2},{value:"h-link",paraId:12,tocIndex:2},{value:"m-link",paraId:12,tocIndex:2},{value:"CodeGeex2-6B",paraId:12,tocIndex:2},{value:"110 \u4E07",paraId:12,tocIndex:2},{value:"256",paraId:12,tocIndex:2},{value:"4096",paraId:12,tocIndex:2},{value:"We are also pleased to release two code-related instruction datasets, meticulously selected from a range of datasets to facilitate multitask training. Moving forward, we are committed to releasing additional instruction datasets covering various code-related tasks.",paraId:13,tocIndex:3},{value:"Dataset",paraId:14,tocIndex:3},{value:"Description",paraId:14,tocIndex:3},{value:"\u2B50 Evol-instruction-66k",paraId:14,tocIndex:3},{value:"Based on open-evol-instruction-80k, filter out low-quality, repeated, and similar instructions to HumanEval, thus get high-quality code instruction dataset.",paraId:14,tocIndex:3},{value:"\u2B50 CodeExercise-Python-27k",paraId:14,tocIndex:3},{value:"python code exercise instruction dataset",paraId:14,tocIndex:3}]},82137:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"Moda Model Access Link\uFF1A",paraId:0,tocIndex:1},{value:"ModelScope TestGPT-7B",paraId:0,tocIndex:1},{value:"Test Agent",paraId:1,tocIndex:2},{value:' aims to build an "intelligent agent" in the testing domain, integrating large models with engineering technologies in the quality domain to promote the generational upgrade of quality technology. We look forward to collaborating with community members to create innovative solutions in the testing domain, establish a 24-hour online testing assistant service, and make testing as smooth as silk.',paraId:1,tocIndex:2},{value:"Model",paraId:2,tocIndex:3},{value:`: This release open-sources the TestGPT-7B model for the testing domain. The model is based on CodeLlama-7B and has been fine-tuned for related downstream tasks:
`,paraId:2,tocIndex:3},{value:"Multilingual Test Case Generation (Java/Python/Javascript)",paraId:3,tocIndex:3},{value:": This has always been an area of great interest to both academia and industry, with new products and tools like EvoSuite, Randoop, SmartUnit, etc., constantly being incubated. However, traditional test case generation has pain points that are difficult to address. Test case generation based on large models is superior to traditional tools in terms of readability, completeness of test scenarios, and multilingual support. This update focuses on multilingual test case generation, initially including Java, Python, and Javascript, and will gradually introduce Go, C++, and other languages in future releases.",paraId:3,tocIndex:3},{value:"Test Case Assert Completion",paraId:3,tocIndex:3},{value:": Analyzing the current state of test cases, we found that a certain proportion of existing test cases in the code repositories do not contain Asserts. Test cases without Asserts may pass during regression but fail to detect issues. Therefore, we expanded the scenario of automatic completion of test case Asserts. With this model capability, combined with the right engineering support, it's possible to perform batch automatic completion for the entire test case repository, intelligently raising the quality level of the project.",paraId:3,tocIndex:3},{value:"Engineering Framework",paraId:2,tocIndex:3},{value:`: Local model quick release and experience engineering framework
`,paraId:2,tocIndex:3},{value:"ChatBot page",paraId:4,tocIndex:3},{value:"Quick model launch",paraId:4,tocIndex:3},{value:"Private deployment, localized GPT large model interactions with your data and environment, no risk of data leakage, 100% safe.",paraId:4,tocIndex:3},{value:"We will continue to iterate on the model and engineering capabilities:",paraId:5,tocIndex:3},{value:"Continuously adding more exciting test domain application scenarios, such as domain knowledge Q&A, test scenario analysis, etc.",paraId:6,tocIndex:3},{value:"Supporting the open copilot engineering framework focused on testing scenarios, such as intelligent embedding of testing domain knowledge, a common tool API system, intelligent testing Agent, and more, so stay tuned!",paraId:6,tocIndex:3},{value:"Expanding from a 7B base to 13B and 34B models gradually. Stay tuned!",paraId:6,tocIndex:3},{value:`Currently, within TestAgent, we default to using the TestGPT-7B model. Compared to existing open-source models, the TestGPT-7B model leads the industry in case execution pass rate (pass@1) and test scenario coverage (average number of test scenarios).
The core capability evaluation results of the TestGPT-7B model are as follows:`,paraId:7,tocIndex:4},{value:"Multilingual Test Case Generation For the three supported languages of the model: Java, Python, and Javascript, the Pass@1 evaluation results are as follows:",paraId:8,tocIndex:4},{value:"Model",paraId:9,tocIndex:4},{value:"Java pass@1",paraId:9,tocIndex:4},{value:"Java Average number of test scenarios",paraId:9,tocIndex:4},{value:"Python pass@1",paraId:9,tocIndex:4},{value:"Python Average number of test scenarios",paraId:9,tocIndex:4},{value:"Javascript pass@1",paraId:9,tocIndex:4},{value:"Javascript Average number of test scenarios",paraId:9,tocIndex:4},{value:"TestGPT-7B",paraId:9,tocIndex:4},{value:"48.6%",paraId:9,tocIndex:4},{value:"4.37",paraId:9,tocIndex:4},{value:"35.67%",paraId:9,tocIndex:4},{value:"3.56",paraId:9,tocIndex:4},{value:"36%",paraId:9,tocIndex:4},{value:"2.76",paraId:9,tocIndex:4},{value:"CodeLlama-13B-Instruct",paraId:9,tocIndex:4},{value:"40.54%",paraId:9,tocIndex:4},{value:"1.08",paraId:9,tocIndex:4},{value:"30.57%",paraId:9,tocIndex:4},{value:"1.65",paraId:9,tocIndex:4},{value:"31.7%",paraId:9,tocIndex:4},{value:"3.13",paraId:9,tocIndex:4},{value:"Qwen-14B-Chat",paraId:9,tocIndex:4},{value:"10.81%",paraId:9,tocIndex:4},{value:"2.78",paraId:9,tocIndex:4},{value:"15.9%",paraId:9,tocIndex:4},{value:"1.32",paraId:9,tocIndex:4},{value:"9.15%",paraId:9,tocIndex:4},{value:"4.22",paraId:9,tocIndex:4},{value:"Baichuan2-13B-Chat",paraId:9,tocIndex:4},{value:"13.5%",paraId:9,tocIndex:4},{value:"2.24",paraId:9,tocIndex:4},{value:"12.7%",paraId:9,tocIndex:4},{value:"2.12",paraId:9,tocIndex:4},{value:"6.1%",paraId:9,tocIndex:4},{value:"3.31",paraId:9,tocIndex:4},{value:`Test Case Assert Completion
Currently, the model supports Assert completion for Java cases, and the Pass@1 evaluation`,paraId:10,tocIndex:4},{value:"Model",paraId:11,tocIndex:4},{value:"pass@1",paraId:11,tocIndex:4},{value:"Percentage of strong validation",paraId:11,tocIndex:4},{value:"Codefuse-TestGPT-7B",paraId:11,tocIndex:4},{value:"71.1%",paraId:11,tocIndex:4},{value:"100%",paraId:11,tocIndex:4},{value:"The clarion call for large models has been sounded, and large models in the testing domain are continuously evolving. With the rich world knowledge accumulated during the pre-training process, they have demonstrated extraordinary reasoning and decision-making abilities in complex interactive environments.",paraId:12,tocIndex:5},{value:"Despite significant achievements of the foundational models in the testing domain, there are still some limitations. Testing tasks in specific domains often require specialized tools or domain knowledge. For instance, foundational models can complete tasks such as single-instance test code generation and test text generation through pre-trained knowledge, but when dealing with complex integrated test case generation, domain-specific case creation, and interactions with test process pipelines, more specialized tools and domain knowledge are necessary. Therefore, integrating specialized tools with foundational models can fully harness their respective strengths. Specialized tools can address insufficiencies in model timeliness, enhance professional knowledge, and improve interpretability and robustness. On the other hand, foundational models possess human-like reasoning and planning abilities, capable of understanding complex data and scenarios, and interacting with the real world.",paraId:13,tocIndex:5},{value:"Building upon the open model engineering deployment and ChatBot foundation in this release, we will continue to invest deeply in the open-source testing domain. Collaborating with community developers who share similar interests, we aim to create the most advanced engineering system for tools in the testing domain, an intelligent testing assistant, and open-source testing engineering!",paraId:14,tocIndex:5}]},11620:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[{value:"You can get detailed information about the model and download the model files from ",paraId:0,tocIndex:2},{value:"modelscope",paraId:0,tocIndex:2},{value:" or ",paraId:0,tocIndex:2},{value:"huggingface",paraId:0,tocIndex:2},{value:`.
Please note:
\u9700\u8981\u6CE8\u610F\u7684\u662F\uFF1A
If you download the model through modelscope, refer to the download instructions: `,paraId:0,tocIndex:2},{value:"Download Instructions",paraId:0,tocIndex:2},{value:`;
If you download the model through huggingface, please make sure you have proper access to huggingface.`,paraId:0,tocIndex:2},{value:"python>=3.8",paraId:1,tocIndex:3},{value:"transformers==4.33.2",paraId:1,tocIndex:3},{value:`git clone https://github.com/codefuse-ai/Test-Agent
cd Test-Agent
pip install -r requirements.txt
`,paraId:2,tocIndex:3},{value:"Before starting to run the TestGPT-7B model, please ensure that your execution environment has about 14GB of VRAM.",paraId:3,tocIndex:3},{value:"The project provides the ability to quickly set up a web UI for a more intuitive display of model interactions and effects. We can use a few simple commands to wake up the front-end page and call the model capabilities in real time. In the project directory, start the following services in order:",paraId:4,tocIndex:4},{value:"1.",paraId:5,tocIndex:4},{value:"Start controller",paraId:5,tocIndex:4},{value:`
python3 -m chat.server.controller`,paraId:5,tocIndex:4},{value:"2.",paraId:6,tocIndex:4},{value:"Start model worker",paraId:6,tocIndex:4},{value:`
python3 -m chat.server.model_worker --model-path models/TestGPT-7B --device mps`,paraId:6,tocIndex:4},{value:"(models/TestGPT-7B is the actual model file path)",paraId:7,tocIndex:4},{value:"For the launch method, you can choose from several configuration options as needed:",paraId:8,tocIndex:4},{value:"--device mps for enabling GPU acceleration on Mac computers (Apple Silicon or AMD GPUs);",paraId:9,tocIndex:4},{value:`--device xpu for enabling acceleration on Intel XPU (Intel Data Center and Arc A-Series GPUs):
`,paraId:9,tocIndex:4},{value:"Install ",paraId:10,tocIndex:4},{value:"Intel Extension for PyTorch",paraId:10,tocIndex:4},{value:"Set the OneAPI environment variable: source /opt/intel/oneapi/setvars.sh",paraId:10,tocIndex:4},{value:`--device npu for enabling acceleration on Huawei AI processors;
`,paraId:9,tocIndex:4},{value:"Install ",paraId:11,tocIndex:4},{value:"Ascend PyTorch Adapter",paraId:11,tocIndex:4},{value:"\u8BBE\u7F6E CANN \u73AF\u5883\u53D8\u91CF\uFF1Asource /usr/local/Ascend/ascend-toolkit/set_env.sh",paraId:11,tocIndex:4},{value:"--device cpu for running using only CPU, no GPU needed;",paraId:9,tocIndex:4},{value:"--num-gpus 2 to specify the option of running GPUs concurrently.",paraId:9,tocIndex:4},{value:"Start the web service",paraId:12,tocIndex:4},{value:`
python3 -m chat.server.gradio_testgpt
`,paraId:12,tocIndex:4},{value:`
Once the service is ready, you can open the local web service address `,paraId:12,tocIndex:4},{value:"http://0.0.0.0:7860",paraId:12,tocIndex:4},{value:` and see the complete front-end page. At the bottom of the page, there are two examples: \u3010Single-test Generation\u3011 and \u3010Assert Completion\u3011. After clicking the button, a sample text will be automatically generated in the input box. Clicking the Send button will trigger the model to run. After waiting patiently for a while (running time depends on the performance of your machine), you can see the complete answer.
`,paraId:12,tocIndex:4}]},19820:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[]},56706:function(t,e,a){a.r(e),a.d(e,{texts:function(){return n}});const n=[]}}]);
