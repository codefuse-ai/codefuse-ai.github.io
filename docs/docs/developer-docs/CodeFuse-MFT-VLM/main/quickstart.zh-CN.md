---
store:
  title: CodeFuse-MFT-VLM
  version: main
group:
  title: ğŸŒ± CodeFuse-MFT-VLM
  order: -1
title: å¿«é€Ÿä½¿ç”¨
order: 0
toc: content
github: https://github.com/codefuse-ai/CodeFuse-MFT-VLM
---

## Contents

- [Install](#Install)
- [Datasets](#Datasets)
- [Multimodal Alignment](#Multimodal-Alignment)
- [Visual Instruction Tuning](#Visual-Instruction-Tuning)
- [Evaluation](#Evaluation)

## Install

è¯·æ‰§è¡Œ sh init_env.sh

## Datasets

ä½¿ç”¨äº†ä»¥ä¸‹æ•°æ®é›†è®­ç»ƒæ¨¡å‹:

| æ•°æ®é›†                                          | ä»»åŠ¡ç§ç±»                    | æ ·æœ¬é‡  |
| ----------------------------------------------- | --------------------------- | ------- |
| synthdog-en                                     | OCR                         | 800,000 |
| synthdog-zh                                     | OCR                         | 800,000 |
| cc3m(downsampled)                               | Image Caption               | 600,000 |
| cc3m(downsampled)                               | Image Caption               | 600,000 |
| SBU                                             | Image Caption               | 850,000 |
| Visual Genome VQA (Downsampled)                 | Visual Question Answer(VQA) | 500,000 |
| Visual Genome Region descriptions (Downsampled) | Reference Grouding          | 500,000 |
| Visual Genome objects (Downsampled)             | Grounded Caption            | 500,000 |
| OCR VQA (Downsampled)                           | OCR and VQA                 | 500,000 |

è¯·åˆ°å„ä¸ªæ•°æ®é›†çš„å®˜ç½‘ä¸Šä¸‹è½½è¿™äº›æ•°æ®ã€‚

## Multimodal Alignment

è¯·æ‰§è¡Œ sh scripts/pretrain.sh æˆ–è€… sh scripts/pretrain_multinode.sh

## Visual Instruction Tuning

è¯·æ‰§è¡Œ sh scripts/finetune.sh æˆ–è€… sh scripts/finetune_multinode.sh

## Evaluation

è¯·æ‰§è¡Œ llava/eval/ å½“ä¸­çš„ python è„šæœ¬. å¯ä»¥é€šè¿‡ä¸‹é¢çš„ä»£ç æ¥åŠ è½½æˆ‘ä»¬é¢„è®­ç»ƒçš„ CodeFuse-VLM-14B:

```
import os
from llava.model.builder import load_mixed_pretrained_model

model_path = '/pretrained/model/path'
tokenizer, model, image_processor, context_len = load_mixed_pretrained_model(model_path, None, 'qwen-vl-14b', os.path.join(model_path, 'Qwen-VL-visual'), 'cross_attn', os.path.join(model_path, 'mm_projector/mm_projector.bin'))
```

æ‚¨ä¹Ÿå¯ä»¥å…ˆè¿è¡Œä¸‹é¢çš„è„šæœ¬æ¥åˆå¹¶å„ä¸ªæ¨¡å‹ç»„ä»¶ï¼šscripts/merge_qwen_vl_weights.shï¼Œç„¶åé€šè¿‡ä¸‹é¢çš„ä»£ç åŠ è½½åˆå¹¶åçš„æ¨¡å‹ï¼š

```
from llava.model import LlavaQWenForCausalLM

model = LlavaQWenForCausalLM.from_pretrained('/path/to/our/pretrained/model')
```

## CodeFuse-VLM äº§å“è§†é¢‘

è¿™æ˜¯æˆ‘ä»¬æ¨¡å‹æ”¯æŒçš„äº§å“çš„è§†é¢‘

<video controls width="1000">
 <source src="https://gw.alipayobjects.com/v/huamei_bvbxju/afts/video/A*jy7aQbVM8BkAAAAAAAAAAAAADlHYAQ" type="video/mp4" />
</video>
