---
store:
  title: MFTCoder
  version: main
group:
  title: ğŸŒ± MFTCoder
  order: -1
title: Atorchæ¡†æ¶ç¯‡
order: 2
toc: content
github: https://github.com/codefuse-ai/MFTCoder
---

[![Generic badge](https://img.shields.io/badge/ğŸ¤—-Huggingface%20Repo-green.svg)](https://huggingface.co/codefuse-ai)&nbsp;
<a href="https://github.com/codefuse-ai/MFTCoder/blob/main/LICENSE">
<img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue">
</a>

## 1. æ›´æ–°

ğŸ”¥ MFTCoder åœ¨ Atorch æ¡†æ¶ä¸‹æ”¯æŒ GPTNeoX æ¨¡å‹çš„å¾®è°ƒï¼›

ğŸ”¥ MFTCoder æ”¯æŒå…¨é‡çš„æœ‰ç›‘ç£å¾®è°ƒï¼›

ğŸ”¥ MFTCoder æ”¯æŒ LoRA å¾®è°ƒï¼›

## 2. æ•°æ®æ ¼å¼

### 2.1 è®­ç»ƒæ•°æ®æ ¼å¼

è®­ç»ƒæ•°æ®ä¸º jsonl æ ¼å¼ï¼Œæ¯ä¸€è¡Œçš„æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼Œå…¶ä¸­ chat_rounds å­—æ®µæ˜¯å¿…éœ€çš„ï¼Œå¯ä»¥æ ¹æ®å®é™…éœ€æ±‚æ·»åŠ æˆ–åˆ é™¤å…¶ä»–å­—æ®µã€‚
å¯ä»¥å‚è€ƒé¡¹ç›®ä¸­çš„ xxx.jsonl æ–‡ä»¶ã€‚

```json
{
  "id": 0,
  "data_name": "code-helper",
  "chat_rounds": [
    {
      "role": "system",
      "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä»£ç åŠ©æ‰‹ï¼Œå¯ä»¥å›å¤ç”¨æˆ·ä¸ä»£ç ç›¸å…³çš„é—®é¢˜",
      "chat_round_id": 0
    },
    {
      "role": "human",
      "content": "å†™ä¸€ä¸ªå¿«é€Ÿæ’åº",
      "chat_round_id": 1
    },
    {
      "role": "bot",
      "content": "ä»¥ä¸‹æ˜¯ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•xxxxxx",
      "chat_round_id": 1
    },
    {
      "role": "human",
      "content": "è§£é‡Šä¸€ä¸‹è¿™æ®µä»£ç ",
      "chat_round_id": 2
    },
    {
      "role": "bot",
      "content": "å¥½çš„ï¼Œè¿™æ®µä»£ç xxx",
      "chat_round_id": 2
    }
  ]
}
```

### 2.2 æ¨ç†æ•°æ®æ ¼å¼

æ¨ç†æ•°æ®æ ¼å¼ä¸ºæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®æ ¼å¼ä¸‹æ‹¼æ¥çš„å­—ç¬¦ä¸²å½¢å¼ï¼Œå®ƒä¹Ÿæ˜¯æ¨ç†æ—¶è¾“å…¥ prompt æ‹¼æ¥çš„æ–¹å¼ï¼š

```python
"""
<|role_start|>system<|role_end|>è¿™æ˜¯SystemæŒ‡ä»¤
<|role_start|>human<|role_end|>è¿™æ˜¯ç¬¬1è½®ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
<|role_start|>bot<|role_end|>è¿™æ˜¯ç¬¬1è½®æ¨¡å‹ç”Ÿæˆçš„å†…å®¹</s>
<|role_start|>human<|role_end|>è¿™æ˜¯ç¬¬2è½®ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
<|role_start|>bot<|role_end|>è¿™æ˜¯ç¬¬2è½®æ¨¡å‹ç”Ÿæˆçš„å†…å®¹</s>
...
...
...
<|role_start|>human<|role_end|>è¿™æ˜¯ç¬¬nè½®ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
<|role_start|>bot<|role_end|>{æ¨¡å‹ç°åœ¨è¦ç”Ÿæˆçš„å†…å®¹}</s>
"""
```

## 3. æ¨¡å‹è®­ç»ƒ

ç›®å‰ "MFTCoder/mft_atorch" ä»£ç åº“æ”¯æŒå…¨é‡å‚æ•°æŒ‡ä»¤å¾®è°ƒå’Œ LoRA æŒ‡ä»¤å¾®è°ƒã€‚
ç›®å‰ä»…æ”¯æŒ GPTNeoX æ¨¡å‹çš„è®­ç»ƒï¼Œç†è®ºä¸Šï¼ŒHuggingFace ä¸Šå¼€æºçš„ GPTNeoX æ¨¡å‹æƒé‡ï¼Œå‡å¯ä½¿ç”¨æœ¬é¡¹ç›®è¿›è¡Œè®­ç»ƒã€‚

æˆ‘ä»¬å°†è®­ç»ƒä¸­ä½¿ç”¨çš„å„ç§ç»„ä»¶æŠ½å–å‡ºæ¥ï¼Œä»¥ä¾¿åç»­çš„æ‰©å±•å’Œä¼˜åŒ–ï¼Œè¯¦è§ä¸»ç›®å½•ä¸‹çš„å®ç°ã€‚å¾®è°ƒè®­ç»ƒçš„å…¥å£ç›®å½•æ˜¯`train/`, è®­ç»ƒå…¥å£æ–‡ä»¶æ˜¯`train/run_train.py`, å‚æ•°é…ç½®å­˜å‚¨åœ¨å¯åŠ¨è„šæœ¬`train/run_gpt_*.sh`ç­‰æ–‡ä»¶ä¸­ï¼Œæ–¹ä¾¿ç»Ÿä¸€ç®¡ç†å’Œæ›´æ”¹ã€‚

### 3.1 æ•°æ®æ ¼å¼

è®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬å°†å¤šè½®å¯¹è¯æ‹¼æ¥æˆå¦‚ä¸‹æ ¼å¼ï¼Œç„¶åè¿›è¡Œ tokenizeã€‚å…¶ä¸­<|role_start|>human<|role_end|>è¡¨ç¤º human è¾“å…¥æç¤ºç¬¦ï¼Œ<|role_start|>bot<|role_end|>è¡¨ç¤º bot è¾“å‡ºæç¤ºç¬¦ï¼Œ`</s>` è¡¨ç¤º eos_tokenã€‚

```
"<|role_start|>human<|role_end|>input1</s>target1</s>input2</s>target2</s>...
```

åœ¨è®¡ç®— loss æ—¶ï¼Œæˆ‘ä»¬é€šè¿‡ mask çš„æ–¹å¼ï¼Œinput éƒ¨åˆ†çš„ loss ä¸å‚ä¸å‚æ•°æ›´æ–°ï¼Œåªæœ‰â€œtarget</s>â€éƒ¨åˆ†çš„ loss å‚ä¸å‚æ•°æ›´æ–°ã€‚
è¿™ç§æ–¹å¼å……åˆ†åˆ©ç”¨äº†æ¨¡å‹å¹¶è¡Œè®¡ç®—çš„ä¼˜åŠ¿ï¼Œè®­ç»ƒæ›´åŠ é«˜æ•ˆï¼Œä¸”å¤šè½®å¯¹è¯ä¸­çš„æ¯ä¸ª target éƒ¨åˆ†éƒ½å‚ä¸äº†è®­ç»ƒï¼Œè®­ç»ƒæ›´å……åˆ†ã€‚
å¦åˆ™ï¼Œå°±éœ€è¦æŠŠä¸€ä¸ª n è½®å¯¹è¯ï¼Œæ‹†åˆ†æˆ n æ¡æ•°æ®ï¼Œä¸”åªè®¡ç®—æœ€åä¸€ä¸ª target çš„ lossï¼Œå¤§å¤§é™ä½äº†è®­ç»ƒæ•ˆç‡ã€‚

### 3.2 å…¨é‡ SFT

æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤å³å¯è¿›è¡Œå…¨é‡ SFTï¼š

```bash
sh run_gpt_mft.sh 10 1 8 5
```

éœ€æ³¨æ„ï¼Œå¯åŠ¨è„šæœ¬åçš„å››ä¸ªå‚æ•°ï¼Œåˆ†åˆ«æ˜¯ï¼š

- ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯æ€»çš„ per gpu batch size
- ç¬¬äºŒä¸ªå‚æ•°æ˜¯ tensor parallel æ•°ï¼ˆæš‚æ—¶åªæ”¯æŒ 1ï¼‰
- ç¬¬ä¸‰ä¸ªå‚æ•°æ˜¯ data parallel æ•°ï¼Œä¸æ‰€ç”¨ GPU æ•°ä¿æŒä¸€è‡´
- ç¬¬å››ä¸ªå‚æ•°æ˜¯è®­ç»ƒ epoch æ•°

åé¢å…¶ä»–çš„è®­ç»ƒæ–¹å¼å¯åŠ¨è„šæœ¬ï¼Œä¹ŸåŒæ ·éœ€è¦é…ç½®è¿™å››ä¸ªå‚æ•°

### 3.3 LoRA å¾®è°ƒ

æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤å³å¯è¿›è¡Œ Lora å¾®è°ƒï¼š

```bash
sh run_gpt_mft_peft.sh 10 1 8 5
```

### 3.4 å¯åŠ¨è„šæœ¬ä¸­ä¸»è¦å‚æ•°è¯´æ˜

`train/run_gpt_*.sh`ä¸­çš„ä¸»è¦å‚æ•°è¯´æ˜å¦‚ä¸‹ï¼Œä»¥ä¸‹å‚æ•°å¯ä»¥æ ¹æ®éœ€æ±‚è¿›è¡Œä¿®æ”¹ï¼Œå…¶ä»–å‚æ•°å»ºè®®ä¸åšä¿®æ”¹ï¼š

- tokenize_mode: ç›®å‰ä»…æ”¯æŒ"sft"ã€‚

- train_mode: ç›®å‰ä»…æ”¯æŒ"sft"ã€‚

- load_raw_dataset: éœ€è¦ä¿æŒ"True"ï¼Œåç»­ä¼šæ”¯æŒå…¶å®ƒæ¨¡å¼æ•°æ®ï¼Œå½“å‰ä»…æ”¯æŒ jsonl è¾“å…¥

- data_paths: "[path1,path2,path3]" è¾“å…¥æ•°æ®åœ°å€ï¼Œå­—ç¬¦ä¸²ï¼Œå¼€å¤´ç»“å°¾ç”¨[]ï¼Œä¸­é—´ç”¨`,`é—´éš”ä¸åŒ pathï¼Œæ¯ä¸ª path æ˜¯ä¸€ä¸ªç›®å½•ï¼Œç›®å½•çš„æœ€åä¸€çº§åå­—ä½œä¸ºä»»åŠ¡åç§°ï¼Œä¸‹é¢åŒ…å« 1 åˆ°å¤šä¸ª jsonl æ•°æ®ã€‚

- output_dir: è®­ç»ƒè¾“å‡ºç›®å½•ï¼Œå­˜å‚¨ checkpointã€lora_adaptor checkpoint ç­‰ã€‚

- tensorboard_dir: å¯ä»¥æš‚æ—¶å¿½ç•¥ï¼Œå®é™… tensorboard å­˜å‚¨åœ¨ output_dir çš„ runs ç›®å½•ä¸‹ã€‚

- model_type: ç›®å‰ä»…æ”¯æŒ gpt_neoxã€‚

- peft_type: ç›®å‰ä»…æ”¯æŒ loraã€‚

- pretrained_model_path: é¢„è®­ç»ƒæ¨¡å‹çš„æœ¬åœ°ç›®å½•ã€‚

- total_train_batch_size: æ‰€æœ‰æ˜¾å¡ train çš„ batch size çš„æ€»å’Œï¼Œä¼šæ ¹æ®å¯åŠ¨è„šæœ¬æ—¶è¾“å…¥çš„ per gpu batch size è‡ªåŠ¨è®¡ç®—ã€‚

- per_device_valid_batch_size: æ¯å¼ æ˜¾å¡ eval çš„ batch sizeï¼Œä¼šæ ¹æ®å¯åŠ¨è„šæœ¬æ—¶è¾“å…¥çš„ per gpu batch size è‡ªåŠ¨è®¡ç®—ã€‚

- gradient*accumulation_steps: æ¢¯åº¦ç´¯è®¡æ­¥æ•°ã€‚global batch=num_gpus * per*device_train_batch_size * gradient_accumulation_stepsã€‚

- checkpoint_activations: å¦‚æœæ˜¾å­˜æ‰è¥Ÿè§è‚˜ï¼Œå¯ä»¥å¼€å¯ã€‚ä»¥æ—¶é—´æ¢ç©ºé—´ï¼Œæ¨¡å‹ä¸ç¼“å­˜æ¿€æ´»çŠ¶æ€ï¼Œä¼šè¿›è¡Œä¸¤æ¬¡ forward è®¡ç®—ï¼Œä»¥èŠ‚çœæ˜¾å­˜ã€‚

- learning_rate: å­¦ä¹ ç‡ã€‚å…¨é‡å‚æ•°å¾®è°ƒçš„æ—¶å€™ï¼Œå»ºè®®å°ä¸€äº›ï¼Œ1e-5 æˆ– 5e-6ã€‚qlora ä¸­çš„å­¦ä¹ ç‡è®¾ç½®æ›´å¤§ä¸€äº›ï¼Œä¸€èˆ¬ä¸º 1e-4ã€2e-4ã€‚

- min_lr: æœ€ä½å­¦ä¹ ç‡ï¼Œ ä¸€èˆ¬æ˜¯ learning_rate çš„ååˆ†ä¹‹ä¸€ã€‚

- seq_length: è®­ç»ƒæ—¶çš„æœ€å¤§é•¿åº¦ã€‚æŒ‰ç…§è‡ªå·±çš„è®¾å¤‡è¿›è¡Œè®¾ç½®ï¼Œè¶Šé•¿éœ€è¦å ç”¨è¶Šå¤šæ˜¾å­˜ã€‚

- log_interval: æ¯éš”å¤šå°‘æ­¥ç»Ÿè®¡ä¸€æ¬¡ train lossã€‚

- checkpointing_steps: æ¯éš”å¤šå°‘æ­¥ä¿å­˜ä¸€ä¸ªæ¨¡å‹ã€‚

- evalation_steps: æ¯éš”å¤šå°‘æ­¥åœ¨éªŒè¯é›†ä¸Š evaluate ä¸€æ¬¡ã€‚

- early_stopping_patience: å¤šå°‘ä¸ª eval point ä¸ç»§ç»­æ”¶æ•›ï¼Œåˆ™åœæ­¢è®­ç»ƒã€‚

- lr_scheduler_type: å­¦ä¹ ç‡å˜åŒ–ç­–ç•¥ã€‚

- num_warmup_steps: warm up æ­¥æ•°ï¼Œå­¦ä¹ ç‡ç»è¿‡å¤šå°‘æ­¥ï¼Œå¢é•¿åˆ°æŒ‡å®šçš„æ•°å€¼ã€‚

- seed: éšæœºç§å­ï¼Œç”¨äºå¤ç°å®éªŒç»“æœã€‚

- train_iters: å¯ä»¥æš‚æ—¶è®¾ä¸ºæ¯”è¾ƒå°çš„æ•°ï¼Œå¦‚ 10ï¼Œå®é™…ä¸Šä¸ä¼šå½±å“è®­ç»ƒæ­¥æ•°ï¼Œç•™ä½œåé¢æ‹“å±•è¯»å–å…¶ä»–å½¢å¼æ•°æ®é›†çš„åŠŸèƒ½ã€‚

- valid_iters: å¯ä»¥æš‚æ—¶è®¾ä¸ºæ¯”è¾ƒå°çš„æ•°ï¼Œå¦‚ 10ï¼Œå®é™…ä¸Šä¸ä¼šå½±å“è®­ç»ƒæ­¥æ•°ï¼Œç•™ä½œåé¢æ‹“å±•è¯»å–å…¶ä»–å½¢å¼æ•°æ®é›†çš„åŠŸèƒ½ã€‚

- evaluation_strategy: è®­ç»ƒæœŸé—´ evaluate çš„ç­–ç•¥ï¼Œ"steps"è¡¨ç¤ºæ¯éš”"valid_interval"æ­¥åšä¸€æ¬¡ evaluateï¼Œ"epoch"è¡¨ç¤ºæ¯éš”ä¸€ä¸ª epoch åšä¸€æ¬¡ evaluateï¼Œæ”¯æŒåŒæ—¶å¼€å¯ã€‚

- save_strategy: è®­ç»ƒæœŸé—´ä¿å­˜æ¨¡å‹æƒé‡çš„ç­–ç•¥ï¼Œ"steps"è¡¨ç¤ºæ¯éš”"checkpointing_steps"æ­¥ä¿å­˜ä¸€æ¬¡ã€‚

- extra_save_by_epoch: æ¯è¿‡ä¸€ä¸ª epoch æ˜¯å¦è¦ä¿å­˜ä¸€ä¸ª epoch çº§åˆ«çš„ checkpointã€‚

- save_total_limit: æœ€å¤šä¿ç•™çš„æ¨¡å‹ checkpoint ä¸ªæ•°ï¼Œä¸€èˆ¬è®¾ç½®ä¸º 2ï¼Œä¼šä¿ç•™ valid loss æœ€ä½ï¼Œä»¥åŠæœ€æ–°çš„ checkpointï¼Œæ³¨æ„ epoch çº§åˆ«çš„ checkpoint ä¼šä¸€ç›´ä¿ç•™ï¼Œä¸”ä¸å—é™åˆ¶ã€‚

- weighted_loss_mode: å¤šä»»åŠ¡è®­ç»ƒçš„ loss åŠ æƒæ–¹å¼ã€‚

## 4. æ¨¡å‹ä½¿ç”¨

### 4.1 æƒé‡åˆå¹¶

å¦‚æœä½¿ç”¨ LoRA è¿›è¡Œè®­ç»ƒï¼Œæœ¬é¡¹ç›®ä»…ä¿å­˜ adapter çš„æƒé‡å’Œé…ç½®æ–‡ä»¶ï¼Œéœ€è¦å°† adapter æƒé‡ä¸ base model è¿›è¡Œåˆå¹¶ã€‚è„šæœ¬è§`utils/merge_base_and_lora_to_hf.py`

### 4.2 æ¨¡å‹æ¨ç†

æˆ‘ä»¬æä¾›äº†å•è½®å¯¹è¯å’Œå¤šè½®å¯¹è¯çš„å¦‚ä¸‹è„šæœ¬ï¼Œè¯¥è„šæœ¬å¯åŒæ—¶å…¼å®¹å¤§éƒ¨åˆ† huggingface æ ¼å¼çš„æ¨¡å‹ã€‚

```python
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
)
tokenizer = AutoTokenizer.from_pretrained(mode_name_or_path, trust_remote_code=True, use_fast=False, legacy=False)
tokenizer.padding_side = "left"
tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids("<unk>")
tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids("</s>")
model = AutoModelForCausalLM.from_pretrained(mode_name_or_path, trust_remote_code=True)

HUMAN_ROLE_START_TAG = "<|role_start|>human<|role_end|>"
BOT_ROLE_START_TAG = "<|role_start|>bot<|role_end|>"
texts = ["write a python function of quick sort."]
texts = [f"{HUMAN_ROLE_START_TAG}{text}{BOT_ROLE_START_TAG}" for text in texts]

inputs = tokenizer(texts, return_tensors='pt', padding=True, add_special_tokens=False).to("cuda")
outputs = model.generate(
        inputs=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=512,
        top_p=0.95,
        temperature=0.1,
        do_sample=True,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id
    )
gen_text = tokenizer.batch_decode(outputs[:, inputs["input_ids"].shape[1]:], skip_special_tokens=True)
print(gen_text)
```

ç”Ÿæˆè„šæœ¬ä¸­çš„ top_pã€temperatureã€repetition_penaltyã€do_sample ç­‰å‚æ•°å¯¹æ¨¡å‹çš„ç”Ÿæˆæ•ˆæœå½±å“è¾ƒå¤§ï¼Œå¯æŒ‰ç…§è‡ªå·±çš„ä½¿ç”¨åœºæ™¯è¿›è¡Œè°ƒè¯•ä¿®æ”¹ã€‚
å®è·µä¸­ï¼Œåœ¨ä»£ç ç”Ÿæˆåœºæ™¯ä¸­ï¼Œå¦‚æœé‡‡æ ·æ¨¡å¼ï¼Œdo_sample=True, top_p=0.95, temperature=0.1 æ˜¯ pass@1 æŒ‡æ ‡çš„ä¸é”™é€‰æ‹©ï¼›
å¦‚æœéé‡‡æ ·æ¨¡å¼ï¼Œ do_sample=False, beam_num=1 æˆ–è€… 3 æ˜¯ä¸é”™çš„é€‰æ‹©ï¼Œå…¶ä¸­ beam_num=1 å³ä¸º greedy decodingã€‚

## 5. FAQ

#### é—®é¢˜ 1ï¼šOOM å¦‚ä½•è§£å†³ï¼Ÿ

å¦‚æœå‘ç”Ÿ OOMï¼Œå¯ä»¥ç¼©å° per GPU batch size (å¯åŠ¨è®­ç»ƒè„šæœ¬æ—¶çš„ç¬¬ä¸€ä¸ªå‚æ•°)ã€seq_length ç­‰å‚æ•°æ¥ç¼“è§£ã€‚ä¹Ÿå¯ä»¥è®¾ gradient_checkpointing=trueï¼Œå¯ä»¥å¤§å¹…é™ä½æ˜¾å­˜å ç”¨ï¼Œä½†è®­ç»ƒé€Ÿåº¦ä¼šå˜æ…¢ä¸€äº›ã€‚
