---
store:
  title: MFTCoder
  version: main
group:
  title: ğŸŒ± MFTCoder
  order: -1
title: Accelerate + DeepSpeed/FSDP æ¡†æ¶ç¯‡
order: 2
toc: content
---

[![Generic badge](https://img.shields.io/badge/ğŸ¤—-Huggingface%20Repo-green.svg)](https://huggingface.co/codefuse-ai)&nbsp;
<a href="https://github.com/codefuse-ai/MFTCoder/blob/main/LICENSE">
<img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue">
</a>

## 1. æ›´æ–°

ğŸ”¥ MFTCoder-accelerate æ–°å¢æ”¯æŒ accelerate + FSDP æ¡†æ¶ï¼Œ æ”¯æŒå…¨é‡å¾®è°ƒå’Œ LoRA;

ğŸ”¥ MFTCoder-accelerate æ”¯æŒæœ€æ–°æ›´å¤šä¸»æµå¼€æºæ¨¡å‹: mistral, mixtral-8x7b(Mixture of Experts), deepseek, chatglm3ï¼›

ğŸ”¥ MFTCoder-accelerate æ–°å¢ self-paced Loss, ç”¨äºæ”¶æ•›å‡è¡¡ï¼›

ğŸ”¥ MFTCoder-accelerate æ”¯æŒä½¿ç”¨ accelerate + DeepSpeed æ¡†æ¶ä¸‹æ”¯æŒ å…¨é‡å‚æ•°/QLoRA/LoRA å¾®è°ƒï¼›

ğŸ”¥ MFTCoder-accelerate åœ¨è®­ç»ƒä¸­æ”¯æŒäº†å¤šä»»åŠ¡å¾®è°ƒ MFTï¼Œ å¯ä»¥åŒæ—¶å¹³è¡¡å¤šä¸ªä»»åŠ¡çš„è®­ç»ƒï¼Œè®­ç»ƒçš„æ¨¡å‹æ”¯æŒå¤šä»»åŠ¡æ¨ç†ï¼›

ğŸ”¥ MFTCoder-accelerate åœ¨è®­ç»ƒä¸­æ”¯æŒå¤šç§æ¨¡å‹åŸºåº§ï¼š codellama, llama2, llama, starcoder, codegeex2, chatglm2, qwen ç­‰

## 2. æ•°æ®æ ¼å¼

### 2.1 è®­ç»ƒæ•°æ®æ ¼å¼

è®­ç»ƒæ•°æ®ä¸º jsonl æ ¼å¼ï¼Œæ¯ä¸€è¡Œçš„æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼Œå…¶ä¸­ chat_rounds å­—æ®µæ˜¯å¿…éœ€çš„ï¼Œå¯ä»¥æ ¹æ®å®é™…éœ€æ±‚æ·»åŠ æˆ–åˆ é™¤å…¶ä»–å­—æ®µã€‚
å¯ä»¥å‚è€ƒé¡¹ç›®ä¸­çš„ xxx.jsonl æ–‡ä»¶ã€‚

```json
{
  "id": 0,
  "data_name": "code-helper",
  "chat_rounds": [
    {
      "role": "system",
      "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä»£ç åŠ©æ‰‹ï¼Œå¯ä»¥å›å¤ç”¨æˆ·ä¸ä»£ç ç›¸å…³çš„é—®é¢˜"
    },
    {
      "role": "human",
      "content": "å†™ä¸€ä¸ªå¿«é€Ÿæ’åº"
    },
    {
      "role": "bot",
      "content": "ä»¥ä¸‹æ˜¯ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•xxxxxx"
    },
    {
      "role": "human",
      "content": "è§£é‡Šä¸€ä¸‹è¿™æ®µä»£ç "
    },
    {
      "role": "bot",
      "content": "å¥½çš„ï¼Œè¿™æ®µä»£ç xxx"
    }
  ]
}
```

### 2.2 æ¨ç†æ•°æ®æ ¼å¼

æ¨ç†æ•°æ®æ ¼å¼ä¸ºæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®æ ¼å¼ä¸‹æ‹¼æ¥çš„å­—ç¬¦ä¸²å½¢å¼ï¼Œå®ƒä¹Ÿæ˜¯æ¨ç†æ—¶è¾“å…¥ prompt æ‹¼æ¥çš„æ–¹å¼ï¼š

```
"""
<s>system
è¿™æ˜¯SystemæŒ‡ä»¤
<s>human
è¿™æ˜¯ç¬¬1è½®ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
<s>bot
è¿™æ˜¯ç¬¬1è½®æ¨¡å‹ç”Ÿæˆçš„å†…å®¹{EOS_TOKEN}
<s>human
è¿™æ˜¯ç¬¬2è½®ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
<s>bot
è¿™æ˜¯ç¬¬2è½®æ¨¡å‹ç”Ÿæˆçš„å†…å®¹{EOS_TOKEN}
...
...
...
<s>human
è¿™æ˜¯ç¬¬nè½®ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
<s>bot
{æ¨¡å‹ç°åœ¨è¦ç”Ÿæˆçš„å†…å®¹}{EOS_TOKEN}
"""
```

## 3. æ¨¡å‹è®­ç»ƒ

ç›®å‰æ”¯æŒå…¨é‡å‚æ•°(Full-parameters)æŒ‡ä»¤å¾®è°ƒã€QLoRA æŒ‡ä»¤å¾®è°ƒï¼ŒLoRA æŒ‡ä»¤å¾®è°ƒã€‚
ä¸€äº›ä¼˜ç§€çš„ä»£ç é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼Œç†è®ºä¸Šï¼ŒHuggingFace ä¸Šå¼€æºçš„æ¨¡å‹ï¼Œå‡å¯ä½¿ç”¨æœ¬é¡¹ç›®è¿›è¡Œè®­ç»ƒï¼š

ğŸ¤— [æœ€æ–°ä»£ç é¢„è®­ç»ƒ SOTAï¼ŒCodeLlama](https://huggingface.co/codellama/CodeLlama-34b-Python-hf) ï¼šcode-llama-34bï¼Œ code-llama-34b-python, æ–°çš„ SOTA åŸºåº§ã€‚

ğŸ¤— [10B çº§åˆ«æœ€ä½³ä»£ç é¢„è®­ç»ƒæ¨¡å‹ Starcoder](https://huggingface.co/bigcode/starcoder) wizardCoder-15B, PanGu-coder2 ç­‰å‰ SOTA çš„åŸºåº§æ¨¡å‹ã€‚

ğŸ¤— [å¤šè¯­è¨€èƒ½æ‰‹ Qwen-7b](https://huggingface.co/Qwen/Qwen-7B) ï¼šé€‚ç”¨äºå¤šè¯­è¨€ä»»åŠ¡ï¼Œä¹Ÿé€‚ç”¨ä¸­æ–‡ä»»åŠ¡ã€‚è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒæ—¶ã€‚

**mftcoder_accelerate æ–‡ä»¶ç»“æ„**

```
mftcoder_accelerate
       |
       src
          configs
          |
          data
          |
          model
          |
          *pefts*
          |
          tokenizer
          |
          utils
       |
       evals
```

æˆ‘ä»¬å°†è®­ç»ƒä¸­ä½¿ç”¨çš„å„ç§ç»„ä»¶æŠ½å–å‡ºæ¥ï¼Œä»¥ä¾¿åç»­çš„æ‰©å±•å’Œä¼˜åŒ–ï¼Œ è¯¦è§`src`ç›®å½•ä¸‹çš„å®ç°ã€‚

è®­ç»ƒå…¥å£æ–‡ä»¶æ˜¯`mftcoder_accelerate/src/pefts/mft_accelerate.py`

å‚æ•°é…ç½®å­˜å‚¨åœ¨`mftcoder_accelerate/src/configs`ç›®å½•ä¸‹ï¼Œæ–¹ä¾¿ç»Ÿä¸€ç®¡ç†å’Œæ›´æ”¹ã€‚

**_æ‰€ä»¥ï¼Œåœ¨ä½ å¼€å¯è®­ç»ƒä¹‹å‰ï¼Œè¯·è¿›å…¥ src ç›®å½•_**

```
cd mftcoder_accelerate/src
```

### 3.1 æ•°æ® tokenization

è®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬å°†å¤šè½®å¯¹è¯æ‹¼æ¥æˆå¦‚ä¸‹æ ¼å¼ï¼ˆä¹Ÿæ˜¯ä¸Šæ–‡ä¸­çš„æ¨ç†æ•°æ®æ ¼å¼ï¼‰ï¼Œç„¶åè¿›è¡Œ tokenizeã€‚
å…¶ä¸­ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼š

`<s>human\n`ä½œä¸º human/user çš„èµ·å§‹ç¬¦ï¼Œ`<s>bot\n`ä½œä¸º bot/assistant çš„èµ·å§‹ç¬¦ï¼Œ`{EOS_TOKEN}` è¡¨ç¤º eos_tokenã€‚
å…¶ä¸­ eos_token å¯ä»¥æ ¹æ®ä¸åŒæ¨¡å‹ä¿®æ”¹æ›¿æ¢ã€‚ä¸åŒè§’è‰²çš„èµ·å§‹ç¬¦å¯ä»¥é…ç½®ï¼Œç”¨æ¥å®ç°ä¸åŒçš„å¯¹è¯/é—®ç­”æ¨¡ç‰ˆã€‚

```
"<s>human\n{input1}<s>bot\n{target1}{EOS_TOKEN}<s>human\n{input2}<s>bot\n{target2}{EOS_TOKEN}\n"
```

åœ¨è®¡ç®— loss æ—¶ï¼Œæˆ‘ä»¬é€šè¿‡ loss mask çš„æ–¹å¼ï¼Œinput éƒ¨åˆ†çš„ loss ä¸å‚ä¸å‚æ•°æ›´æ–°ï¼Œåªæœ‰â€œtarget{EOS_TOKEN}â€éƒ¨åˆ†çš„ loss å‚ä¸å‚æ•°æ›´æ–°ã€‚
è¿™ç§æ–¹å¼å……åˆ†åˆ©ç”¨äº†æ¨¡å‹å¹¶è¡Œè®¡ç®—çš„ä¼˜åŠ¿ï¼Œè®­ç»ƒæ›´åŠ é«˜æ•ˆï¼ŒåŒæ—¶ä¹Ÿå……åˆ†åˆ©ç”¨äº† decoder-only æ¨¡å‹ä»å·¦åˆ°å³ attention çš„ç‰¹æ€§ï¼Œä¸€æ¬¡æ€§å°†å¤šè½®å¯¹è¯ä¸­çš„æ¯ä¸ª target éƒ¨åˆ†éƒ½å‚ä¸äº†è®­ç»ƒï¼Œè®­ç»ƒæ›´å……åˆ†é«˜æ•ˆã€‚

### 3.2 LoRA/QLoRA å¾®è°ƒ

#### LoRA/QLoRA å¾®è°ƒç®€ä»‹

å…³äº LoRA çš„è¯¦ç»†ä»‹ç»å¯å‚è€ƒè®ºæ–‡ï¼š[LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)

å…³äº QLoRA çš„è¯¦ç»†ä»‹ç»å¯å‚è€ƒè®ºæ–‡ï¼š[QLORA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314.pdf)

QLoRA é€šè¿‡ 4-bit çš„ nf4 é‡åŒ–ï¼Œä¸”åŠ å…¥æ›´å¤š adapterï¼Œåœ¨å¤§å¹…å‡å°‘æ˜¾å­˜æ¶ˆè€—çš„åŒæ—¶ï¼Œå°½å¯èƒ½é€¼è¿‘å…¨é‡å‚æ•°å¾®è°ƒçš„æ•ˆæœã€‚
QLoRA è®ºæ–‡æŒ‡å‡ºï¼Œè¯¥æ–¹æ³•å¯ä»¥åœ¨ä¸€å¼  V100 ä¸Šå¯¹ 33B çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶ä¸”æ€§èƒ½é€¼è¿‘å…¨é‡å‚æ•°å¾®è°ƒã€‚

æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤å³å¯è¿›è¡Œ Lora/QLora/å…¨é‡ å¾®è°ƒï¼š

#### Launch via Deepspeed

DeepSpeed é…ç½®åœ¨ accelerate_ds_config.yaml ä¸­ã€‚

```bash
accelerate launch --config_file accelerate_ds_config.yaml pefts/mft_accelerate.py --train_config configs/xxx_train_config.json --distributed_type "DeepSpeed"
```

æˆ–è€…

DeepSpeed é…ç½®åœ¨è„šæœ¬ä¸­é€šè¿‡å‘½ä»¤è¡Œè¾“å…¥ã€‚

```bash
sh ds_single_launch.sh
```

#### Launch via FSDP

FSDP é…ç½®åœ¨ accelerate_fsdp_config.yaml ä¸­ã€‚

```bash
accelerate launch --config_file accelerate_fsdp_config.yaml pefts/mft_accelerate.py --train_config configs/xxx_train_config.json --distributed_type "FSDP"
```

æˆ–è€…

FSDP é…ç½®åœ¨è„šæœ¬ä¸­é€šè¿‡å‘½ä»¤è¡Œè¾“å…¥ã€‚

```bash
sh fsdp_single_launch.sh
```

#### è®­ç»ƒå‚æ•°

_**è®­ç»ƒéœ€è¦çš„å‚æ•°é…ç½®åœ¨`configs/*_train_config`ä¸­ï¼Œä¸»è¦å‚æ•°è¯´æ˜å¦‚ä¸‹ï¼š**_

- **load_raw_dataset**: éœ€è¦ä¿æŒ trueï¼Œåç»­ä¼šæ”¯æŒå…¶å®ƒæ¨¡å¼æ•°æ®ï¼Œå½“å‰ä»…æ”¯æŒ jsonl è¾“å…¥
- **data_paths**: "[path1,path2,path3]" è¾“å…¥æ•°æ®åœ°å€ï¼Œå­—ç¬¦ä¸²ï¼Œå¼€å¤´ç»“å°¾ç”¨[]ï¼Œä¸­é—´ç”¨`,`é—´éš”ä¸åŒ pathï¼Œæ¯ä¸ª path æ˜¯ä¸€ä¸ªç›®å½•ï¼Œç›®å½•çš„æœ€åä¸€çº§åå­—ä½œä¸ºä»»åŠ¡åç§°ï¼Œä¸‹é¢åŒ…å« 1 åˆ°å¤šä¸ª jsonl æ•°æ®
- **output_dir**ï¼šè®­ç»ƒè¾“å‡ºç›®å½•ï¼Œå­˜å‚¨ checkpoint(å…¨é‡è®­ç»ƒæ—¶)ã€lora_adaptorï¼ˆLora æˆ–è€… Qlora æ—¶ï¼‰ç­‰
- **tb_dir**: å­˜å‚¨ tensorboard ç­‰
- **model_type**: "mixtral|mistral|deepseek|llama|starcoder|chatglm2|qwen|gpt_neox"
- **attn_implementation**: "flash_attention_2" æˆ–è€… "eager"
- **peft_type**: lora æˆ–è€… qlora æˆ–è€… null(å…¨é‡å¾®è°ƒ)
- **lora_rank**: lora rank
- **lora_alpha**: lora alpha
- **lora_dropout**: lora dropout
- **target_modules**: List[str], lora ç›®æ ‡æ¨¡å—ï¼Œå¦‚æœ nullï¼Œä¼šä½¿ç”¨é»˜è®¤ï¼Œå‚è€ƒ model_mapping.py
- **quantization**: æ˜¯å¦é‡åŒ–ï¼Œ"4bit", "8bit" æˆ–è€… nullï¼Œ qlora æ¨è 4bit é‡åŒ–
- **pretrained_model_path**ï¼šé¢„è®­ç»ƒæ¨¡å‹çš„æœ¬åœ°ç›®å½•ï¼Œæˆ–è€…åœ¨ huggingface ä¸Šçš„æ¨¡å‹åç§°ã€‚
- **weighted_loss_mode**: å¤šä»»åŠ¡ loss åŠ æƒæ¨¡å¼ï¼Œ "case3"æ˜¯å½“å‰æ¨èã€‚
- **padding_mode**: æ•°æ®çš„æ ·æœ¬ç»„ç»‡æ–¹å¼ï¼Œ "padding"æ˜¯å°†æ¯ä¸ªåŸå§‹æ ·æœ¬å¡«å……åˆ° seq_length, "pack"æ˜¯å°†å°½é‡å¤šçš„æ ·æœ¬æ‰“åŒ…åˆ°æ¯ä¸ª seq_length çš„åºåˆ—ä¸­ã€‚
- **num_train_epochs**ï¼šè®­ç»ƒçš„è½®æ¬¡ã€‚å¦‚æœæ•°æ®é‡è¶³å¤Ÿå¤§ï¼Œä¸€èˆ¬å»ºè®®åªè®­ 1-2 ä¸ª epochã€‚
- **per_device_train_batch_size**ï¼šæ¯å¼ æ˜¾å¡ train çš„ batch sizeã€‚
- **per_device_eval_batch_size**ï¼šæ¯å¼ æ˜¾å¡ eval çš„ batch sizeã€‚
- **gradient_accumulation_steps**ï¼šæ¢¯åº¦ç´¯è®¡æ­¥æ•°ã€‚global batch=num*gpus * per*device_train_batch_size * gradient_accumulation_stepsã€‚
- **learning_rate**ï¼šå­¦ä¹ ç‡ã€‚å…¨é‡å‚æ•°å¾®è°ƒçš„æ—¶å€™ï¼Œå»ºè®®å°ä¸€äº›ï¼Œ1e-5 æˆ– 5e-6ã€‚qlora ä¸­çš„å­¦ä¹ ç‡è®¾ç½®æ›´å¤§ä¸€äº›ï¼Œä¸€èˆ¬ä¸º 1e-4ã€2e-4ã€‚
- **min_lr**: æœ€ä½å­¦ä¹ ç‡ï¼Œ ä¸€èˆ¬æ˜¯ learning_rate çš„ååˆ†ä¹‹ä¸€
- **seq_length**ï¼šè®­ç»ƒæ—¶çš„æœ€å¤§é•¿åº¦ã€‚æŒ‰ç…§è‡ªå·±çš„è®¾å¤‡è¿›è¡Œè®¾ç½®ï¼Œè¶Šé•¿éœ€è¦å ç”¨è¶Šå¤šæ˜¾å­˜ã€‚
- **log_interval**ï¼šæ¯éš”å¤šå°‘æ­¥ç»Ÿè®¡ä¸€æ¬¡ train lossã€‚
- **checkpointing_steps**ï¼šæ¯éš”å¤šå°‘æ­¥ä¿å­˜ä¸€ä¸ªæ¨¡å‹ã€‚
- **evaluation_steps**ï¼šæ¯éš”å¤šå°‘æ­¥åœ¨éªŒè¯é›†ä¸Š evaluate ä¸€æ¬¡ã€‚
- **early_stopping** ï¼š æ˜¯å¦æ‰§è¡Œ early_stop
- **early_stopping_stall_num**ï¼š å¤šå°‘ä¸ª eval point ä¸ç»§ç»­æ”¶æ•›ï¼Œåˆ™åœæ­¢è®­ç»ƒ
- **lr_scheduler_type**ï¼šå­¦ä¹ ç‡å˜åŒ–ç­–ç•¥ã€‚å¸¸ç”¨"cosine"
- **warmup_steps**ï¼šwarm up æ­¥æ•°ã€‚å­¦ä¹ ç‡ç»è¿‡å¤šå°‘æ­¥ï¼Œå¢é•¿åˆ°æŒ‡å®šçš„æ•°å€¼ã€‚
- **seed**ï¼šéšæœºç§å­ï¼Œç”¨äºå¤ç°å®éªŒç»“æœã€‚
- **saving_limit**ï¼šæ•´æ•°ï¼Œckpt å­˜å‚¨æ•°é‡ä¸Šé™ï¼Œ å…¨é‡è®­ç»ƒå¿…é¡»è®¾ç½®ã€‚é»˜è®¤ null å³ä¸é™åˆ¶æ•°é‡ã€‚
- **role_markers**: nullï¼Œå³ä½¿ç”¨{"system": "\<s\>system\n", "user": "\<s\>human\n", "assistant": "\<s\>bot\n"}ã€‚ ä½ å¯ä»¥è‡ªå®šä¹‰ "system", "user" and "assistant"çš„æ¨¡æ¿ï¼Œ ç”¨äºå®šåˆ¶è‡ªå·±çš„é—®ç­”æˆ–è€…å¯¹è¯æ¨¡æ¿ï¼Œæ¯”å¦‚ {"system": "### System:\n", "user": "### Instruction:\n", "assistant": "### Response:\n"}

## 4. æ¨¡å‹ä½¿ç”¨

### 4.1 æƒé‡åˆå¹¶

å¦‚æœä½¿ç”¨ LoRA æˆ–è€… QLoRA è¿›è¡Œè®­ç»ƒï¼Œæœ¬é¡¹ç›®ä»…ä¿å­˜ adapter çš„æƒé‡å’Œé…ç½®æ–‡ä»¶ï¼Œéœ€è¦å°† adapter æƒé‡ä¸ base model è¿›è¡Œåˆå¹¶ã€‚
å¯ä»¥ä½¿ç”¨å¦‚ä¸‹ merge_base_and_lora_to_hf.py è„šæœ¬ã€‚

```
python pefts/merge_base_and_lora_to_hf.py \
    --base_model_or_path model_path \
    --adaptor_path lora_adapter_path \
    --model_type model_type \
    --merged_output_path output_path
```

### 4.2 æ¨¡å‹æ¨ç†

æˆ‘ä»¬æä¾›äº†å•è½®å¯¹è¯å’Œå¤šè½®å¯¹è¯çš„å¦‚ä¸‹è„šæœ¬ï¼Œè¯¥è„šæœ¬å¯åŒæ—¶å…¼å®¹å¤§éƒ¨åˆ† huggingface æ ¼å¼çš„æ¨¡å‹ã€‚

```python
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
)
model_name_or_path = "codefuse-ai/CodeFuse-Deepseek-33B"
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True, padding_side="left")
tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids("<ï½œendâ–ofâ–sentenceï½œ>")
tokenizer.pad_token_id = tokenizer.eos_token_id
model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True)

HUMAN_ROLE_START_TAG = "<s>human\n"
BOT_ROLE_START_TAG = "<s>bot\n"
texts = ["write a python function of quick sort."]
texts = [f"{HUMAN_ROLE_START_TAG}{text}{BOT_ROLE_START_TAG}" for text in texts]

inputs = tokenizer(texts, return_tensors='pt', padding=True, add_special_tokens=False).to("cuda")
outputs = model.generate(
        inputs=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=512,
        top_p=0.95,
        temperature=0.1,
        do_sample=True,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id
    )
gen_text = tokenizer.batch_decode(outputs[:, inputs["input_ids"].shape[1]:], skip_special_tokens=True)
print(gen_text)
```

ç”Ÿæˆè„šæœ¬ä¸­çš„ top_pã€temperatureã€repetition_penaltyã€do_sample ç­‰å‚æ•°å¯¹æ¨¡å‹çš„ç”Ÿæˆæ•ˆæœå½±å“è¾ƒå¤§ï¼Œå¯æŒ‰ç…§è‡ªå·±çš„ä½¿ç”¨åœºæ™¯è¿›è¡Œè°ƒè¯•ä¿®æ”¹ã€‚
å®è·µä¸­ï¼Œåœ¨ä»£ç ç”Ÿæˆåœºæ™¯ä¸­ï¼Œå¦‚æœé‡‡æ ·æ¨¡å¼ï¼Œdo_sample=True, top_p=0.95, temperature=0.1 æ˜¯ pass@1 æŒ‡æ ‡çš„ä¸é”™é€‰æ‹©ï¼›
å¦‚æœéé‡‡æ ·æ¨¡å¼ï¼Œ do_sample=False, beam_num=1 æˆ–è€… 3 æ˜¯ä¸é”™çš„é€‰æ‹©ï¼Œå…¶ä¸­ beam_num=1 å³ä¸º greedy decodingã€‚

## 5. FAQ

#### é—®é¢˜ 1ï¼šOOM å¦‚ä½•è§£å†³ï¼Ÿ

å¦‚æœå‘ç”Ÿ OOMï¼Œå¯ä»¥ç¼©å° per_device_train_batch_sizeã€seq_length ç­‰å‚æ•°æ¥ç¼“è§£ã€‚ç”±äºé¢å¯¹çš„æ¨¡å‹æ™®éè¾ƒå¤§ï¼ˆ6bï¼Œ 13bï¼Œ 34bï¼Œ 70b ç­‰ï¼‰æˆ‘ä»¬å·²ç»é»˜è®¤ä½¿ç”¨ gradient_checkpointing æŠ€æœ¯ï¼Œå¯ä»¥å¤§å¹…é™ä½æ˜¾å­˜å ç”¨ï¼Œä½†è®­ç»ƒé€Ÿåº¦ä¼šç¨æ…¢ä¸€äº›ã€‚

#### é—®é¢˜ 2ï¼šå®‰è£…åŒ…é”™è¯¯

å‚è€ƒ init_env.sh å’Œ requirements.txt

#### é—®é¢˜ 3ï¼šå¦‚ä½•æŒ‡å®šä½¿ç”¨æŸäº›å¡è®­ç»ƒï¼Ÿ

é€šè¿‡å¦‚ä¸‹æ–¹å¼ï¼Œå³å¯æŒ‡å®šä½¿ç”¨ 0 å’Œ 1 å·å¡è¿›è¡Œè®­ç»ƒ:

```bash
CUDA_VISIBLE_DEVICES=0,1 accelerate launch --config_file pefts/accelerate_ds_config.yaml pefts/mft_accelerate.py --train_config configs/xxx_train_config.json --distributed_type "deepspeed"
```

#### é—®é¢˜ 4ï¼šå…³äº Flash Attention, è¯¥å¦‚ä½•é…ç½®è®­ç»ƒï¼Ÿ

é¦–å…ˆï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®æ‚¨å®‰è£… Flash Attention 2(FA2)ï¼Œï¼ˆ>=2.1.0, 2.3.6 åŠŸèƒ½æ›´é½å…¨ï¼‰ã€‚

è®­ç»ƒå‚æ•°ä¸­"attn_implementation" è®¾ç½®æˆ "eager" å¯ä»¥ç”¨ naive attentionï¼Œä¹Ÿå°±æ˜¯æœªç»åŠ é€Ÿçš„ attentionã€‚

è®­ç»ƒå‚æ•°ä¸­"attn_implementation" è®¾ç½®æˆ "flash_attention_2" å¯ä»¥ç”¨ FA2ï¼Œé€Ÿåº¦å¿«ï¼Œçœæ˜¾å­˜ã€‚

å¦‚æœä½ å¯ä»¥è‡ªè¡Œå®‰è£…ç¯å¢ƒå¹¶ä½¿ç”¨ torch>=2.1.1ï¼Œå¯ä»¥å°è¯•è®¾ç½®å‚æ•°"attn_implementation"ä¸º "sdpa"ã€‚è¿™æ ·ä¼šå°è¯•ä½¿ç”¨ transformers å…¼å®¹çš„ torch.nn.functional.scaled_dot_product_attentionã€‚æ”¯æŒçš„æ¨¡å‹è¿˜ä¸å…¨é¢ã€‚

#### é—®é¢˜ 5ï¼šæ¨èçš„åˆ†å¸ƒå¼æ¡†æ¶æ˜¯æ€æ ·çš„ï¼Ÿ

å¯¹äº LoRA/QLoRA, æˆ‘ä»¬æ¨èä½¿ç”¨ DeepSpeed ä½œä¸ºåº•å±‚åˆ†å¸ƒå¼æ¡†æ¶ï¼Œå®ƒå…·æœ‰æ˜“ç”¨æ€§å’Œå…¼å®¹æ€§å¥½çš„ç‰¹ç‚¹ï¼Œå¹¶ä¸”é€Ÿåº¦å¾ˆå¿«ã€‚
FSDP ä¸æ”¯æŒ QLoRA, å› ä¸º bitsandbytes æš‚ä¸æ”¯æŒ FSDPã€‚

å¯¹äºå…¨é‡å¾®è°ƒï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨ FSDPï¼Œ å› ä¸ºå®ƒåœ¨å…¨é‡è®­ç»ƒæ—¶å¯ä»¥å‘æŒ¥ fully sharding çš„ä¼˜åŠ¿ï¼Œè¾¾åˆ°æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦ã€‚

#### é—®é¢˜ 6ï¼šå½“å‰æ”¯æŒçš„æ¨¡å‹ä¸­ï¼Œæœ‰ä»€ä¹ˆåŒºåˆ«

å›½äº§å¤§æ¨¡å‹æ¯”å¦‚ chatglm2ï¼Œ chatglm3ï¼Œ baichuan2ï¼Œ qwenï¼Œ aquila2 ç­‰ï¼Œä½¿ç”¨çš„æ˜¯å’Œæ¨¡å‹å…±åŒå‘å¸ƒçš„ modeling_xxx.py.
å…¶å®ƒè¢« transformers å®˜æ–¹æ”¯æŒçš„å¤§æ¨¡å‹ï¼Œç”±äºå·²ç»å‡çº§æ”¯æŒ flash attention ç­‰ï¼Œæ‰€ä»¥å…¨é¢åˆ‡æ¢åˆ°å®˜æ–¹çš„ modeling æ”¯æŒè®­ç»ƒï¼Œä¹‹å‰çš„è‡ªå®šä¹‰ modeling ä¼šè¢« deprecated
