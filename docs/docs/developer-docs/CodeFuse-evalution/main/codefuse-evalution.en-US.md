---
nav:
  title: Docs
  order: -1
  second:
    title: Developer-Docs
    order: -1
store:
  title: CodeFuse-evalution
  version: main
group:
  title: ðŸŒ± CodeFuse-evalution
  index: true
  order: -1
title: CodeFuse-evalution
order: -1
toc: content
github: https://github.com/codefuse-ai/codefuse-evaluation
---

# CodeFuseEval: Multi-tasking Evaluation Benchmark for Code Large Language Model

<div align="center">
  <p>
    <a href="https://modelscope.cn/datasets/codefuse-ai/CodeFuseEval/summary" target="_blank">CodeFuseEval on ModelScope</a>ï½œ
    <a href="https://huggingface.co/datasets/codefuse-ai/CodeFuseEval" target="_blank">CodeFuseEval on Hugging Face</a>
  </p>
  
</div>

CodeFuseEval is a Code Generation benchmark that combines the multi-tasking scenarios of CodeFuse Model with the benchmarks of HumanEval-x and MBPP. This benchmark is designed to evaluate the performance of models in various multi-tasking tasks, including code completion, code generation from natural language, test case generation, cross-language code translation, and code generation from Chinese commands, among others.Continuously open, stay tuned !

<p>
    <img src="https://mdn.alipayobjects.com/huamei_bvbxju/afts/img/A*1btDRYRFVEQAAAAAAAAAAAAADlHYAQ/original" alt="English Introduction"/>
</p>
