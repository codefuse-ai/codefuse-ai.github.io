---
store:
  title: CodeFuse-DevOps-Model
  version: main
group:
  title: ğŸŒ± CodeFuse-DevOps-Model
  order: -1
title: QuickStart
order: 1
toc: content
github: https://github.com/codefuse-ai/CodeFuse-DevOps-Model
---

## Dependency Installation

Please install the packages listed in the requirements.txt file from the GitHub address first. You can refer to the following code:

```
pip install -r requirements.txt
```

## Model Download

Model download information is as follows:

ğŸ¤— Huggingface Address

| -   | Base Model            | Aligned Model         |
| --- | --------------------- | --------------------- |
| 7B  | DevOps-Model-7B-Base  | DevOps-Model-7B-Chat  |
| 14B | DevOps-Model-14B-Base | DevOps-Model-14B-Chat |

ğŸ¤– ModelScope Address

| -   | Base Model            | Aligned Model         |
| --- | --------------------- | --------------------- |
| 7B  | DevOps-Model-7B-Base  | DevOps-Model-7B-Chat  |
| 14B | DevOps-Model-14B-Base | DevOps-Model-14B-Chat |

Find the version of the Chat model you want to download; currently, 7B and 14B models are provided.

## Model Usage

Interact with the Chat model using the following code:

```
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

tokenizer = AutoTokenizer.from_pretrained("path_to_DevOps-Model-Chat", trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained("path_to_DevOps-Model-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()

# æŒ‡å®š generation_config
model.generation_config = GenerationConfig.from_pretrained("path_to_DevOps-Model-Chat", trust_remote_code=True)

# First round of conversation
resp, hist = model.chat(query='ä½ æ˜¯è°', tokenizer=tokenizer, history=None)
print(resp)
# æˆ‘æ˜¯ DevOps-Modelï¼Œä¸€ä¸ªç”±èš‚èšé›†å›¢å¹³å°æŠ€æœ¯äº‹ä¸šç¾¤é£é™©æ™ºèƒ½å›¢é˜Ÿå’ŒåŒ—äº¬å¤§å­¦è”åˆç ”å‘çš„äººå·¥æ™ºèƒ½æœºå™¨äººï¼Œå¯ä»¥ä¸ç”¨æˆ·è¿›è¡Œè‡ªç„¶è¯­è¨€äº¤äº’ï¼Œå¹¶ååŠ©è§£ç­” DevOps å…¨ç”Ÿå‘½å‘¨æœŸä¸­çš„å„ç§é—®é¢˜ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•éœ€è¦ååŠ©çš„é—®é¢˜æˆ–è€…æƒ³è¦è¿›è¡Œé—²èŠï¼Œéƒ½å¯ä»¥å’Œæˆ‘äº¤æµå“¦ã€‚

# Second round of conversation
resp2, hist2 = model.chat(query='Java ä¸­ HashMap å’Œ Hashtable æœ‰ä»€ä¹ˆåŒºåˆ«', tokenizer=tokenizer, history=hist)
print(resp2)
# HashMap å’Œ Hashtable éƒ½æ˜¯ Java ä¸­å¸¸ç”¨çš„å“ˆå¸Œè¡¨å®ç°ï¼Œå®ƒä»¬çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼š
# 1. Hashtable æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œè€Œ HashMap ä¸æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œå› æ­¤åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹ï¼ŒHashtable çš„æ€§èƒ½æ›´ç¨³å®šã€‚
# 2. Hashtable ä¸­çš„æ–¹æ³•éƒ½æ˜¯åŒæ­¥çš„ï¼Œè€Œ HashMap çš„æ–¹æ³•ä¸æ˜¯åŒæ­¥çš„ï¼Œå› æ­¤åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹ï¼ŒHashtable çš„æ€§èƒ½æ›´å¥½ã€‚
# 3. Hashtable ä¸­çš„ key å’Œ value éƒ½å¿…é¡»å®ç° Serializable æ¥å£ï¼Œè€Œ HashMap ä¸­çš„ key å’Œ value å¯ä»¥æ˜¯ä»»ä½•å¯¹è±¡ï¼ŒåŒ…æ‹¬åŸºæœ¬æ•°æ®ç±»å‹ã€‚
# 4. Hashtable çš„åˆå§‹å®¹é‡æ˜¯ 11ï¼Œè€Œ HashMap çš„åˆå§‹å®¹é‡æ˜¯ 16ã€‚
# æ€»ä¹‹ï¼Œå¦‚æœéœ€è¦åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹ä½¿ç”¨å“ˆå¸Œè¡¨ï¼Œå¹¶ä¸”éœ€è¦ä¿è¯çº¿ç¨‹å®‰å…¨ï¼Œé‚£ä¹ˆåº”è¯¥ä½¿ç”¨ Hashtableï¼›å¦‚æœä¸éœ€è¦è€ƒè™‘çº¿ç¨‹å®‰å…¨ï¼Œæˆ–è€…éœ€è¦å¿«é€Ÿåœ°è¿›è¡Œå“ˆå¸Œè¡¨æ“ä½œï¼Œé‚£ä¹ˆåº”è¯¥ä½¿ç”¨ HashMapã€‚

# Third round of conversation
resp3, hist3 = model.chat(query='çº¿ç¨‹å®‰å…¨ä»£è¡¨ä»€ä¹ˆ', tokenizer=tokenizer, history=hist2)
print(resp3)
# çº¿ç¨‹å®‰å…¨æ˜¯æŒ‡åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹ï¼Œç¨‹åºèƒ½å¤Ÿæ­£ç¡®åœ°å¤„ç†å¹¶å‘è®¿é—®ï¼Œå¹¶ä¸”ä¸ä¼šå‡ºç°æ•°æ®ç«äº‰ã€æ­»é”ã€é¥¥é¥¿ç­‰å¼‚å¸¸æƒ…å†µã€‚çº¿ç¨‹å®‰å…¨çš„ç¨‹åºå¯ä»¥ä¿è¯åœ¨ä¸åŒçš„çº¿ç¨‹ä¹‹é—´å…±äº«åŒä¸€ä¸ªæ•°æ®ç»“æ„æ—¶ï¼Œæ•°æ®çš„æ­£ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚çº¿ç¨‹å®‰å…¨çš„å®ç°é€šå¸¸éœ€è¦ä½¿ç”¨åŒæ­¥æœºåˆ¶ï¼Œå¦‚é”ã€åŸå­æ“ä½œç­‰ï¼Œæ¥ä¿è¯å¯¹å…±äº«æ•°æ®çš„è®¿é—®æ˜¯çº¿ç¨‹å®‰å…¨çš„ã€‚åœ¨ Java ä¸­ï¼Œå¯ä»¥é€šè¿‡ synchronized å…³é”®å­—ã€Lock æ¥å£ç­‰æœºåˆ¶æ¥å®ç°çº¿ç¨‹å®‰å…¨ã€‚
```
