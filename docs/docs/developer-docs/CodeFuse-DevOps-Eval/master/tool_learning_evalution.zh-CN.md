---
store:
  title: CodeFuse-DevOps-Eval
  version: master
group:
  title: ğŸŒ± CodeFuse-DevOps-Eval
  order: -1
order: 1
title: Tool Learning æ•°æ®é›†è¯„æµ‹æ•™ç¨‹
toc: content
---

## tool learning æ•°æ®é›†è¯„æµ‹æ•™ç¨‹

### chatml æ¥å…¥æ–¹å¼

å¦‚æœéœ€è¦åœ¨è‡ªå·±çš„ huggingface æ ¼å¼çš„æ¨¡å‹ä¸Šè¿›è¡Œæµ‹è¯•çš„è¯ï¼Œæ€»çš„æ­¥éª¤åˆ†ä¸ºå¦‚ä¸‹å‡ æ­¥:

1. ç¼–å†™ ~/evals/FuncCallEvalution çš„ create_prompts å‡½æ•°
2. ç¼–å†™ ~/models/base_model çš„ ç›¸å…³å‡½æ•°
3. æ³¨å†Œæ¨¡å‹å’Œè¯„ä¼°å‡½æ•°
4. æ‰§è¡Œæµ‹è¯•è„šæœ¬
   å¦‚æœæ¨¡å‹åœ¨åŠ è½½è¿›æ¥åä¸éœ€è¦ç‰¹æ®Šçš„å¤„ç†ï¼Œè€Œä¸”è¾“å…¥ä¹Ÿä¸éœ€è¦è½¬æ¢ä¸ºç‰¹å®šçš„æ ¼å¼ï¼ˆe.g. chatml æ ¼å¼æˆ–è€…å…¶ä»–çš„ human-bot æ ¼å¼ï¼‰ï¼Œè¯·ç›´æ¥è·³è½¬åˆ°ç¬¬å››æ­¥ç›´æ¥å‘èµ·æµ‹è¯•ã€‚

#### 1. ç¼–å†™ loader å‡½æ•°

å¦‚æœæ¨¡å‹åœ¨åŠ è½½è¿›æ¥è¿˜éœ€è¦åšä¸€äº›é¢å¤–çš„å¤„ç†ï¼ˆe.g. tokenizer è°ƒæ•´ï¼‰ï¼Œéœ€è¦å» `src.context_builder.context_builder_family.py` ä¸­ç»§æ‰¿ `ModelAndTokenizerLoader` ç±»æ¥è¦†å†™å¯¹åº”çš„ `load_model` å’Œ `load_tokenizer` å‡½æ•°ï¼Œå…·ä½“å¯ä»¥å‚ç…§ä»¥ä¸‹ç¤ºä¾‹ï¼š

```python
class FuncCallEvalution(ToolEvalution):

    def create_prompts(self, func_call_datas):
        '''
        datas: [
            {
                "instruction": history[his_idx],
                "input": "",
                "output": output,
                "history": [(human_content, ai_content), (), ()],
                "functions": tools
            }
        ]
        '''
        system_content = '''CodeFuseæ˜¯ä¸€ä¸ªé¢å‘ç ”å‘é¢†åŸŸçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œæ—¨åœ¨ä¸­ç«‹çš„ã€æ— å®³çš„å¸®åŠ©ç”¨æˆ·è§£å†³å¼€å‘ç›¸å…³çš„é—®é¢˜ï¼Œæ‰€æœ‰çš„å›ç­”å‡ä½¿ç”¨Markdownæ ¼å¼è¿”å›ã€‚
        ä½ èƒ½åˆ©ç”¨è®¸å¤šå·¥å…·å’ŒåŠŸèƒ½æ¥å®Œæˆç»™å®šçš„ä»»åŠ¡ï¼Œåœ¨æ¯ä¸€æ­¥ä¸­ï¼Œä½ éœ€è¦åˆ†æå½“å‰çŠ¶æ€ï¼Œå¹¶é€šè¿‡æ‰§è¡Œå‡½æ•°è°ƒç”¨æ¥ç¡®å®šä¸‹ä¸€æ­¥çš„è¡ŒåŠ¨æ–¹å‘ã€‚ä½ å¯ä»¥è¿›è¡Œå¤šæ¬¡å°è¯•ã€‚å¦‚æœä½ è®¡åˆ’è¿ç»­å°è¯•ä¸åŒçš„æ¡ä»¶ï¼Œè¯·æ¯æ¬¡å°è¯•ä¸€ç§æ¡ä»¶ã€‚è‹¥ç»™å®šäº†Finishå‡½æ•°,åˆ™ä»¥Finishè°ƒç”¨ç»“æŸï¼Œè‹¥æ²¡æä¾›Finishå‡½æ•°ï¼Œåˆ™ä»¥ä¸å¸¦function_callçš„å¯¹è¯ç»“æŸã€‚'''
        function_format = '''You are ToolGPT, you have access to the following APIs:\n{tools}'''

        func_call_train_datas = []
        history_error_cnt = 0
        funccall_error_cnt = 0

        for data in func_call_datas:
            tools = data["functions"]
            chatrounds = data["chatrounds"]

            function_content = ""
            if len(tools) > 0:
                function_content = function_format.format(tools=json.dumps(tools, ensure_ascii=False, sort_keys=True))

            history = []
            for i in chatrounds:
                if i["role"]=="system":
                    continue

                if i["role"]=="user":
                    history.append(("user", i["content"]))

                if i["role"] == "assistant":
                    if "function_call" in i:
                        if not isinstance(i["function_call"], dict):
                            funccall_error_cnt+=1
                            continue
                        content  = "#function" + json.dumps({**{"content": i["content"]}, **i["function_call"]}, ensure_ascii=False)
                    else:
                        content = i["content"]
                    history.append(("assistant", content))


                if i["role"] == "function":
                    content  = json.dumps({**{"content": i["content"]}, **{"name": i["name"]}}, ensure_ascii=False)
                    history.append(("user", content))


            history = [i[1] for i in history]
            history[0] = "\n".join([system_content,function_content, history[0]])

            for his_idx in range(0, len(history), 2):
                output = history[his_idx+1]

                if "#function" in output:
                    output = output.split("#function")[-1]

                try:
                    output = json.loads(output)
                except:
                    output = {"content": output}


                func_call_train_datas.append(
                    {
                        "instruction": history[his_idx],
                        "input": "",
                        "output": output,
                        "history": [history[:his_idx+2][i:i+2] for i in range(0, len(history[:his_idx]), 2)],
                        "functions": tools
                    },
                )
        return func_call_train_datas
```

#### 2. ç¼–å†™ Model çš„ context_builder å‡½æ•°

å¦‚æœè¾“å…¥éœ€è¦è½¬æ¢ä¸ºç‰¹å®šçš„æ ¼å¼ï¼ˆe.g. chatml æ ¼å¼æˆ–è€…å…¶ä»–çš„ human-bot æ ¼å¼ï¼‰ï¼Œåˆ™éœ€è¦å» `src.context_builder.context_builder_family` ä¸­ç»§æ‰¿ ContextBuilder ç±»æ¥è¦†å†™ make_context å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°æ˜¯ç”¨æ¥å°†è¾“å…¥è½¬æ¢æ ¼å¼ä¸ºå¯¹åº”éœ€è¦çš„è¾“å‡ºçš„ï¼Œä¸€ä¸ªç¤ºä¾‹å¦‚ä¸‹ï¼š

```python
class ToolModel:
    def __init__(self, model_path: str, template: str, trust_remote_code=True, tensor_parallel_size=1, gpu_memory_utilization=0.25):
        self.model_path = model_path
        self.trust_remote_code = trust_remote_code
        self.tensor_parallel_size = tensor_parallel_size
        self.gpu_memory_utilization = gpu_memory_utilization
        self.load_model(self.model_path, self.trust_remote_code, self.tensor_parallel_size, self.gpu_memory_utilization)

    def generate(self, prompts: str, template: str = None, generate_configs: GenerateConfigs = None) -> list:
        '''äº§å‡ºå¯¹åº”ç»“æœ'''
        pass

    def generate_params(
        self, generate_configs: GenerateConfigs,
    ):
        '''generate param'''
        kargs = generate_configs.dict()
        return kargs

    def load_model(self, model_path, trust_remote_code=True, tensor_parallel_size=1, gpu_memory_utilization=0.25):
        '''åŠ è½½æ¨¡å‹'''
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=trust_remote_code)
        self.model = AutoModelForCausalLM.from_pretrained(self.model_path, device_map="auto", trust_remote_code=trust_remote_code).eval()

        # self.model = LLM(model=model_path, trust_remote_code=trust_remote_code, tensor_parallel_size=tensor_parallel_size, gpu_memory_utilization=gpu_memory_utilization)
```

#### 3. æ³¨å†Œæ¨¡å‹å’Œ eval å‡½æ•°å³å¯

åœ¨ ~/models/**init**.py ä¸­æ³¨å†Œå³å¯

```python
from .base_model import ToolModel

__all__ = [
    "ToolModel",
]
```

åœ¨ ~/evasl/**init**.py ä¸­æ³¨å†Œå³å¯

```python
from .base_evalution import ToolEvalution
from .toolfill_evalution import ToolFillEvalution
from .toolparser_evalution import ToolParserEvalution
from .toolsummary_evalution import ToolSummaryEvalution
from .func_call_evalution import FuncCallEvalution


__all__ = [
    "ToolEvalution", "ToolFillEvalution", "ToolParserEvalution", "ToolSummaryEvalution", "FuncCallEvalution"
]
```

#### 4. æ‰§è¡Œæµ‹è¯•è„šæœ¬

ä¿®æ”¹ ~/src/qwen_eval_main.py# datainfos å’Œ model_infos

```python
model_infos = [
    {"model_name": "", "template": "chatml", "model_path": "",
     "peft_path": "", "model_class": QwenModel}]

datainfos = [
    {"dataset_path": "~/fcdata_luban_zh_test.jsonl", "dataset_name": "fcdata_luban_zh", "tool_task": "func_call"},
    {"dataset_path": "~/test_datas/fcdata_zh_test_v1.jsonl", "dataset_name": "fcdata_zh", "tool_task": "func_call"},
]
```

è¿è¡Œä¸‹è¿°å‘½ä»¤å³å¯

```Bash
python qwen_eval_main.py
```

<br>

### é chatml æ¥å…¥

å¦‚æœéœ€è¦åœ¨è‡ªå·±çš„ huggingface æ ¼å¼çš„æ¨¡å‹ä¸Šè¿›è¡Œæµ‹è¯•çš„è¯ï¼Œæ€»çš„æ­¥éª¤åˆ†ä¸ºå¦‚ä¸‹å‡ æ­¥:

1. ç¼–å†™ ~/getAssistantAns.py ç›¸å…³ä»£ç 
2. æ‰§è¡Œæµ‹è¯•è„šæœ¬

#### 1ã€ç¼–å†™ getAssistantAns ç¤ºä¾‹

```
class GetAssistantAns():
    # æŒ‰ç…§è‡ªå·±æ¨ç†éœ€æ±‚è‡ªå·±ä¿®æ”¹ä»£ç 

    def __init__(self, gpu_num=1):
        model = AutoModelForCausalLM.from_pretrained(model_name)
        device_list = []
        for gpu_idx in range(gpu_num):
            device_list.append(torch.device("cuda:0"))

        # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šçš„GPUè®¾å¤‡
        model.to(device)


    def gen_answer(self, chat_dict, gpu_index):
        # è¿™é‡Œå®é™…æ ¹æ®è‡ªå·±æ¨ç†é€»è¾‘ ç„¶åè½¬ä¸ºæ ‡å‡†æ ¼å¼è¿”å›
        # ä»¥ä¸‹ä»…ä»…æ˜¯æ ·ä¾‹
        import time
        print(os.environ["CUDA_VISIBLE_DEVICES"])
        time.sleep(1)
        rtn_dict1 = {
                "role": "assistant",
                "content": None,
                "function_call":
                {
                    "name": "get_fudan_university_scoreline",
                    "arguments": "{\n  \"year\": \"2020\"\n}"
                }
            }

        rtn_dict2 =  {
                "role": "assistant",
                "content": "2020å¹´å¤æ—¦å¤§å­¦çš„åˆ†æ•°çº¿å¦‚ä¸‹ï¼š\n\n- æ–‡ç§‘ä¸€æ‰¹ï¼š630åˆ†\n- æ–‡ç§‘äºŒæ‰¹ï¼š610åˆ†\n- ç†ç§‘ä¸€æ‰¹ï¼š650åˆ†\n- ç†ç§‘äºŒæ‰¹ï¼š630åˆ†"
            }

        return random.choice([rtn_dict1, rtn_dict2])
```

#### 2ã€æ‰§è¡Œæµ‹è¯•è„šæœ¬

ä¿®æ”¹ ~/src/opensource_functioncall_evalution.py # test_ans_file_list

```python
test_ans_file_list = [
        "fcdata_zh_test.jsonl"
        ]
```

è¿è¡Œä¸‹è¿°å‘½ä»¤å³å¯

```Bash
python opensource_functioncall_evalution.py
```
