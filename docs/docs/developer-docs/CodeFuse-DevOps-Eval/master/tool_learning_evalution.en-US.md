---
store:
  title: CodeFuse-DevOps-Eval
  version: master
# resource: true
# title: Tool Learning æ•°æ®é›†è¯„æµ‹æ•™ç¨‹
group:
  title: ğŸŒ± CodeFuse-DevOps-Eval
  order: -1
order: 1
title: Tool Learning Evaluate
toc: content
---

## Tool Learning Dataset Evaluation Tutorial

### ChatML Integration Methods

To test on your own Huggingface-format model, the overall steps are divided into the following:

1. Write the `create_prompts` function in `~/evals/FuncCallEvaluation`
2. Write the related functions in `~/models/base_model`
3. Register the model and evaluation functions
4. Execute the test script
   If the model does not require special handling after loading, and the input does not need to be converted into a specific format (e.g. ChatML format or other human-bot formats), please skip directly to step four and start testing.

#### 1. Write the Loader Function

If the model needs additional processing after loading (e.g., tokenizer adjustments), you will need to inherit the `ModelAndTokenizerLoader` class in `src.context_builder.context_builder_family.py` and override the corresponding `load_model` and `load_tokenizer` functions.

An example is as follows:

```python
class FuncCallEvalution(ToolEvalution):

    def create_prompts(self, func_call_datas):
        '''
        datas: [
            {
                "instruction": history[his_idx],
                "input": "",
                "output": output,
                "history": [(human_content, ai_content), (), ()],
                "functions": tools
            }
        ]
        '''
        system_content = '''CodeFuseæ˜¯ä¸€ä¸ªé¢å‘ç ”å‘é¢†åŸŸçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œæ—¨åœ¨ä¸­ç«‹çš„ã€æ— å®³çš„å¸®åŠ©ç”¨æˆ·è§£å†³å¼€å‘ç›¸å…³çš„é—®é¢˜ï¼Œæ‰€æœ‰çš„å›ç­”å‡ä½¿ç”¨Markdownæ ¼å¼è¿”å›ã€‚
        ä½ èƒ½åˆ©ç”¨è®¸å¤šå·¥å…·å’ŒåŠŸèƒ½æ¥å®Œæˆç»™å®šçš„ä»»åŠ¡ï¼Œåœ¨æ¯ä¸€æ­¥ä¸­ï¼Œä½ éœ€è¦åˆ†æå½“å‰çŠ¶æ€ï¼Œå¹¶é€šè¿‡æ‰§è¡Œå‡½æ•°è°ƒç”¨æ¥ç¡®å®šä¸‹ä¸€æ­¥çš„è¡ŒåŠ¨æ–¹å‘ã€‚ä½ å¯ä»¥è¿›è¡Œå¤šæ¬¡å°è¯•ã€‚å¦‚æœä½ è®¡åˆ’è¿ç»­å°è¯•ä¸åŒçš„æ¡ä»¶ï¼Œè¯·æ¯æ¬¡å°è¯•ä¸€ç§æ¡ä»¶ã€‚è‹¥ç»™å®šäº†Finishå‡½æ•°,åˆ™ä»¥Finishè°ƒç”¨ç»“æŸï¼Œè‹¥æ²¡æä¾›Finishå‡½æ•°ï¼Œåˆ™ä»¥ä¸å¸¦function_callçš„å¯¹è¯ç»“æŸã€‚'''
        function_format = '''You are ToolGPT, you have access to the following APIs:\n{tools}'''

        func_call_train_datas = []
        history_error_cnt = 0
        funccall_error_cnt = 0

        for data in func_call_datas:
            tools = data["functions"]
            chatrounds = data["chatrounds"]

            function_content = ""
            if len(tools) > 0:
                function_content = function_format.format(tools=json.dumps(tools, ensure_ascii=False, sort_keys=True))

            history = []
            for i in chatrounds:
                if i["role"]=="system":
                    continue

                if i["role"]=="user":
                    history.append(("user", i["content"]))

                if i["role"] == "assistant":
                    if "function_call" in i:
                        if not isinstance(i["function_call"], dict):
                            funccall_error_cnt+=1
                            continue
                        content  = "#function" + json.dumps({**{"content": i["content"]}, **i["function_call"]}, ensure_ascii=False)
                    else:
                        content = i["content"]
                    history.append(("assistant", content))


                if i["role"] == "function":
                    content  = json.dumps({**{"content": i["content"]}, **{"name": i["name"]}}, ensure_ascii=False)
                    history.append(("user", content))


            history = [i[1] for i in history]
            history[0] = "\n".join([system_content,function_content, history[0]])

            for his_idx in range(0, len(history), 2):
                output = history[his_idx+1]

                if "#function" in output:
                    output = output.split("#function")[-1]

                try:
                    output = json.loads(output)
                except:
                    output = {"content": output}


                func_call_train_datas.append(
                    {
                        "instruction": history[his_idx],
                        "input": "",
                        "output": output,
                        "history": [history[:his_idx+2][i:i+2] for i in range(0, len(history[:his_idx]), 2)],
                        "functions": tools
                    },
                )
        return func_call_train_datas
```

#### 2. Write the Model's Context Builder Function

If the input needs to be converted to a specific format (e.g., ChatML format or other human-bot formats), then you will need to go to `src.context_builder.context_builder_family` and inherit the `ContextBuilder` class to override the `make_context` function. This function is used to convert the input to the corresponding required output. An example is as follows:

```python
class ToolModel:
    def __init__(self, model_path: str, template: str, trust_remote_code=True, tensor_parallel_size=1, gpu_memory_utilization=0.25):
        self.model_path = model_path
        self.trust_remote_code = trust_remote_code
        self.tensor_parallel_size = tensor_parallel_size
        self.gpu_memory_utilization = gpu_memory_utilization
        self.load_model(self.model_path, self.trust_remote_code, self.tensor_parallel_size, self.gpu_memory_utilization)

    def generate(self, prompts: str, template: str = None, generate_configs: GenerateConfigs = None) -> list:
        '''äº§å‡ºå¯¹åº”ç»“æœ'''
        pass

    def generate_params(
        self, generate_configs: GenerateConfigs,
    ):
        '''generate param'''
        kargs = generate_configs.dict()
        return kargs

    def load_model(self, model_path, trust_remote_code=True, tensor_parallel_size=1, gpu_memory_utilization=0.25):
        '''åŠ è½½æ¨¡å‹'''
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=trust_remote_code)
        self.model = AutoModelForCausalLM.from_pretrained(self.model_path, device_map="auto", trust_remote_code=trust_remote_code).eval()

        # self.model = LLM(model=model_path, trust_remote_code=trust_remote_code, tensor_parallel_size=tensor_parallel_size, gpu_memory_utilization=gpu_memory_utilization)
```

#### 3. Register the Model and Eval Function

Registration can be done in `~/models/__init__.py`

```python
from .base_model import ToolModel
__all__ = [
    "ToolModel",
]
```

Registration can also be done in `~/evals/__init__.py`

```python
from .base_evaluation import ToolEvaluation
from .toolfill_evaluation import ToolFillEvaluation
from .toolparser_evaluation import ToolParserEvaluation
from .toolsummary_evaluation import ToolSummaryEvaluation
from .func_call_evaluation import FuncCallEvaluation
__all__ = [
    "ToolEvaluation", "ToolFillEvaluation", "ToolParserEvaluation", "ToolSummaryEvaluation", "FuncCallEvaluation"
]
```

#### 4. Execute the Test Script

Modify `~/src/qwen_eval_main.py# datainfos` and `model_infos`

```python
model_infos = [
    {"model_name": "", "template": "chatml", "model_path": "",
     "peft_path": "", "model_class": QwenModel}]
datainfos = [
    {"dataset_path": "~/fcdata_luban_zh_test.jsonl", "dataset_name": "fcdata_luban_zh", "tool_task": "func_call"},
    {"dataset_path": "~/test_datas/fcdata_zh_test_v1.jsonl", "dataset_name": "fcdata_zh", "tool_task": "func_call"},
]
```

To execute, run the following command:

```Bash
python qwen_eval_main.py
```

<br>

### Non-ChatML Integration

To test on your own Huggingface-format model, the overall steps are divided into the following:

1. Write related code in `~/getAssistantAns.py`
2. Execute the test script

#### 1. Writing the `getAssistantAns` Example

```
class GetAssistantAns():
    # æŒ‰ç…§è‡ªå·±æ¨ç†éœ€æ±‚è‡ªå·±ä¿®æ”¹ä»£ç 

    def __init__(self, gpu_num=1):
        model = AutoModelForCausalLM.from_pretrained(model_name)
        device_list = []
        for gpu_idx in range(gpu_num):
            device_list.append(torch.device("cuda:0"))

        # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šçš„GPUè®¾å¤‡
        model.to(device)


    def gen_answer(self, chat_dict, gpu_index):
        # è¿™é‡Œå®é™…æ ¹æ®è‡ªå·±æ¨ç†é€»è¾‘ ç„¶åè½¬ä¸ºæ ‡å‡†æ ¼å¼è¿”å›
        # ä»¥ä¸‹ä»…ä»…æ˜¯æ ·ä¾‹
        import time
        print(os.environ["CUDA_VISIBLE_DEVICES"])
        time.sleep(1)
        rtn_dict1 = {
                "role": "assistant",
                "content": None,
                "function_call":
                {
                    "name": "get_fudan_university_scoreline",
                    "arguments": "{\n  \"year\": \"2020\"\n}"
                }
            }

        rtn_dict2 =  {
                "role": "assistant",
                "content": "2020å¹´å¤æ—¦å¤§å­¦çš„åˆ†æ•°çº¿å¦‚ä¸‹ï¼š\n\n- æ–‡ç§‘ä¸€æ‰¹ï¼š630åˆ†\n- æ–‡ç§‘äºŒæ‰¹ï¼š610åˆ†\n- ç†ç§‘ä¸€æ‰¹ï¼š650åˆ†\n- ç†ç§‘äºŒæ‰¹ï¼š630åˆ†"
            }

        return random.choice([rtn_dict1, rtn_dict2])
```

#### 2. Execute the Test Script

Modify `~/src/opensource_functioncall_evaluation.py # test_ans_file_list`

```python
test_ans_file_list = [
        "fcdata_zh_test.jsonl"
        ]
```

To execute, run the following command:

```Bash
python opensource_functioncall_evaluation.py
```
