---
title: '2024年5月90篇代码大模型论文最全整理'
time: '2024-06-14'
toc: content
---

## 引言

本文整理 2024 年 5 月发布的 90 篇代码大模型相关论文，其中包括 17 篇发表在今年 ICLR 的论文。根据论文内容，我们将这些论文整理为了基座模型、代码微调、测试基准、代码 Agent、低资源语言处理、AI 代码安全与分析、人机交互、软件工程下游任务应用（包括代码生成、代码翻译、代码总结、代码修复、代码检索、SQL 生成、测试生成、漏洞检测、日志分析、需求工程）等主题。全文篇幅较长，建议电脑端阅读。

若您想了解其他时期的代码大模型论文，也欢迎关注我们的代码大模型综述 https://arxiv.org/abs/2311.07989 和 GitHub 开源项目 https://github.com/codefuse-ai/Awesome-Code-LLM。

## 编辑精选

Large Language Models Synergize with Automated Machine Learning
本文提出了一种新颖的机器学习程序合成方法，称为 Text-to-ML。该方法结合了大语言模型和自动机器学习技术，可以根据机器学习任务的文本描述，自动生成和优化完整的机器学习工作流程代码，包括数据准备、建模和后处理等环节。为了应对机器学习程序的长度和多样性，该方法采用了将程序分解为更小、更易管理的部分，并针对机器学习程序设计了测试技术和数值评估方法。实验结果表明，Text-to-ML 在生成机器学习程序方面优于现有方法，且自动机器学习技术可以显著提高生成程序的性能。
发布日期：2024-05-06
链接：https://arxiv.org/abs/2405.03727
机构：University of Tokyo

DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model
DeepSeek-V2 是 DeepSeek 系列最新的混合专家语言模型（MoE）。该模型在保证高效推理和经济训练成本的同时，达到了 236B 的总参数量，其中每个 token 激活 21B 参数，并支持 128K tokens 的上下文长度。此外，论文还引入了创新的 multi-head latent attention（MLA）和 DeepSeekMoE 架构，前者通过将 KV 缓存压缩为潜在向量来保证推理效率，后者则通过稀疏计算实现了经济成本下的强模型训练。与之前的 DeepSeek 67B 模型相比，DeepSeek-V2 在显著提升性能的同时，减少了 42.5%的训练成本，将 KV 缓存减少了 93.3%，并将最大生成吞吐量提高了 5.76 倍。
发布日期：2024-05-07
链接：https://arxiv.org/abs/2405.04434
机构：DeepSeek-AI

MHPP: Exploring the Capabilities and Limitations of Language Models Beyond Basic Code Generation
本文提出了一个新的数据集 Mostly Hard Python Problems (MHPP)，用于更全面、深入地评估大型语言模型在函数级代码生成方面的能力。作者分析了现有的 HumanEval 和 MBPP 两个常用基准测试的局限性，发现它们在质量、难度和粒度方面存在不足。因此，他们创建了由 140 个独特的人工精心设计的问题组成的 MHPP 数据集，重点测试语言模型在理解规范和限制、进行多步推理以及有效应用编码知识方面的能力。通过在 22 个语言模型上进行初步评估，发现许多在 HumanEval 上 表现出色的模型在 MHPP 上难以取得类似的成功，突显出这些模型的各种局限性。
发布日期：2024-05-19
链接：https://arxiv.org/abs/2405.11430
机构：University of Edinburgh, University of Hong Kong

Multiple-Choice Questions are Efficient and Robust LLM Evaluators
本文基于当前最流行的四个大模型测评数据集 GSM8K、MATH、HumanEval、MBPP 构建了三个多项选择题格式的测试基准 GSM-MC、MATH-MC 和 PythonIO。对 GSM8K 和 MATH，作者收集 60 个开源模型在原始数据集上的错误答案构建选择题，并通过大量实验表明大模型在选择题格式的测试基准上得分与在生成式评估的测试基准上得分强相关，且对干扰选项设置和选项顺序具有相当的鲁棒性，而测试开销则减小了 30 倍。对于 HumanEval 和 MBPP 两个代码生成数据集，作者则构造了一个全新的 Python 程序输出预测测试基准，且实验表明大模型还有很大的提升空间。
发布日期：2024-05-20
链接：https://arxiv.org/abs/2405.11966
机构：Shanghai Jiao Tong University

AutoCoder: Enhancing Code Large Language Model with AIEV-Instruct
AutoCoder 是首个在 HumanEval 基准测试中超越 GPT-4 Turbo（2024 年 4 月版）和 GPT-4o 的大语言模型，实现了 90.9% 的 pass@1 得分。此外，AutoCoder 提供了比 GPT-4 Turbo 和 GPT-4o 更加通用的代码解释器，可以安装外部程序包。论文还提出了一种名为 AIEV-Instruct 的训练数据生成方法，结合了 agent 交互和外部代码执行验证，较之前的大规模代码数据集生成方法，减少了对大型专有模型的依赖，并提供了经过执行验证的代码数据集。
发布日期：2024-05-23
链接：https://arxiv.org/abs/2405.14906
机构：University of Connecticut

## 基座模型

Granite Code Models: A Family of Open Foundation Models for Code Intelligence
Granite 是在 116 种编程语言上训练的一系列大模型，大小有 3B、8B、30B 和 34B，其中 34B 模型训练数据量为 3.5T tokens，且是在 20B 模型训练 1.4T tokens 后使用 depth upscaling 扩展为 34B 继续训练。
发布日期：2024-05-07
链接：https://arxiv.org/abs/2405.04324
机构：IBM Research

GECKO: Generative Language Model for English, Code and Korean
GECKO 是在英语、韩语、代码语料上训练的 7B 双语开源模型，在 MMLU 上得分 28.3，在 HumanEval 上得分 17.7。
发布日期：2024-05-24
链接：https://arxiv.org/abs/2405.15640
机构：KIFAI

MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series
MAP-Neo 是在英语、中文、代码语料上训练的双语开源模型，有 2B 与 7B 两个版本，其中 7B 模型训练数据量为 4.5T tokens，在 MMLU 上得分 58.1，GSM8K 得分 53.7，HumanEval 得分 23.8。本文还开源了预训练所使用的数据集 Matrix。
发布日期：2024-05-29
链接：https://arxiv.org/abs/2405.19327
机构：M-A-P, University of Waterloo

## 代码微调

AlchemistCoder: Harmonizing and Eliciting Code Capability by Hindsight Tuning on Multi-source Data
本文提出了 AlchemistCoder，一系列在多源数据上微调的代码大语言模型。作者创新性地揭示了多源代码语料库中固有的风格和质量冲突，并引入了具有事后重新标记的数据特定提示（称为 AlchemistPrompts）来协调不同的数据源和指令-响应对。此外，作者还提出将数据构建过程纳入微调数据中，作为代码理解任务，包括指令演变、数据筛选和代码审查。大量实验表明，AlchemistCoder 在同等规模的模型中处于领先地位，甚至可以与更大的模型相媲美或超越，展示了该方法在改进指令跟随能力和推进代码智能边界方面的有效性。
发布日期：2024-05-29
链接：https://arxiv.org/abs/2405.19265
机构：Tongji University, Shanghai AI Laboratory

From Symbolic Tasks to Code Generation: Diversification Yields Better Task Performers
本文通过一系列合成实验和真实世界的代码生成应用，探究了指令微调中影响语言模型理解和执行未见指令能力的关键因素。研究发现，即使每个任务的样本数量很少，只要提供足够多样化的任务集合，模型就能在训练分布之外表现出泛化性和鲁棒性。此外，在代码生成任务中，使用涵盖代码相关任务之外的更加多样化的指令集可以提升模型的性能。本文的结果表明，指令微调数据集的语义空间多样性是提高模型执行指令和完成任务能力的关键。
发布日期：2024-05-30
链接：https://arxiv.org/abs/2405.19787
机构：University of Illinois Urbana-Champaign

Unveiling the Impact of Coding Data Instruction Fine-Tuning on Large Language Models Reasoning
本文深入探究了在指令微调阶段，代码数据对大语言模型推理能力的影响。通过在不同编程数据比例、模型类别、模型规模和推理领域等方面进行全面分析，论文得出了几个重要结论：1）代码数据微调可以提升不同类别和规模 LLM 的整体推理能力；2）代码数据的效果因领域而异，但在每个领域内表现出一致的趋势；3）代码数据对不同模型家族的具体任务带来的好处大体相当，但指令微调数据集中的最佳代码数据比例因任务而异。这些发现为理解代码数据在 LLM 指令微调中的作用提供了有价值的见解。
发布日期：2024-05-30
链接：https://arxiv.org/abs/2405.20535
机构：University of California, Santa Barbara

## 测试基准

NaturalCodeBench: Examining Coding Performance Mismatch on HumanEval and Natural User Prompts
本文提出了一个新的代码合成基准测试集 NaturalCodeBench (NCB)，旨在更好地反映真实编码任务中的复杂性和多样性。NCB 包含了来自在线编码服务的 402 个高质量的 Python 和 Java 编程问题，涵盖了 6 个不同的领域。此外，论文还介绍了一种半自动化的测试用例构建流程，与手动解决方案相比，效率提高了 4 倍以上。通过在 39 个大语言模型上进行系统实验，发现在 HumanEval 得分接近的模型在 NCB 上的性能差距仍然显著，表明现有模型缺乏对实际代码合成场景的关注或过度优化了 HumanEval。同时，即使是表现最好的 GPT-4 在 NCB 上的表现也远未达到满意的程度。
发布日期：2024-05-07
链接：https://arxiv.org/abs/2405.04520
机构：Zhipu.AI

Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots
本文提出了一个全面的视觉编码基准测试集合 Plot2Code，用于公平和深入地评估多模态大语言模型（MLLM）在将视觉图形转化为可执行代码方面的能力。论文作者精心收集了 132 个高质量的 matplotlib 图表，并为每个图表提供了相应的源代码和由 GPT-4 总结的描述性指令。此外，论文还提出了三个自动评估指标，包括代码通过率、文本匹配率和 GPT-4V 综合评分，以对输出代码和渲染图像进行细粒度评估。通过在 Plot2Code 上评估 14 个 MLLM，论文揭示了大多数现有的 MLLM 在处理文本密集型图表的视觉编码时存在困难，并严重依赖文本指令。这项工作有望为未来 MLLM 的发展提供指导。
发布日期：2024-05-13
链接：https://arxiv.org/abs/2405.07990
机构：The University of Hong Kong

Aligning LLMs through Multi-perspective User Preference Ranking-based Feedback for Programming Question Answering
本文提出了一个名为 ALMupQA 的新框架，旨在解决代码社区问答（CCQA）任务中的用户偏好多样性和新 API 偏好等挑战。ALMupQA 框架首先通过多角度偏好排序对齐（MPRA）综合考虑不同用户的偏好，然后引入了一个基于检索的上下文学习（RIL）模块，通过从问题库中检索相似问题的答案来缓解过时答案的问题。此外，论文还开发了一个名为 StaCCQA 的高质量多答案代码问答数据集。广泛的实验表明，与基础模型相比，ALMupQA 框架在准确性和用户偏好方面取得了显著改进，BLEU 提高了近 11%，BERTScore 和 CodeBERTScore 分别提高了 20%和 17.5%。
发布日期：2024-05-27
链接：https://arxiv.org/abs/2406.00037
机构：University of Science and Technology of China

## 代码 Agent

SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering
本文提出了一个名为 SWE-agent 的系统，该系统为语言模型智能体提供了定制化的智能体-计算机接口（ACI），大幅提升了智能体自主使用计算机解决软件工程任务的能力。通过在 SWE-bench 和 HumanEvalFix 基准测试中取得远超先前非交互式语言模型的最佳性能，论文证实了 ACI 设计对智能体行为和性能的重要影响，为语言模型智能体这一新兴的终端用户群体开发专门的界面和工具奠定了基础。
发布日期：2024-05-06
链接：https://arxiv.org/abs/2405.15793
机构：Princeton University

MapCoder: Multi-Agent Code Generation for Competitive Problem Solving
本文提出了一种新的代码生成方法 MapCoder。它利用多个 LLM 智能体模拟人类开发者的程序合成过程，具体包含四个智能体，分别负责回忆相关示例、规划、代码生成和调试。通过在八个具有挑战性的问题解决和程序合成基准测试中进行全面实验，MapCoder 展现出卓越的代码生成能力，在多个数据集上取得了最先进的结果。此外，该方法在不同编程语言和问题难度下都表现出色。这项工作为复杂代码生成任务提供了一种新的思路。
发布日期：2024-05-18
链接：https://arxiv.org/abs/2405.11403
机构：Bangladesh University of Engineering and Technology

Fight Fire with Fire: How Much Can We Trust ChatGPT on Source Code-Related Tasks?
本文全面评估了 ChatGPT 在代码生成、代码补全和程序修复任务中的自我验证能力。研究发现，ChatGPT 常常错误地预测其生成的不正确代码是正确的，表现出自相矛盾的幻觉行为。提出引导性问题可以增强 ChatGPT 的自我验证能力，但 ChatGPT 生成的测试报告对于错误生成的代码和修复失败的解释大多不准确。这些发现为进一步利用 ChatGPT 进行研究或开发提供了启示。
发布日期：2024-05-21
链接：https://arxiv.org/abs/2405.12641
机构：Zhejiang University

SOAP: Enhancing Efficiency of Generated Code via Self-Optimization
本文提出了一个名为 SOAP 的自优化框架，可以利用执行开销信息来改进大型语言模型生成的代码效率。通过迭代式的自我优化，SOAP 能够显著提升生成代码的执行速度和内存使用效率。实验结果表明，在 EffiBench 测试中，优化后的代码执行时间减少了 87.1%，总内存消耗降低了 90.8%。这项工作为提高大型语言模型在代码生成任务中的实用性做出了重要贡献。
发布日期：2024-05-24
链接：https://arxiv.org/abs/2405.15189
机构：The University of Hong Kong, University of Edinburgh

Code Repair with LLMs gives an Exploration-Exploitation Tradeoff
本文提出了一种新的基于大语言模型的程序合成算法。该算法利用 Thompson 采样来平衡探索和利用，在循环不变式合成、视觉推理谜题和编程竞赛问题等多个领域取得了更好的效果，同时减少了对语言模型的调用次数。这一工作为如何利用大语言模型迭代优化和修复源代码提供了新的思路。
发布日期：2024-05-26
链接：https://arxiv.org/abs/2405.17503
机构：Cornell University, Shanghai Jiao Tong University

ReflectionCoder: Learning from Reflection Sequence for Enhanced One-off Code Generation
本文提出了一种名为 ReflectionCoder 的新方法，通过整合编译器反馈构建反射序列，有效提升了一次性代码生成的性能。此外，论文还提出了反射自蒸馏和动态掩码蒸馏技术，以更好地利用这些反射序列。实验结果表明，使用该方法微调后的模型在 HumanEval(+)、MBPP(+) 和 MultiPl-E 等基准测试中取得了最先进的性能，与 GPT-3.5-Turbo 和 Claude-3-opus 相当，甚至超过了早期的 GPT-4。这一方法不仅可应用于代码领域，还有望惠及其他注重最终结果且需要长推理路径的领域。
发布日期：2024-05-27
链接：https://arxiv.org/abs/2405.17057
机构：Shanghai Jiao Tong University, SenseTime Research

Training LLMs to Better Self-Debug and Explain Code
本文提出了一个训练框架，显著提高了大语言模型在代码生成领域的自我调试能力。作者观察到，对错误代码进行一系列解释，然后进行代码优化，可以帮助 LLM 更好地分析错误代码并进行改进。他们提出了一个自动化流程，通过生成大量解释和优化轨迹，并通过执行验证进行过滤，收集用于代码解释和优化的高质量数据集。在成功和失败的轨迹上进行监督微调（SFT）和强化学习（RL），并设计了一种新颖的奖励机制，同时考虑代码解释和优化质量。实验结果表明，该框架可以显著提高 LLM 的代码生成性能和迭代优化能力，并通过人工评估证实，经过训练的 LLM 可以生成更有用的代码解释，帮助开发人员更好地理解源代码中的错误。
发布日期：2024-05-28
链接：https://arxiv.org/abs/2405.18649
机构：Purdue University, AWS AI Lab

## 低资源语言

MEIC: Re-thinking RTL Debug Automation using LLMs
本文提出了一个名为 MEIC（Make Each Iteration Count）的新框架，用于利用大语言模型调试寄存器传输（RTL）代码。与传统的一次性 LLM 调试方法不同，MEIC 采用迭代过程，可有效识别和修正 RTL 代码中的语法和功能错误，同时能够管理 LLM 操作中固有的不确定性。论文还提供了一个包含 178 个常见 RTL 编程错误的开源数据集。实验结果表明，与有经验的工程师相比，该框架在调试过程中可以实现高达 48 倍的加速，对语法错误的修复率达到 93%，对功能错误的修复率达到 78%。
发布日期：2024-05-10
链接：https://arxiv.org/abs/2405.06840
机构： Southeast University

Tackling Execution-Based Evaluation for NL2Bash
本文提出了一种基于执行结果的评估方法，用于评估自然语言转 Bash 脚本（NL2Bash）任务中大语言模型生成代码的质量。作者设计并实现了一个用于 NL2Bash 任务的执行评估系统，并创建了一组 50 个提示词来评估一些流行的大型语言模型在该任务上的表现。此外，作者还分析了执行评估方法的几个优势和挑战，例如不同模型生成的语法不同但语义等价的 Bash 脚本，以及语法正确但语义错误的 Bash 脚本，并讨论了如何正确地捕获和处理这些情况。
发布日期：2024-05-10
链接：https://arxiv.org/abs/2405.06807
机构：IBM Research

Evaluating AI-generated code for C++, Fortran, Go, Java, Julia, Matlab, Python, R, and Rust
本文评估了 ChatGPT 3.5 和 4 版本在生成多种编程语言的科学计算代码方面的能力。研究发现，尽管 ChatGPT 能够生成可编译运行的代码，但不同编程语言的生成难度存在差异，并行代码对于 AI 来说也更具挑战性。这项研究为理解当前大型语言模型在科学计算领域的应用潜力提供了重要参考。
发布日期：2024-05-21
链接：https://arxiv.org/abs/2405.13101
机构：Louisiana State University

Optimizing Large Language Models for OpenAPI Code Completion
本文评估了 GitHub Copilot 在 OpenAPI 定义补全任务上的性能，并提出了一系列针对 CodeLlama 的优化方法，包括提示工程和微调技术，使其在参数量仅为 Codex 模型的 1/25 的情况下，在自定义的 OpenAPI 补全基准测试中实现了比 GitHub Copilot 高出 55.2% 的正确率，同时还改进了一种广泛使用的代码填充训练技术，解决了模型在提示上下文大小小于训练时使用的上下文大小时性能下降的问题，并公开了数据集、基准测试和模型微调代码。
发布日期：2024-05-24
链接：https://arxiv.org/abs/2405.15729
机构：Kaunas University of Technology

## AI 代码安全

Codexity: Secure AI-assisted Code Generation
本文提出了一个名为 Codexity 的安全代码生成框架，该框架集成了五个大语言模型。Codexity 利用静态分析工具（如 Infer 和 CppCheck）的反馈，来减少大语言模型生成的程序中的安全漏洞。在一个包含 751 个自动生成的易受攻击主题的真实基准测试中，该论文的评估结果表明，Codexity 可以防止 60% 的漏洞暴露给软件开发人员。这项工作有助于提高人工智能程序助手生成代码的安全性，减少引入漏洞的风险。
发布日期：2024-05-07
链接：https://arxiv.org/abs/2405.03927
机构：National University of Singapore

Measuring Impacts of Poisoning on Model Parameters and Embeddings for Large Language Models of Code
本文针对大语言模型在软件开发中的安全问题，特别是隐藏的后门攻击，提出了一种新的检测方法。通过分析模型的参数，包括注意力权重、偏差以及上下文嵌入，论文发现在中毒样本的上下文嵌入中存在明显的模式，而在注意力权重和偏差中没有显著差异。这项工作为通过分析参数和嵌入来进行白盒检测代码大型语言模型中的后门信号做出了贡献，推动了相关领域的研究进展。
发布日期：2024-05-19
链接：https://arxiv.org/abs/2405.11466
机构：University of Houston

## AI 代码分析

A Controlled Experiment on the Energy Efficiency of the Source Code Generated by Code Llama
本文对比了由大语言模型 CodeLlama 生成的代码和人类编写的代码在能耗效率方面的差异。通过在 C++、JavaScript 和 Python 三种语言上进行实验，作者发现 CodeLlama 生成代码的能耗效率很大程度上取决于所选择的编程语言和具体的编码问题，总体而言人类编写的代码能耗效率更高，但 JavaScript 是个例外。此外，即便明确要求 CodeLlama 生成节能的代码，其结果也并不理想，改变温度参数也不会影响生成代码的能耗。论文得出结论，使用 CodeLlama 生成的代码并不能保证能耗效率，因此开发者在将其集成到软件系统前，应当先评估代码的能耗效率。
发布日期：2024-05-06
链接：https://arxiv.org/abs/2405.03616
机构：Vrije Universiteit Amsterdam

ChatGPT Code Detection: Techniques for Uncovering the Source of Code
本文提出了一种新颖的方法，通过结合强大的嵌入特征(黑盒)和监督学习算法，包括深度神经网络、随机森林和极端梯度提升，以高达 98% 的准确率区分人工编写的代码和 ChatGPT 生成的代码。此外，论文还提供了白盒特征和可解释的贝叶斯分类器，以阐明代码来源之间的关键差异，从而提高了方法的可解释性和透明度。这项研究对于理解和减轻人工智能在代码生成中的潜在风险至关重要，特别是在高等教育、软件开发和竞争性编程的背景下。
发布日期：2024-05-24
链接：https://arxiv.org/abs/2405.15512
机构：TH Köln

Evaluation of the Programming Skills of Large Language Models
本文通过比较两个领先的大型语言模型 ChatGPT 和 Gemini 生成的编程代码质量，深入探讨了如何评估聊天机器人在处理复杂任务时的输出质量。论文选择编程代码作为研究对象，不仅因为这两个模型在代码生成方面表现出色，更因为编程代码的复杂性常常会上升到难以验证的程度，凸显了研究的重要性。通过结合真实案例和系统化数据集，这项研究旨在揭示大型语言模型在生成高质量编程代码方面的效能和可靠性，其结果将对软件开发乃至其他领域产生重大影响。
发布日期：2024-05-23
链接：https://arxiv.org/abs/2405.14388
机构：University of Applied Sciences and Arts

Uncovering LLM-Generated Code: A Zero-Shot Synthetic Code Detector via Code Rewriting
本文提出了一种新颖的零样本合成代码检测方法，利用代码与其重写变体之间的相似性来判断代码是否为大型语言模型生成的合成代码。作者基于合成代码与其重写版本之间差异较小的直觉，使用自监督对比学习训练了一个代码相似度模型。在两个合成代码检测基准测试中，该方法相较现有的面向通用文本的合成内容检测器有显著提升，在 APPS 基准测试中提高了 20.5%，在 MBPP 基准测试中提高了 29.1%。本文的贡献在于针对编程语言的独特语法结构和大量"低熵"词法单元，提出了一种专门用于检测合成代码的有效方法。
发布日期：2024-05-25
链接：https://arxiv.org/abs/2405.16133
机构：Zhejiang University

## 人机交互

Sketch Then Generate: Providing Incremental User Feedback and Guiding LLM Code Generation through Language-Oriented Code Sketches
本文提出了一种名为"面向语言的代码草图"的交互式方法，旨在改善人类与大语言模型在代码生成或编辑过程中的交互。该方法利用提示语句中固有的语言结构，并应用经典的自然语言处理技术，将提示语句转化为代码草图，为用户提供即时、增量的反馈。生成的代码草图不仅可以预览目标代码结构，还能引导语言模型生成期望的代码。这种方法增强了人类与语言模型之间的交互，使得用户能够更有效地编写出高质量的提示语句，从而提高代码生成的效率和质量。
发布日期：2024-05-07
链接：https://arxiv.org/abs/2405.03998
机构：University of Minnesota

Using AI Assistants in Software Development: A Qualitative Study on Security Practices and Concerns
本文研究了软件专业人员如何在安全软件开发中使用 AI 助手，以及由此产生的安全影响和考量。通过对软件工程师、团队负责人和安全测试人员的访谈，以及对 Reddit 上相关帖子的分析，论文发现尽管存在许多安全和质量问题，参与者仍广泛使用 AI 助手进行安全关键任务。他们对 AI 助手的不信任导致他们以类似于检查人工代码的方式检查 AI 建议，但预计未来 AI 助手会有所改进，因此会在安全任务中得到更多应用。论文提出了针对软件专业人员、AI 创建者和学术研究者的建议，以提高 AI 在软件开发中的安全性和适用性。
发布日期：2024-05-10
链接：https://arxiv.org/abs/2405.06371
机构：CISPA Helmholtz Center for Information Security, Ruhr University Bochum

Full Line Code Completion: Bringing AI to Desktop
本文 JetBrains 公司为 IntelliJ 平台开发的多词代码补全功能"Full Line Code Completion"。该功能可以在用户的本地设备上独立运行，生成语法正确的代码建议，满足了时间和内存消耗的限制，并符合代码补全引擎的设计原则。论文分享了在开发过程中使用的一些有用技术，以及离线和在线评估流程，帮助做出更好的决策。在线评估显示，使用该工具可以将 IDE 中由代码补全产生的代码量提高 1.5 倍。这一解决方案最初在研究人员的帮助下启动，并于 2023 年底集成到 PyCharm Pro 和 DataSpell 两个 JetBrains IDE 中，成功地将复杂的研究成果应用到实际产品中，为学术界和工业界之间搭建了桥梁。
发布日期：2024-05-14
链接：https://arxiv.org/abs/2405.08704
机构：JetBrains

Developers' Perceptions on the Impact of ChatGPT in Software Development: A Survey
本文通过对 207 名软件开发人员进行调查，深入探讨了 ChatGPT 等大型语言模型对软件开发实践和开发者观念的影响。研究揭示了 AI 工具如何影响软件质量、生产效率和工作满意度，同时也分析了开发者对 ChatGPT 未来发展的预期、对潜在失业风险的担忧，以及对监管措施的看法。这项研究有助于理解 AI 驱动工具在软件开发过程中的作用，为应对 AI 与软件工程交叉领域的挑战提供了重要见解。
发布日期：2024-05-20
链接：https://arxiv.org/abs/2405.12195
机构：Federal University of Bahia

A Transformer-Based Approach for Smart Invocation of Automatic Code Completion
本文解决了代码自动补全工具中的一个重要问题:何时向开发者提供代码补全建议。研究者开发了一个机器学习模型，可以根据代码上下文和可用的遥测数据准确预测何时调用代码补全工具。他们收集了一个包含 20 万次开发者与跨 IDE 代码补全插件交互的数据集，并训练了多个调用过滤模型。结果表明，他们的小规模 transformer 模型在保持较低延迟的同时显著优于基线。此外，研究者还探索了将额外的遥测数据直接集成到预训练的 transformer 中的搜索空间，并获得了有希望的结果。为了进一步证明他们的方法的实际潜力，研究者在线上环境中与 34 名开发者一起部署了该模型，并基于 7.4 万次实际调用提供了真实世界的见解。
发布日期：2024-05-23
链接：https://arxiv.org/abs/2405.14753
机构：Delft University of Technology

A Study on Developer Behaviors for Validating and Repairing LLM-Generated Code Using Eye Tracking and IDE Actions
本文通过对 28 名参与者的实验研究，探讨了开发者如何验证和修复由大型语言模型（如 GitHub Copilot）生成的代码，并分析了代码来源意识对这些过程的影响。研究发现，在没有明确信息的情况下，开发者通常无法识别代码的语言模型来源。尽管开发者对语言模型生成的代码采用类似的验证和修复策略，但表现出频繁在代码和注释之间切换、注意力集中不同以及倾向于删除和重写代码等行为。意识到代码的来源可以提高性能，增加搜索努力，更频繁地使用 Copilot，但同时也会增加认知负荷。这些发现加深了我们对开发者如何与语言模型生成的代码交互的理解，并为设计促进人与语言模型在软件开发中有效协作的工具提供了启示。
发布日期：2024-05-25
链接：https://arxiv.org/abs/2405.16081
机构：University of Notre Dame

Analyzing Chat Protocols of Novice Programmers Solving Introductory Programming Tasks with ChatGPT
本文研究了大学生在学习编程入门课程时如何使用 ChatGPT 等大型语言模型工具。研究者收集并分析了 213 名学生提交的 2335 条与 ChatGPT 的对话记录，从对话内容、频率、进展和其他使用模式等方面进行了研究。结果显示，学生与 ChatGPT 的互动方式多种多样，既有可能起到支持作用的，也有令人担忧的。通过了解学生与 ChatGPT 的互动方式，可以为未来高等教育中的编程入门课程提供教学实践和指导方面的参考。
发布日期：2024-05-29
链接：https://arxiv.org/abs/2405.19132
机构：Nuremberg Tech

Benchmarking the Communication Competence of Code Generation for LLMs and LLM Agent
探索了大型语言模型在代码生成任务中的交流能力。作者认为，优秀的软件工程师在面对不明确的需求和编码解决方案时，往往会提出澄清性问题，而大型语言模型在执行代码生成任务时也应该具备这种能力。为了评估语言模型在这方面的表现，作者创建了一个新的基准测试集 HumanEvalComm，并定义了交流率和良好问题率等新的评估指标。此外，作者还提出了一种新的语言模型代理方法 Okanagan，用于识别代码和描述中的模糊部分，并提出问题以进一步优化生成的代码。通过比较不同的代码语言模型和 Okanagan，作者讨论了评估结果，为大型语言模型在代码生成任务中的交流能力研究提供了新的见解。
发布日期：2024-05-31
链接：https://arxiv.org/abs/2406.00215
机构：University of British Columbia

# 软工下游任务

## 代码生成

CodeGRAG: Extracting Composed Syntax Graphs for Retrieval Augmented Cross-Lingual Code Generation
本文提出了一种名为 CodeGRAG 的方法，通过提取和总结代码块的控制流和数据流，弥补了编程语言与自然语言之间的差距。这种外部结构知识可以帮助大型语言模型更好地理解代码语法，并在不同编程语言之间架起桥梁。CodeGRAG 显著提高了大型语言模型在单轮代码生成任务中的性能，甚至可以在跨语言代码生成（如用 C++ 生成 Python 代码）中获得性能提升。
发布日期：2024-05-03
链接：https://arxiv.org/abs/2405.02355
机构：Shanghai Jiao Tong University

Prompt-based Code Completion via Multi-Retrieval Augmented Generation
本文提出了一个名为 ProCC 的代码补全框架，通过利用提示工程和上下文多臂老虎机算法，灵活地结合多个代码视角来改进代码补全效果。ProCC 首先采用基于提示的多检索器系统，利用大语言模型的知识从多个检索视角理解代码语义。然后，它采用自适应检索选择算法将代码相似度纳入决策过程，以确定最合适的检索视角供语言模型完成代码。实验结果表明，ProCC 在开源和私有领域的基准测试中，在 Exact Match 指标上分别比现有最佳技术提高了 8.6% 和 10.1%，同时还可以通过即插即用的方式增强微调模型的性能。
发布日期：2024-05-13
链接：https://arxiv.org/abs/2405.07530
机构：Southern University of Science and Technology

Model Cascading for Code: Reducing Inference Costs with Model Cascading for LLM Based Code Generation
本文提出了一种基于模型级联的方法，用于在代码补全任务中优化大语言模型的计算成本和准确性之间的平衡。作者提出让每个模型生成并执行一组测试用例，并使用测试结果作为级联阈值。与单一模型生成输出相比，该策略可以降低计算成本，同时提高准确性。此外，作者还引入了一种启发式方法来确定每个模型应生成的解决方案、测试用例和测试行数的最佳组合。这是首次在代码生成任务中使用模型级联来优化大语言模型的成本-准确性权衡。
发布日期：2024-05-24
链接：https://arxiv.org/abs/2405.15842
机构：New York University

Divide-and-Conquer Meets Consensus: Unleashing the Power of Functions in Code Generation
本文提出了一种名为 FunCoder 的代码生成框架，它采用分治策略和函数共识的方法来解决大型语言模型在生成满足复杂要求的程序时面临的挑战。FunCoder 通过递归地将子函数作为更小的目标进行分支，并通过识别程序行为的相似性形成共识来指定函数，从而减轻了错误传播的影响。实验结果表明，FunCoder 在多个数据集上优于现有的最先进方法，并且在较小的模型上也表现出优越性。此外，论文的分析揭示了所提出的动态函数分解能够处理复杂的需求，而函数共识在正确性评估方面优于自我测试。
发布日期：2024-05-30
链接：https://arxiv.org/abs/2405.20092
机构：Harbin Institute of Technology

## 仓库级代码生成

Contextual API Completion for Unseen Repositories Using LLMs
本文提出了一种新的技术，通过利用代码仓库中的全局和局部上下文信息，来缓解大型语言模型在 API 补全任务中由于缺乏真实世界、特定领域信息而导致的输出不一致问题。作者开发了一个名为 LANCE 的工具，针对 API 令牌补全和对话式 API 补全两种任务进行了优化。在作者提出的跨两种编程语言的基准测试 APIEval 中，LANCE 的平均准确率分别达到了 82.6% 和 76.9%，显著超过了 Copilot。这项研究表明，作者提出的轻量级上下文分析方法可以应用于多语言环境，无需语言特定的训练或微调，从而以最小的样本和努力达到高效的实现。
发布日期：2024-05-07
链接：https://arxiv.org/abs/2405.04600
机构：University of British Columbia

Dataflow-Guided Retrieval Augmentation for Repository-Level Code Completion
本文提出了一种名为 DraCo 的数据流引导的检索增强方法，用于仓库级别的代码补全任务。DraCo 通过扩展的数据流分析，将私有仓库解析为代码实体并建立它们之间的关系，形成特定于仓库的上下文图。在触发代码补全时，DraCo 能够从上下文图中精确地检索相关的背景知识，并生成格式良好的提示来查询代码语言模型。此外，论文还构建了一个更加多样化的 Python 数据集 ReccEval。实验结果表明，与现有最先进的方法相比，DraCo 在代码精确匹配率和标识符 F1 分数方面平均提高了 3.43% 和 3.27%，展现出了优越的准确性和适用效率。
发布日期：2024-05-30
链接：https://arxiv.org/abs/2405.19782
机构：Nanjing University

## 代码翻译

Towards Translating Real-World Code with LLMs: A Study of Translating to Rust
本文首次对大语言模型在将真实世界的代码翻译为 Rust 语言方面的能力进行了大规模的研究。作者评估了五个最先进的大型语言模型在代码翻译任务上的表现，并开发了一个名为 FLOURINE 的端到端代码翻译工具，该工具使用差分模糊测试来检查翻译后的 Rust 代码与原始源程序是否具有 I/O 等价性，从而消除了对预先存在的测试用例的需求。研究结果表明，表现最好的语言模型可以成功翻译 47% 的基准测试。
发布日期：2024-05-19
链接：https://arxiv.org/abs/2405.11514
机构：MPI-SWS, University of Bristol

## 代码总结

DocuMint: Docstring Generation for Python using Small Language Models
本文研究了小型语言模型在生成高质量文档字符串方面的有效性。研究人员通过数学公式和人工评估的方式，从准确性、简洁性和清晰度等方面对模型的性能进行了定量和定性的评估。此外，论文还引入了一个名为 DocuMint 的大规模微调数据集，其中包含 100,000 个样本。通过在 DocuMint 数据集上对 CodeGemma 2B 模型进行微调，该模型在所有指标上的性能都有显著提高，其中简洁性提高了 22.5%。这项研究为使用小型语言模型生成高质量的文档字符串提供了重要的实证支持。
发布日期：2024-05-16
链接：https://arxiv.org/abs/2405.10243
机构：University of Tennessee

Natural Is The Best: Model-Agnostic Code Simplification for Pre-trained Large Language Models
本文提出了一种名为 SlimCode 的模型无关的代码简化方法，用于简化输入给大语言模型的代码。与现有的基于 LLM 注意力分数过滤输入代码令牌的方法不同，SlimCode 依赖于输入代码令牌的性质，因此不受模型架构和预训练数据集的影响。通过在 CodeBERT、CodeT5 和 GPT-4 等 LLM 上进行实证研究，论文发现了代码删除率与训练时间节省率之间的线性关系，不同类别令牌对代码简化的影响差异显著，且这种影响与任务相关但与模型无关。此外，这些发现适用于范式提示工程和交互式上下文学习。实验结果表明，与现有技术相比，SlimCode 在代码搜索和摘要方面分别提高了 9.46% 和 5.15% 的性能，速度提高了 133 倍，并且在生成与原始代码相当的结果的同时，每次 GPT-4 API 查询的成本最多可降低 24%。
发布日期：2024-05-18
链接：https://arxiv.org/abs/2405.11196
机构：Central University of Finance and Economics

Large Language Models for Code Summarization
本文研究了最新的大语言模型在软件工程领域的应用，特别关注了这些模型在代码解释和总结方面的表现，同时也探讨了它们根据自然语言描述生成代码的能力。
发布日期：2024-05-29
链接：https://arxiv.org/abs/2405.19032
机构：Eötvös Loránd University

## 代码修复

A Systematic Literature Review on Large Language Models for Automated Program Repair
本文对 2020 年至 2024 年期间大型语言模型在自动程序修复领域的应用进行了系统性的文献综述。作者分析了 127 篇相关论文，从大语言模型、自动程序修复以及二者的结合角度出发，总结了目前大型语言模型在自动程序修复中的应用现状、面临的挑战以及未来的研究机会。论文不仅分类介绍了现有的主流大型语言模型及其在自动程序修复中的三种利用策略，还详细阐述了一些能够从大语言模型中获益的特定修复场景，如语义缺陷和安全漏洞等。此外，作者还讨论了将大语言模型与自动程序修复相结合时的几个关键方面，如输入形式和科学开源等，并指出了一系列有待探索的挑战以及未来研究的潜在方向。
发布日期：2024-05-02
链接：https://arxiv.org/abs/2405.01466
机构：Nanjing University

NAVRepair: Node-type Aware C/C++ Code Vulnerability Repair
本文提出了一种名为 NAVRepair 的新框架，用于修复 C/C++ 代码中的漏洞。该框架结合了从抽象语法树（AST）中提取的节点类型信息和错误类型，以更好地解决 C/C++ 漏洞修复的挑战。NAVRepair 采用类型分析来定位最小编辑节点（MEN），并根据不同的错误类型定制上下文信息收集。在离线阶段，它解析代码补丁以定位 MEN，并为每种 MEN 类型设计规则以提取相关的上下文信息。在在线修复阶段，它分析可疑代码，将其与来自通用弱点枚举（CWE）的漏洞类型模板相结合，并生成针对性的修复提示。该框架独立于任何特定的大语言模型，可以快速适应新的漏洞类型。大量实验验证了 NAVRepair 在辅助 LLM 准确检测和修复 C/C++ 漏洞方面取得了优异的结果，与现有的基于 LLM 的 C/C++ 漏洞修复方法相比，准确率提高了 26%。
发布日期：2024-05-08
链接：https://arxiv.org/abs/2405.04994
机构：Harbin Institute of Technology

Automated Program Repair: Emerging trends pose and expose problems for benchmarks
本文指出目前机器学习技术在自动程序修复领域的应用与早期工作存在重要差异，尤其是大语言模型的训练数据集可能包含评估时使用的问题，这可能导致评估结果无法泛化。因此，论文强调在使用机器学习技术进行自动程序修复时，评估和比较必须谨慎，以确保结果的有效性和泛化性。同时，论文也指出目前最流行的自动程序修复评估基准并非为机器学习技术设计，这进一步凸显了该问题的重要性。
发布日期：2024-05-08
链接：https://arxiv.org/abs/2405.05455
机构：Arizona State University

Automated Repair of AI Code with Large Language Models and Formal Verification
本文提出了一种利用大语言模型自动检测和修复神经网络代码中内存安全漏洞的方法。研究者们首先通过程序变异的方式扩充了现有的神经网络代码数据集 NeuroCodeBench，使其规模达到约 81k 个程序。然后，他们使用先进的软件验证器 ESBMC 对变异后的神经网络实现进行内存安全验证。一旦 ESBMC 发现漏洞，就调用大语言模型来修复源代码。在代码修复任务中，研究者比较了各种最先进的提示工程技术和一种重复调用大语言模型的迭代方法的性能。这项工作为提高下一代 AI 系统的安全性提供了新的思路和方法。
发布日期：2024-05-14
链接：https://arxiv.org/abs/2405.08848
机构：The University of Manchester

A Case Study of LLM for Automated Vulnerability Repair: Assessing Impact of Reasoning and Patch Validation Feedback
本文提出了一种名为 VRpilot 的基于大型语言模型的漏洞修复技术。VRpilot 在生成补丁候选之前使用了思维链提示来推理漏洞，并根据外部工具（如编译器、代码检查器、测试套件等）对先前生成的补丁的输出迭代地优化提示。通过与现有技术的比较，作者发现 VRpilot 在 C 语言和 Java 语言上分别平均生成了 14% 和 7.6% 更多正确的补丁。消融研究表明，推理和补丁验证反馈对于提高漏洞修复的性能至关重要。这项研究为推进大型语言模型在漏洞修复领域的应用提供了宝贵的经验和潜在方向。
发布日期：2024-05-24
链接：https://arxiv.org/abs/2405.15690
机构：North Carolina State University

## 代码检索

Refining Joint Text and Source Code Embeddings for Retrieval Task with Parameter-Efficient Fine-Tuning
本文提出了一种新的微调框架，利用参数高效微调技术和对比学习目标，在只调整少量模型参数的情况下，显著提升了代码-文本检索任务的性能。这一方法降低了计算资源需求，加快了微调速度，并通过在两个数据集上的广泛实验证明了其有效性。论文还就参数高效微调技术进行了全面的基准测试，弥补了现有文献中的不足。
发布日期：2024-05-07
链接：https://arxiv.org/abs/2405.04126
机构：Innopolis University

Typhon: Automatic Recommendation of Relevant Code Cells in Jupyter Notebooks
本文提出了一种名为 Typhon 的方法，旨在为 Jupyter notebook 自动推荐相关的代码单元。该方法通过对开发者的 markdown 描述单元进行标记化，并使用 BM25 排序函数或 CodeBERT 等文本相似度技术从数据库中查找最相似的代码单元。然后，该算法计算标记化查询与 markdown 单元之间的相似度距离，以向开发者返回最相关的代码单元。通过在 Kaggle 竞赛的 Jupyter notebook 上评估 Typhon 工具，作者发现该方法可以以中等精度推荐代码单元。这篇论文的方法和结果可以进一步改进 Jupyter notebook 中的代码单元推荐。
发布日期：2024-05-15
链接：https://arxiv.org/abs/2405.09075
机构：Mahidol University

## SQL 生成

CoE-SQL: In-Context Learning for Multi-Turn Text-to-SQL with Chain-of-Editions
本文提出了一种名为 CoE-SQL 的方法，旨在提高大语言模型在多轮文本到 SQL 生成任务中的推理能力。该方法利用了对话上下文的依赖性，通过引入编辑链，提示语言模型基于先前生成的 SQL 语句生成当前语句，只需进行少量修改操作。通过广泛的消融研究，确定了该方法的最佳配置。CoE-SQL 在 SParC 和 CoSQL 两个基准测试中稳定地超越了不同的上下文学习基线方法，使用大型语言模型达到了最先进的性能，并且与目前最优的微调模型具有竞争力。
发布日期：2024-05-04
链接：https://arxiv.org/abs/2405.02712
机构：Shanghai Jiao Tong University

Open-SQL Framework: Enhancing Text-to-SQL on Open-source Large Language Models
本文针对开源大语言模型在文本到 SQL 任务中遇到的问题，提出了一套系统化的方法。论文的主要贡献包括:全面评估了开源大语言模型在文本到 SQL 任务中的表现，提出了 openprompt 策略以更好地表示问题，并提出了新的监督微调策略。论文还探讨了思维链在分步推理中的好处，提出了 openexample 方法来增强小样本学习的效果。此外，论文还引入了一些节省令牌的技术，如可变长度的开放数据库模式、目标列截断和样本列截断，以应对大规模数据库带来的挑战。该方法显著提高了 Llama2-7B 和 CodeLlama-7B 在 BIRD-Dev 数据集上的性能，其中 CodeLlama-7B 的表现甚至超过了 GPT-4。
发布日期：2024-05-04
链接：https://arxiv.org/abs/2405.06674
机构：Shenzhen University

MCS-SQL: Leveraging Multiple Prompts and Multiple-Choice Selection For Text-to-SQL Generation
本文提出了一种新颖的基于大语言模型解决文本到 SQL 任务的方法。该方法利用多个提示来探索更广泛的答案搜索空间，并有效地聚合它们。通过使用多个提示对数据库模式进行稳健的细化，生成各种候选 SQL 查询，并基于置信度分数对候选查询进行过滤，最终通过呈现给 LLM 的多项选择获得最佳查询。在 BIRD 和 Spider 基准测试中，该方法分别实现了 65.5% 和 89.6% 的执行精度，显著优于以前的基于 ICL 的方法，并在 BIRD 上建立了新的 SOTA 性能，无论是在生成查询的准确性还是效率方面。
发布日期：2024-05-13
链接：https://arxiv.org/abs/2405.07467
机构：Dunamu

PromptMind Team at EHRSQL-2024: Improving Reliability of SQL Generation using Ensemble LLMs
本文针对电子健康记录中的文本到 SQL 转换任务（EHRSQL-2024），提出了两种利用大型语言模型进行提示和微调的方法。作者着重缩小了语言模型的现实世界知识与任务所需的领域特定知识之间的差距。实验结果表明，每种方法单独使用都能取得较高的执行准确率，而集成方法则能进一步提高生成的可靠性并减少错误。该论文中概述的方法旨在适用于强调准确性和可靠性的特定领域文本到 SQL 查询转换问题。
发布日期：2024-05-14
链接：https://arxiv.org/abs/2405.08839
机构：无

LG AI Research & KAIST at EHRSQL 2024: Self-Training Large Language Models with Pseudo-Labeled Unanswerable Questions for a Reliable Text-to-SQL System on EHRs
本文提出了一种自训练策略，通过使用伪标记的不可回答问题来提高电子健康记录领域文本到 SQL 模型的可靠性。该方法包括两阶段的训练过程以及基于令牌熵和查询执行的过滤方法。作者在 EHRSQL 2024 共享任务中的出色表现证明了该方法的有效性，展示了通过更可靠的文本到 SQL 系统来改善医疗决策的潜力。
发布日期：2024-05-18
链接：https://arxiv.org/abs/2405.11162
机构：LG AI Research, KAIST

Before Generation, Align it! A Novel and Effective Strategy for Mitigating Hallucinations in Text-to-SQL Generation
本文针对大语言模型在文本到 SQL 转换任务中存在的幻觉问题，提出了一种新的策略——任务对齐（Task Alignment）。该策略鼓励大语言模型利用相似任务的经验，而不是从头开始执行任务，从而减轻模型泛化的负担，有效缓解幻觉问题。基于这一策略，作者提出了一个新的文本到 SQL 转换框架 TA-SQL。实验结果和全面分析证明了该框架的有效性和鲁棒性，在多个主流复杂数据集上显著提高了包括 GPT-4 在内的多个基线模型的性能。
发布日期：2024-05-24
链接：https://arxiv.org/abs/2405.15307
机构：The University of Hong Kong

CHESS: Contextual Harnessing for Efficient SQL Synthesis
本文提出了一种新的流水线方法，用于将自然语言问题转化为 SQL 查询。该方法通过引入分层检索、自适应模式修剪等技术，有效地检索相关数据和上下文，选择高效的模式，并生成正确且高效的 SQL 查询。作者在具有挑战性的跨领域 BIRD 数据集上进行了一系列消融实验，证明了所提出方法的有效性，并取得了最先进的性能表现。这项工作为利用大型语言模型解决实际复杂数据库中的文本到 SQL 转换问题提供了新的思路和方案。
发布日期：2024-05-27
链接：https://arxiv.org/abs/2405.16755
机构：Stanford University

## 测试生成

TOGLL: Correct and Strong Test Oracle Generation with LLMs
本文提出了一种基于大语言模型自动生成测试预言（test oracle）的新方法 TOGLL。通过在 SF110 数据集上微调七个代码大模型并使用六种不同的提示，研究人员发现 TOGLL 能够生成正确、多样且强大的测试预言，能够有效识别大量独特的 bug。在 25 个大型 Java 项目上的研究表明，与 EvoSuite 和现有最先进的神经方法 TOGA 相比，TOGLL 可以产生 3.8 倍以上正确的断言预言和 4.9 倍以上的异常预言，并且能够检测到 1023 个 EvoSuite 无法检测到的独特 bug，是 TOGA 的十倍以上。这项研究揭示了 LLM 在测试预言生成方面的巨大潜力。
发布日期：2024-05-06
链接：https://arxiv.org/abs/2405.03786
机构：University of Virginia

Leveraging Large Language Models for Automated Web-Form-Test Generation: An Empirical Study
本文对比研究了 11 种大语言模型在 146 个网页表单测试用例生成方面的有效性。研究发现，不同的大语言模型在生成测试用例的效果上存在差异，其中 GPT-4、GLM-4 和 Baichuan2 表现较好。相比 GPT-4，其他模型在生成合适的测试用例方面存在困难，导致成功提交率降低。此外，研究还发现，当设计的提示信息包含完整清晰的网页表单上下文信息时，所有模型都能生成更有效的测试用例。最后，论文为利用大语言模型指导自动化网页表单测试提供了一些见解。
发布日期：2024-05-16
链接：https://arxiv.org/abs/2405.09965
机构：Macau University of Science and Technology

Test Oracle Automation in the era of LLMs
本文探讨了利用大语言模型自动生成测试预言（test oracle）的潜力和挑战。作者指出，LLM 在自动测试生成和程序修复等软件测试任务中已经表现出色，因此有望用于自动化生成不同类型的测试预言。同时，论文也分析了在使用 LLM 进行测试预言自动化时，软件工程研究人员需要考虑的主要威胁，包括预言缺陷和数据泄露等问题。总的来说，这项研究为探索 LLM 在测试谕示自动化中的应用提供了新的视角和讨论方向。
发布日期：2024-05-21
链接：https://arxiv.org/abs/2405.12766
机构：IMDEA Software Institute

## 漏洞检测

DLAP: A Deep Learning Augmented Large Language Model Prompting Framework for Software Vulnerability Detection
本文提出了一个名为 DLAP 的新框架，结合了深度学习模型和大语言模型的优势，用于软件漏洞检测。与传统的深度学习模型相比，DLAP 不仅在漏洞检测性能上表现出色，而且可以为开发人员提供解释，帮助他们理解检测结果。实验评估结果证实，DLAP 在多个指标上优于现有的先进提示框架和微调方法。这项工作为改进软件漏洞检测提供了一种新的思路和方法。
发布日期：2024-05-02
链接：https://arxiv.org/abs/2405.01202
机构：Nanjing University

Bridging the Gap: A Study of AI-based Vulnerability Management between Industry and Academia
本文探讨了人工智能在自动化软件漏洞管理领域的研究进展与工业界实际应用之间存在的差距。作者通过讨论和实践经验，发现阻碍工业界采用学术界模型的三个主要障碍：可扩展性和优先级的复杂需求、有限的定制灵活性，以及不明确的财务影响。同时，缺乏广泛的真实世界安全数据和专业知识也显著影响了研究工作。论文提出了一系列未来方向，以帮助更好地理解行业期望，提高基于人工智能的安全漏洞研究的实用性，并推动工业界和学术界之间的协同关系。
发布日期：2024-05-03
链接：https://arxiv.org/abs/2405.02435
机构：Meta

Bridge and Hint: Extending Pre-trained Language Models for Long-Range Code
本文提出了一个名为 EXPO 的框架，用于扩展预训练语言模型以处理长程代码。EXPO 引入了两种创新的记忆机制：桥接记忆和提示记忆。桥接记忆使用标记机制连接长程代码中不连续的片段，帮助模型维持上下文连贯性。提示记忆通过集成 kNN 注意力层，自适应地选择全局上下文中的关键代码元素，如包导入等。这种双重记忆方法弥合了理解局部代码片段和维持全局代码连贯性之间的差距，从而增强了模型对长代码序列的总体理解。实验结果表明，EXPO 显著提高了预训练语言模型在 API 推荐和漏洞检测等代码智能任务上的性能。
发布日期：2024-05-18
链接：https://arxiv.org/abs/2405.11233
机构：Harbin Institute of Technology

Harnessing Large Language Models for Software Vulnerability Detection: A Comprehensive Benchmarking Study
本文探索了使用大语言模型来辅助检测源代码中的漏洞。作者测试了多个最先进的 LLM，并确定了最佳的提示策略，以充分利用 LLM 的能力。通过将 LLM 与传统的静态分析工具进行比较，作者发现 LLM 能够发现比传统工具更多的问题，在召回率和 F1 分数方面表现更好。这项研究的结果对负责确保代码无漏洞的软件开发人员和安全分析师具有重要意义。
发布日期：2024-05-24
链接：https://arxiv.org/abs/2405.15614
机构：Tallinn University of Technology

LLM-Assisted Static Analysis for Detecting Security Vulnerabilities
本文提出了一种创新的方法 IRIS，将大语言模型与静态分析相结合，以检测大型 Java 项目中的安全漏洞。作者还精心构建了一个新的数据集 CWE-Bench-Java，包含 120 个在实际 Java 项目中手工验证的安全漏洞。实验结果表明，IRIS 使用 GPT-4 可以检测出其中的 69 个漏洞，而最先进的静态分析工具只能检测到 27 个。此外，IRIS 还大大减少了误报的数量（最好的情况下可减少 80% 以上）。这项工作展示了大语言模型与传统程序分析技术相结合的巨大潜力。
发布日期：2024-05-27
链接：https://arxiv.org/abs/2405.17238
机构：University of Pennsylvania

## 日志分析

On the Influence of Data Resampling for Deep Learning-Based Log Anomaly Detection: Insights and Recommendations
本文研究了数据重采样方法对深度学习日志异常检测方法的影响。论文作者在三个数据集上评估了现有方法的性能，探讨了不同的正常数据与异常数据的重采样比例对十种数据重采样方法的影响。此外，作者还评估了使用最优重采样比例时，数据重采样方法的有效性。研究结果表明，过采样方法通常优于欠采样和混合方法，在原始数据上进行重采样优于在特征空间中进行重采样。论文建议通过过采样为少数类别生成更多数据，同时通过欠采样从多数类别中删除更少的数据。总的来说，这项研究为理解数据重采样方法与深度学习日志异常检测之间的复杂关系提供了宝贵的见解。
发布日期：2024-05-06
链接：https://arxiv.org/abs/2405.03489
机构：City University of Hong Kong

## 需求工程

MARE: Multi-Agents Collaboration Framework for Requirements Engineering
本文提出了一个名为 MARE 的创新框架，利用大语言模型在整个需求工程过程中进行协作。MARE 将需求工程过程划分为四个任务：引出、建模、验证和规格说明，每个任务由一个或两个特定的智能体执行，每个智能体可以执行多个动作。MARE 设计了一个工作空间，方便智能体上传生成的中间需求工件并获取所需信息。通过在五个公共案例、一个数据集和本工作创建的四个新案例上进行实验，并使用三个广泛使用的指标对生成的需求模型进行比较，结果表明 MARE 可以生成更正确的需求模型，并比现有最先进的方法提高 15.4%。对于生成的需求规格说明，本文从三个方面进行了人工评估，并提供了关于质量的见解。
发布日期：2024-05-06
链接：https://arxiv.org/abs/2405.03256
机构：Peking University

# ICLR 2024 专辑

5 月 7 日至 5 月 11 日，一年一度的人工智能盛会 ICLR 在奥地利维也纳召开。本期综述我们也收录了 17 篇来自今年 ICLR 的代码大模型相关论文。

Lemur: Harmonizing Natural Language and Code for Language Agents
本论文推出了 Lemur 和 Lemur-Chat，兼具自然语言和编程能力的开源模型。通过结合代码密集的预训练和指令微调，这些模型在文本和编码基准上的表现达到了开源模型的最先进水平，展示出在多种代理任务中的优越性，提供了开发高级开源代理的新思路。
发布日期：2023-10-10
链接：https://arxiv.org/abs/2310.06830
机构：University of Hong Kong, Salesforce Research

Code Representation Learning At Scale
本文提出代码表征模型 CodeSage，通过双阶段预训练方案，利用大量代码数据来提升代码表示学习。具体方法包括混合 MLM 和编码编程语言结构的训练，以及通过无监督方式构建困难负样本和困难正样本的对比学习。结果显示，该方法在众多下游任务中显著超越现有模型。同时，论文还通过详细消融实验分析了代码级别降噪方案、困难样本的重要性、双模态对比学习对跨语言语义搜索的提升，以及预训练方案对模型尺寸和任务表现的影响。
发布日期：2024-02-02
链接：https://arxiv.org/abs/2402.01935
机构：AWS AI Labs

WizardCoder: Empowering Code Large Language Models with Evol-Instruct
本文提出了 WizardCoder，通过将 Evol-Instruct 方法应用到代码领域，对代码大模型进行复杂指令微调。实验结果表明，WizardCoder 在四个主要代码生成基准测试（HumanEval、HumanEval+、MBPP、DS-1000）上的表现显著优于所有其他开源的代码大模型，并且在 HumanEval 和 HumanEval+ 测试中甚至超越了最大的闭源模型（Anthropic 的 Claude 和 Google 的 Bard）。
发布日期：2023-06-14
链接：https://arxiv.org/abs/2306.08568
机构：Microsoft

OctoPack: Instruction Tuning Code Large Language Models
本文通过利用 Git 提交记录中代码变更和人类指令的自然结构，对大语言模型进行指令微调，提出了一个名为 CommitPack 的数据集，包含 4TB 的 Git 提交记录，涵盖 350 种编程语言。实验结果表明在 StarCoder 模型上使用 CommitPack 数据集进行微调后，在 HumanEval 上达到了 46.2% 的 pass@1 成绩，以及在扩展后的 HumanEvalPack 基准测试中表现优异，证明了 CommitPack 在推广到更多编程语言和任务上的优势。
发布日期：2023-08-14
链接：https://arxiv.org/abs/2308.07124
机构：BigCode

At Which Training Stage Does Code Data Help LLMs Reasoning?
本文探讨了在训练大语言模型时，不同阶段引入代码数据对模型推理能力的影响。研究结果表明：1）在预训练阶段引入代码和文本混合数据可以显著提升 LLM 的推理能力，几乎不影响其他任务的表现；2）在指令微调阶段，引入代码数据可以赋予 LLM 特定任务的推理能力；3）通过动态混合策略逐步引入代码和文本数据，有助于 LLM 在训练过程中逐步学习推理能力。这些发现有助于更好地理解和应用 LLM 在科学问答和法律支持等领域的推理能力。
发布日期：2023-09-28
链接：https://arxiv.org/abs/2309.16298
机构：National University of Defense Technology

LLM-Assisted Code Cleaning For Training Accurate Code Generators
本文主要探讨了代码数据质量对代码生成模型的影响。作者提出了一种新的数据清理管道，通过重命名变量、将复杂代码模块化分解以及插入基于自然语言的计划来提升现有程序的结构和可读性。实验结果表明，与直接在原始数据集上进行微调相比，在经过其清理和优化的模块化程序数据集上进行微调能提升最高达 30%的性能。此外，研究还发现即使使用更少量但质量更高的数据，经过清理的数据模型在表现上也优于原始完整数据集模型，并且在与闭源的 AlphaCoder 模型对比中也表现优异。
发布日期：2023-11-25
链接：https://arxiv.org/abs/2311.14904
机构：University of California, Berkeley

B-Coder: Value-Based Deep Reinforcement Learning for Program Synthesis
本文提出了一种基于价值方法的代码生成新方法——B-Coder（贝尔曼编码器）。论文采用了强化学习（RL）和大模型相结合的手段，通过预训练语言模型初始化 RL 代理，并引入保守的贝尔曼算子来简化训练。它采用基于值函数的方法，而不是传统的基于策略方法。同时，论文展示了如何利用学习到的值函数后处理生成的程序。实验表明，B-Coder 在处理编程合成任务时达到了最先进的性能，且无需复杂的奖励设计，验证了基于价值的 RL 方法的有效性。
发布日期：2023-10-04
链接：https://arxiv.org/abs/2310.03173
机构：University of Illinois Chicago, ByteDance Inc.

Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification
本文探讨了代码对大语言模型（如 GPT-4）的数学推理能力提升的影响，尤其是通过引入不同的代码使用频率限制。研究发现，GPT-4 Code Interpreter 的成功主要归功于其生成和执行代码、评估代码执行结果以及修正错误解答的能力。在此基础上，作者提出了一种新颖且有效的提示方法，即基于代码的自我验证（CSV），用于进一步提高 GPT-4 在数学推理方面的性能。该方法使用 0-shot 提示词，鼓励模型使用代码自我验证答案，并在验证结果为“错误”时自动修改解答。使用这种方法，GPT-4 Code Interpreter 在 MATH 数据集上的 0-shot 准确率显著提升，从 53.9% 提高到 84.3%。
发布日期：2023-08-15
链接：https://arxiv.org/abs/2308.07921
机构：The Chinese University of Hong Kong

MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning
本文通过生成包含数学问题及其代码解决方案的新颖高质量数据集（MathCodeInstruct），并结合定制的监督微调和推理方法，来微调开源语言模型，从而提升其数学推理能力。最终基于微调 LLaMA-2 得到的 MathCoder 模型在 MATH 和 GSM8K 数据集上的表现超越了现有的开源模型，甚至超过了 ChatGPT-3.5、PaLM-2 和 GPT-4 的表现。
发布日期：2023-10-05
链接：https://arxiv.org/abs/2310.03731
机构：The Chinese University of Hong Kong

CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules
本文提出了一个名为 CodeChain 的新框架，用于改进大语言模型在复杂编程任务中的表现。通过引导模型生成模块化代码并使用自我修正链的方式，CodeChain 显著提升了生成代码的模块性和正确性。具体方法包括思维链提示生成模块化代码、提取和聚类生成的子模块，以及利用这些模块重新生成代码。实验证明，CodeChain 在多个编程基准测试上取得了显著的性能提升，并且适用于不同的大模型。论文还进行了消融研究，分析了不同提示方法、聚类数目、模型大小等因素对效果的影响。
发布日期：2023-10-13
链接：https://arxiv.org/abs/2310.08992
机构：Salesforce Research

Is Self-Repair a Silver Bullet for Code Generation?
本主要研究了大语言模型在自我修复代码方面的性能，发现当考虑修复成本时，性能提升往往有限，且在数据子集之间差异很大。作者认为这是由于模型提供反馈的能力受限，通过使用更强的模型来提高反馈质量，观察到更大的性能提升。此外，一项小型研究表明，即使是最强的模型，自我修复仍然远远落后于人类调试所能达到的效果。
发布日期：2023-06-16
链接：https://arxiv.org/abs/2306.09896
机构：MIT, Microsoft Research

An interpretable error correction method for enhancing code-to-code translation
本文提出了一种新的方法，名为 kNN-ECD，它结合了最近邻搜索和关键值错误校正数据存储器，用于重写 TransCoder-ST 生成的不正确的程序翻译。此外，论文还提出了 kNN-ECS_m，这是一种采用串联连接的分布式结构，利用 m 个不同的专家进行多轮错误校正。这种方法能够提高程序翻译的准确性，并克服基于 Transformer 的代码翻译模型的固有限制，例如需要大量的新训练资源和结果的不可解释性。
发布日期：2024-01-16
链接：https://openreview.net/forum?id=fVxIEHGnVT&noteId=CyxZE2UbHF
机构：Heidelberg University

Coeditor: Leveraging Contextual Changes for Multi-round Code Auto-editing
本文提出了一种面向代码编辑任务的生成式语言模型 Coeditor。与现有模型关注代码生成不同，Coeditor 专注于基于同一代码库中的近期更改来预测对代码区域的编辑。该模型采用行差异格式表示代码变更，并利用静态分析形成大型自定义模型上下文，确保预测所需信息的可用性。在单轮单编辑任务中， Coedito r 明显优于 GPT-3.5 和其他代码补全模型。在多轮多编辑场景下，通过迭代条件化用户编辑可进一步提高性能。作者开源了代码、数据和模型权重，并发布了基于该模型的 VSCode 扩展，以促进未来研究和实际应用。
发布日期：2023-05-29
链接：https://arxiv.org/abs/2305.18584
机构：University of Texas at Austin

RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems
本文提出了一个新的基准测试集 RepoBench，用于评估仓库级别的代码自动补全系统。与现有的测试集不同，RepoBench 专注于更加复杂和真实的多文件编程场景，支持 Python 和 Java 两种语言。RepoBench 包含三个相互关联的评估任务：检索其他文件中最相关的代码片段作为跨文件上下文，利用跨文件和文件内上下文预测下一行代码，以及结合检索和下一行预测的复杂任务。通过 RepoBench 可以更全面地比较不同自动补全系统的性能，并推动其不断改进。
发布日期：2023-06-05
链接：https://arxiv.org/abs/2306.03091
机构：University of California, San Diego

SWE-bench: Can Language Models Resolve Real-World GitHub Issues?
本文提出了一个名为 SWE-bench 的软件工程问题评估框架，用于评估下一代语言模型在实际软件开发中的能力。该框架包含了从 12 个流行的 Python 仓库中提取的 2,294 个真实 GitHub 问题和相应的合并请求。论文发现，目前最先进的语言模型在解决这些软件工程问题上的表现还很有限，突显了软件工程领域对语言模型的能力还有更高的要求，有助于评估和推动语言模型发展。
发布日期：2023-10-10
链接：https://arxiv.org/abs/2310.06770
机构：Princeton University

A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis
本文提出了一个名为 WebAgent 的智能体系统，它结合了大语言模型 Flan-U-PaLM 和 HTML-T5，可以根据自然语言指令自主完成在真实网站上的任务。WebAgent 通过把指令分解为子指令，总结长 HTML 文档为与任务相关的片段，并生成 Python 程序来执行网页操作。实验表明，该模块化方案可以将在真实网站上的成功率提高 50%以上，HTML-T5 在各种 HTML 理解任务上表现最佳，在 MiniWoB 网页自动化基准测试中比先前方法高出 18.7%，并在 Mind2Web 离线任务规划评估中取得了最先进的性能。
发布日期：2023-07-24
链接：https://arxiv.org/abs/2307.12856
机构：Google DeepMind

Learning Performance-Improving Code Edits
本文提出了一个利用大语言模型进行高层程序优化的框架。作者收集了一个包含超过 77,000 对人类程序员提交的 C++ 程序优化数据集，并设计了基于 gem5 全系统模拟器的环境来可靠地评估程序优化的影响。此外，作者提出了多种适应代码优化的策略，包括基于检索的少样本提示、思维链、性能条件生成和基于自我博弈的合成数据增强等。结合这些技术，该模型在八次生成中实现了平均 6.86 倍的加速，高于单个程序员的平均优化水平（3.66 倍）。利用该模型的最快生成结果，作者将数据集上可能达到的最快加速上限刷新为为 9.64 倍，超过了使用人类提交的最快结果（9.56 倍）。
发布日期：2023-02-15
链接：https://arxiv.org/abs/2302.07867
机构：University of Pennsylvania

## 联系我们

我们团队的多项工作，包括综述、模型、数据集，都在陆续开源中。如果您喜欢我们的工作，欢迎试用、指正错误和贡献代码，可以的话请给我们的项目增加 Star、引用我们的论文以支持我们。

- 代码大模型综述（覆盖 900 篇论文）：https://arxiv.org/abs/2311.07989
- GitHub 项目：https://github.com/codefuse-ai/Awesome-Code-LLM
- HuggingFace 主页：https://huggingface.co/codefuse-ai
- 魔搭社区主页：https://modelscope.cn/organization/codefuse-ai
